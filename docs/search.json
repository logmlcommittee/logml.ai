[
  {
    "objectID": "logml2024/speaker.html",
    "href": "logml2024/speaker.html",
    "title": "Speakers",
    "section": "",
    "text": "Keynote speakers\n\n\n\n\n\n\n\n\n\n\nKathryn Hess\n\n\nEPFL\n\n\n\n\n\n\n\n\n\n\n\n\n\nPietro Liò\n\n\nUniversity of Cambridge\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nSpeakers\n\n\n\n\n\n\n\n\n\n\nAnthea Monod\n\n\nImperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\nIslem Rekik\n\n\nImperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\nJoan Bruna\n\n\nNew York University\n\n\n\n\n\n\n\n\n\n\n\n\n\nMichalis Vazirgiannis\n\n\nEcole Polytechnique\n\n\n\n\n\n\n\n\n\n\n\n\n\nVishnu Jejjala\n\n\nUniversity of the Witwatersrand: Johannesburg\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Archives",
      "LOGML 2024",
      "Speakers"
    ]
  },
  {
    "objectID": "logml2024/projects.html",
    "href": "logml2024/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Calabi-Yau Metrics with U(1)-invariant Neural Networks\n\n\n\n\n\n\n\n\n\n\n\nYidi Qi\n\n\n\n\n\n\n\n\n\n\n\n\nExploiting Graph Neural Networks for Prescriptive maintenance of CERN’s technical infrastructure\n\n\n\n\n\n\n\n\n\n\n\nLorenzo Giusti\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating Calabi-Yau Manifolds with Machine Learning\n\n\n\n\n\n\n\n\n\n\n\nElli Heyes\n\n\n\n\n\n\n\n\n\n\n\n\nGeometric GNNs for particle level reconstruction\n\n\n\n\n\n\n\n\n\n\n\nDolores Garcia\n\n\n\n\n\n\n\n\n\n\n\n\nGeometry for Distribution Learning\n\n\n\n\n\n\n\n\n\n\n\nZhengang Zhong\n\n\n\n\n\n\n\n\n\n\n\n\nGraph Learning for Uplift Modeling\n\n\n\n\n\n\n\n\n\n\n\nGeorge Panagopoulos\n\n\n\n\n\n\n\n\n\n\n\n\nInvariantly learning terminal singularities\n\n\n\n\n\n\n\n\n\n\n\nSara Veneziale\n\n\n\n\n\n\n\n\n\n\n\n\nLearning to predict optimal solution value for NP-Hard Combinatorial problems\n\n\n\n\n\n\n\n\n\n\n\nSahil Manchanda\n\n\n\n\n\n\n\n\n\n\n\n\nMatching graphs with spatial constrains\n\n\n\n\n\n\n\n\n\n\n\nAnna Calissano\n\n\n\n\n\n\n\n\n\n\n\n\nMixed Curvature Graph Neural Networks\n\n\n\n\n\n\n\n\n\n\n\nRishi Sonthalia\n\n\n\n\n\n\n\n\n\n\n\n\nMultimodal Protein Representation Learning\n\n\n\n\n\n\n\n\n\n\n\nMichail Chatzianastasis\n\n\n\n\n\n\n\n\n\n\n\n\nOn the Geometry of Relative Representations\n\n\n\n\n\n\n\n\n\n\n\nMarco Fumero\n\n\n\n\n\n\n\n\n\n\n\n\nPowerful Graph Neural Networks for Relational Databases\n\n\n\n\n\n\n\n\n\n\n\nJoshua Robinson\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting the pathogenicity of a (missense) mutation\n\n\n\n\n\n\n\n\n\n\n\nAbhishek Sharma\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-supervised learning for Topological Neural Networks\n\n\n\n\n\n\n\n\n\n\n\nClaudio Battiloro\n\n\n\n\n\n\n\n\n\n\n\n\nSpectral Signed GNNs for fMRI Connectomes\n\n\n\n\n\n\n\n\n\n\n\nRahul Singh\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Archives",
      "LOGML 2024",
      "Projects"
    ]
  },
  {
    "objectID": "logml2024/projects2024/project5/index.html",
    "href": "logml2024/projects2024/project5/index.html",
    "title": "Generating Calabi-Yau Manifolds with Machine Learning",
    "section": "",
    "text": "I am final year PhD student at City, University of London and soon to be postdoctoral researcher at Imperial College London. Broadly speaking my research involves using techniques from data science and machine learning to study compactification spaces in string theory, namely Calabi-Yau manifolds and G2 manifolds. I attended LOGML just before I started my PhD and learnt a lot, so I look forward to being part of LOGML once again this time as a mentor."
  },
  {
    "objectID": "logml2024/projects2024/project5/index.html#elli-heyes",
    "href": "logml2024/projects2024/project5/index.html#elli-heyes",
    "title": "Generating Calabi-Yau Manifolds with Machine Learning",
    "section": "",
    "text": "I am final year PhD student at City, University of London and soon to be postdoctoral researcher at Imperial College London. Broadly speaking my research involves using techniques from data science and machine learning to study compactification spaces in string theory, namely Calabi-Yau manifolds and G2 manifolds. I attended LOGML just before I started my PhD and learnt a lot, so I look forward to being part of LOGML once again this time as a mentor."
  },
  {
    "objectID": "logml2024/projects2024/project5/index.html#project",
    "href": "logml2024/projects2024/project5/index.html#project",
    "title": "Generating Calabi-Yau Manifolds with Machine Learning",
    "section": "Project",
    "text": "Project\nCalabi-Yau manifolds are of interest in string theory as they describe the shape of the extra dimensions that arise in the theory. It is an active area of research to construct new Calabi-Yau manifolds, and in this project we will use genetic algorithms to generate Calabi-Yau manifolds arising from the construction of hypersurfaces in toric varieties. This was pioneered in the preprint “New Calabi-Yau Manifolds from Genetic Algorithms” (https://arxiv.org/abs/2306.06159). Using our algorithm we will search for Calabi-Yau manifolds with the set of properties that are required in order to give rise to the physics of the Standard Model. Time permitting, we may experiment with more complicated algorithms, for example by combining genetic algorithms with neural networks to optimise the evolutionary process."
  },
  {
    "objectID": "logml2024/projects2024/project3/index.html",
    "href": "logml2024/projects2024/project3/index.html",
    "title": "Self-supervised learning for Topological Neural Networks",
    "section": "",
    "text": "Claudio Battiloro is a PhD candidate in ICT at Sapienza University of Rome, a Visiting Scholar at Harvard SPH, and a former Visiting Associate at the SEAS of University of Pennsylvania. He is a prospective co-appointed postdoctoral fellow at Harvard University and University of Pennsylvania. Claudio’s research interests include theory and methods for topological and algebraic signal processing, topological deep learning, and distributed optimization. He has several publications in top-tier journals and conferences. Claudio received different awards such as the IEEE SPS Italian Chapter Best M.Sc. Thesis Award (2020). In 2020, he graduated with distinction in the M.Sc. in Data Science with a (university-overall) Top 400 Students award at Sapienza University."
  },
  {
    "objectID": "logml2024/projects2024/project3/index.html#claudio-battiloro",
    "href": "logml2024/projects2024/project3/index.html#claudio-battiloro",
    "title": "Self-supervised learning for Topological Neural Networks",
    "section": "",
    "text": "Claudio Battiloro is a PhD candidate in ICT at Sapienza University of Rome, a Visiting Scholar at Harvard SPH, and a former Visiting Associate at the SEAS of University of Pennsylvania. He is a prospective co-appointed postdoctoral fellow at Harvard University and University of Pennsylvania. Claudio’s research interests include theory and methods for topological and algebraic signal processing, topological deep learning, and distributed optimization. He has several publications in top-tier journals and conferences. Claudio received different awards such as the IEEE SPS Italian Chapter Best M.Sc. Thesis Award (2020). In 2020, he graduated with distinction in the M.Sc. in Data Science with a (university-overall) Top 400 Students award at Sapienza University."
  },
  {
    "objectID": "logml2024/projects2024/project3/index.html#project",
    "href": "logml2024/projects2024/project3/index.html#project",
    "title": "Self-supervised learning for Topological Neural Networks",
    "section": "Project",
    "text": "Project\nSelf-supervised methods can be broadly categorized into contrastive approaches and predictive approaches. In the context of GNNs, self-supervision is particularly useful for learning graph encoders by utilizing information from the distribution of unlabeled graphs and minimizing a self-supervised loss. However, literature about self-supervised techniques for Topological Deep Learning is extremely sparse, and only a couple of works for simplicial complexes have been presented. For this reason, I propose to design novel self-predictive and contrastive objectives for training TDL architectures.\nIn particular, I will mentor the attendants with the aim of investigating generative objectives based on the predictive approach for architectures defined over regular cell complexes. We will explore cell complex-based reconstruction tasks that can properly generalize the well-studied node property prediction and attribute denoising tasks in the space of attributed graphs.\nWe will go through the actual SoA to design novel techniques able to combine my personal signal processing-grounded approach with cutting edge ML research on Topological and Geometric DL."
  },
  {
    "objectID": "logml2024/projects2024/project11/index.html",
    "href": "logml2024/projects2024/project11/index.html",
    "title": "Spectral Signed GNNs for fMRI Connectomes",
    "section": "",
    "text": "Rahul is a Wu Tsai postdoctoral fellow at Yale University where he is mentored by Smita Krishnaswamy and Joy Hirsch. He received his PhD in Machine Learning in under the mentorship of Yongxin Chen from the Georgia Institute of Technology. His research interests are in the areas of signal processing, graph neural networks, and machine learning applications in bioinformatics and neuroscience. One of his current research focuses is to explore the neural complexes of brain-to-brain communication when two humans interact."
  },
  {
    "objectID": "logml2024/projects2024/project11/index.html#rahul-singh",
    "href": "logml2024/projects2024/project11/index.html#rahul-singh",
    "title": "Spectral Signed GNNs for fMRI Connectomes",
    "section": "",
    "text": "Rahul is a Wu Tsai postdoctoral fellow at Yale University where he is mentored by Smita Krishnaswamy and Joy Hirsch. He received his PhD in Machine Learning in under the mentorship of Yongxin Chen from the Georgia Institute of Technology. His research interests are in the areas of signal processing, graph neural networks, and machine learning applications in bioinformatics and neuroscience. One of his current research focuses is to explore the neural complexes of brain-to-brain communication when two humans interact."
  },
  {
    "objectID": "logml2024/projects2024/project11/index.html#project",
    "href": "logml2024/projects2024/project11/index.html#project",
    "title": "Spectral Signed GNNs for fMRI Connectomes",
    "section": "Project",
    "text": "Project\nThe existing GNN architectures have focused almost exclusively on graphs with nonnegative edges, which encode some kind of similarity relation between the incident nodes. In contrast, negative edges are often useful to model dissimilarity relations: for instance, in social networks, users may have common/opposite political views or like/dislike each other. Such negative correlations also arise in functional magnetic resonance imaging (fMRI)-derived brain networks or connectomes [1]. These dissimilarity relations are modeled using signed graphs allowing the edges to take both positive or negative values.\nWe will build upon the recently proposed spectral signed graph neural network (GNN) architechtures [2] that can handle (directed) signed graphs and provide interpretable models backed by frequency analysis of signed graphs. Existing representation learning based methods such as BrainGNN [3] for fMRI data do not take these negative correlations into account. The focus of the project will be to utilize interpretable spectral signed GNNs to understand the neurobiological information of negative edges in fMRI connectomes. We will use open-source fMRI datasets human connectome project (HCP) and autism brain imaging data exchange (ABIDE) for our analysis. The brain will be modeled as a signed graph with nodes representing brain regions of interest (ROIs) and edges representing the functional connectivity between those ROIs computed as the pairwise (positive as well as negative) correlations of fMRI time series. The goal will be to identify and explain the effect of negative correlations between different brain regions on learned representations over fMRI connectomes.\n[1] Liang Zhan et al. The significance of negative correlations in brain connectivity, Journal of Comparative Neurology 2017.\n[2] Rahul Singh and Yongxin Chen, Signed graph neural networks: A frequency perspective, Transactions on Machine Learning Research 2023.\n[3] Xiaoxiao Li et al. BrainGNN: Interpretable brain graph neural network for fMRI analysis, Medical Image Analysis 2021."
  },
  {
    "objectID": "logml2024/projects2024/project6/index.html",
    "href": "logml2024/projects2024/project6/index.html",
    "title": "Graph Learning for Uplift Modeling",
    "section": "",
    "text": "George Panagopoulos is a postdoctoral scientist at the University of Luxembourg, researching graph neural networks and causal inference for biomedical applications. Before that, he was working on machine learning for operations research as an applied scientist in the Algorithms and Optimization lab of Amazon Transportation Services in Luxembourg. He received his PhD in machine learning from the Data Science and Mining Team of the École Polytechnique, specializing in graph learning for forecasting and combinatorial optimization. Previously, he was a research assistant and obtained his M.Sc. in computer science with a focus on digital signal processing for neural/physiological data from the University of Houston."
  },
  {
    "objectID": "logml2024/projects2024/project6/index.html#george-panagopoulos",
    "href": "logml2024/projects2024/project6/index.html#george-panagopoulos",
    "title": "Graph Learning for Uplift Modeling",
    "section": "",
    "text": "George Panagopoulos is a postdoctoral scientist at the University of Luxembourg, researching graph neural networks and causal inference for biomedical applications. Before that, he was working on machine learning for operations research as an applied scientist in the Algorithms and Optimization lab of Amazon Transportation Services in Luxembourg. He received his PhD in machine learning from the Data Science and Mining Team of the École Polytechnique, specializing in graph learning for forecasting and combinatorial optimization. Previously, he was a research assistant and obtained his M.Sc. in computer science with a focus on digital signal processing for neural/physiological data from the University of Houston."
  },
  {
    "objectID": "logml2024/projects2024/project6/index.html#project",
    "href": "logml2024/projects2024/project6/index.html#project",
    "title": "Graph Learning for Uplift Modeling",
    "section": "Project",
    "text": "Project\nFrom precision medicine and drug discovery to recommendation systems and online marketing, causal inference using randomized experiments is the gold standard for decision-making.\nHowever, these experiments require interventions that might be too costly, time-consuming, or simply impossible. Machine learning has provided promising solutions in predicting the conditional average treatment effect of an intervention, without actually making it [1]. In this project, we will examine the use of machine learning to facilitate a large-scale marketing campaign.\nDuring such a campaign, promotional codes are distributed to users to increase their activity. Given a specific promotional budget, the campaign should minimize the outreach to users who will not respond, or even worse respond negatively. Hence the aim is to build a model that can predict which users will respond positively to an intervention, commonly called uplift modelling. In uplift modeling, we run an experiment on a representative subset of the users and train a model to predict the average treatment effect on the rest of the user base.\nThe plan is to focus on the network of interactions between samples, which can hide potential confounders that introduce bias in the experiment [2]. We will tackle uplift modeling using graph learning on a heterogeneous graph of an e-commerce system with ground truth interventions and outcomes from an actual marketing campaign. We will then compare the performance with state-of-the-art methods using one of the libraries for ML-based causal inference [3].\n[1] Künzel, Sören R., et al. “Metalearners for estimating heterogeneous treatment effects using machine learning.” Proceedings of the national academy of sciences 116.10 (2019): 4156-4165.\n[2] Guo, Ruocheng, Jundong Li, and Huan Liu. “Learning individual causal effects from networked observational data.” Proceedings of the 13th international conference on web search and data mining. 2020.\n[3] Syrgkanis, Vasilis, et al. “Causal inference and machine learning in practice with econml and causalml: Industrial use cases at microsoft, tripadvisor, uber.” Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining. 2021."
  },
  {
    "objectID": "logml2024/projects2024/project8/index.html",
    "href": "logml2024/projects2024/project8/index.html",
    "title": "Exploiting Graph Neural Networks for Prescriptive maintenance of CERN’s technical infrastructure",
    "section": "",
    "text": "Lorenzo Giusti is currently a senior research scientist at CERN working on geometric and topological deep learning for particle physics in the cryogenics group. Lorenzo holds a PhD in Data Science at La Sapienza, University of Rome, specialized in topological neural networks. His research includes a period of visiting at the University of Cambridge and as a research scientist intern at NASA’s Jet Propulsion Laboratory, where he led a project on Martian terrain modeling using spacecraft imagery and Neural Radiance Fields."
  },
  {
    "objectID": "logml2024/projects2024/project8/index.html#lorenzo-giusti",
    "href": "logml2024/projects2024/project8/index.html#lorenzo-giusti",
    "title": "Exploiting Graph Neural Networks for Prescriptive maintenance of CERN’s technical infrastructure",
    "section": "",
    "text": "Lorenzo Giusti is currently a senior research scientist at CERN working on geometric and topological deep learning for particle physics in the cryogenics group. Lorenzo holds a PhD in Data Science at La Sapienza, University of Rome, specialized in topological neural networks. His research includes a period of visiting at the University of Cambridge and as a research scientist intern at NASA’s Jet Propulsion Laboratory, where he led a project on Martian terrain modeling using spacecraft imagery and Neural Radiance Fields."
  },
  {
    "objectID": "logml2024/projects2024/project8/index.html#project",
    "href": "logml2024/projects2024/project8/index.html#project",
    "title": "Exploiting Graph Neural Networks for Prescriptive maintenance of CERN’s technical infrastructure",
    "section": "Project",
    "text": "Project\nCERN, the European Organization for Nuclear Research, is the largest centre for scientific research in particle physics, and it is known for its complex system of systems comprising advanced particle accelerators and detectors. To fulfill the physics program and deliver the required luminosity for the experiments, advanced tools are required to operate, maintain, and guide device consolidation. It is, therefore, critical to monitor the activities objectively and guide the implementation of strategies to improve performance, optimize costs and highlight key areas needing prioritization. Moreover, the availability of reliable, cost-effective, and energy-efficient sensors entails growing data that captures the underlying phenomena happening within the CERN’s technical infrastructure. To identify failures early and perform prescriptive maintenance of such a complex system of systems, this project aims to reveal hidden dependencies and approach the maintenance operations within the technical infrastructure of the largest particle accelerator complex of the world using graph neural networks."
  },
  {
    "objectID": "logml2024/projects2024/project7/index.html",
    "href": "logml2024/projects2024/project7/index.html",
    "title": "Powerful Graph Neural Networks for Relational Databases",
    "section": "",
    "text": "Joshua is a postdoctoral scholar at Stanford working with Jure Leskovec. He obtained his PhD from MIT CSAIL in 2023, where we was advised by Stefanie Jegelka and Suvrit Sra. His interests include 1) designing algorithms for learning over structured domains, such as graphs, eigenvectors, and relational databases, and 2) self-supervised representation learning. He was also a co-organizer of the first LoG conference."
  },
  {
    "objectID": "logml2024/projects2024/project7/index.html#joshua-robinson",
    "href": "logml2024/projects2024/project7/index.html#joshua-robinson",
    "title": "Powerful Graph Neural Networks for Relational Databases",
    "section": "",
    "text": "Joshua is a postdoctoral scholar at Stanford working with Jure Leskovec. He obtained his PhD from MIT CSAIL in 2023, where we was advised by Stefanie Jegelka and Suvrit Sra. His interests include 1) designing algorithms for learning over structured domains, such as graphs, eigenvectors, and relational databases, and 2) self-supervised representation learning. He was also a co-organizer of the first LoG conference."
  },
  {
    "objectID": "logml2024/projects2024/project7/index.html#project",
    "href": "logml2024/projects2024/project7/index.html#project",
    "title": "Powerful Graph Neural Networks for Relational Databases",
    "section": "Project",
    "text": "Project\nMuch of the world’s data is stored in relational databases, which contain multiple tables whose rows are connected via primary-foreign key relations [1]. Consequently, many interesting forecasting problems can be thought of as predictions on relational data (Who will win next weekend’s F1? Will patient A respond to treatment X?). Crucially, relational databases can be viewed as “relational graphs”, with one node per table row, and edges given by primary-foreign key relations. However, relational graphs are not arbitrary graphs. They are k-partite, and some of the node partitions have a fixed number of in- and out-edges per node. This suggests a need for specialized GNN architectures suited to this graph data. The aim of this project will be an initial scoping of the (in)suitability of existing GNNs for processing relational graphs. Depending on student interest, this project’s scope is both a) empirical: evaluating the performance of existing GNN models and components to determine best modeling practices (for testing we have recently released a benchmark suite: https://relbench.stanford.edu/), or b) theoretical: finding examples of relational data structures that existing GNNs cannot distinguish. In both cases, this lays the groundwork for designing more powerful graph networks for relational data.\n[1] Relational Deep Learning: Graph Representation Learning on Relational Databases, Fey et al. 2023 arXiv:2312.04615."
  },
  {
    "objectID": "logml2024/projects2024/project15/index.html",
    "href": "logml2024/projects2024/project15/index.html",
    "title": "Calabi-Yau Metrics with U(1)-invariant Neural Networks",
    "section": "",
    "text": "Yidi Qi is a PhD student in the Department of Physics at Northeastern University, under the supervision of Fabian Ruehle. His research focuses on applying machine learning techniques to string theory and mathematics. He is also a junior investigator at the NSF AI Institute for Artificial Intelligence and Fundamental Interactions (IAIFI)."
  },
  {
    "objectID": "logml2024/projects2024/project15/index.html#yidi-qi",
    "href": "logml2024/projects2024/project15/index.html#yidi-qi",
    "title": "Calabi-Yau Metrics with U(1)-invariant Neural Networks",
    "section": "",
    "text": "Yidi Qi is a PhD student in the Department of Physics at Northeastern University, under the supervision of Fabian Ruehle. His research focuses on applying machine learning techniques to string theory and mathematics. He is also a junior investigator at the NSF AI Institute for Artificial Intelligence and Fundamental Interactions (IAIFI)."
  },
  {
    "objectID": "logml2024/projects2024/project15/index.html#project",
    "href": "logml2024/projects2024/project15/index.html#project",
    "title": "Calabi-Yau Metrics with U(1)-invariant Neural Networks",
    "section": "Project",
    "text": "Project\nCalabi-Yau manifolds are compact Kähler manifolds which admit a Ricci-flat metric. They play important roles in modern physics, notably as the extra dimensions in string theory. Knowing the explicit Ricci-flat metrics on Calabi-Yau manifolds is essential for constructing realistic models that describe our four-dimensional universe. However, finding such metrics requires solving the complex Monge-Ampère equation and it is generally believed that an analytical solution does not exist. Even solving it numerically has been proved to be challenging. Recent progress shows that specially designed physics-informed neural networks can help obtain numerical Calabi-Yau metrics much more efficiently. These networks must be invariant under a certain U(1)-action (which is the same as an SO(2)-action), and so far only one special architecture of invariant networks has been implemented. In this project, we will implement other architectures which are invariant under this U(1) group action. Because this gives much more flexibility, there is a chance that this can improve the performance of the physics-informed neural networks, therefore leading to better approximations for Calabi-Yau metrics."
  },
  {
    "objectID": "logml2024/projects2024/project13/index.html",
    "href": "logml2024/projects2024/project13/index.html",
    "title": "Learning to predict optimal solution value for NP-Hard Combinatorial problems",
    "section": "",
    "text": "Sahil Manchanda is a PhD scholar at the Department of Computer Science at the Indian Institute of Technology Delhi, working under the supervision of Prof. Sayan Ranu. Sahil works in the area of Learning to solve graph optimization problems with focus on Combinatorial Optimization, Graph Neural Networks, Lifelong Learning and Generative modeling. He is also interested in applications of Computer Science concepts in other fields such as Material Science and Hardware. He has interned at prestigious institutes such as the University of Tokyo, NAVER Labs- France and Qualcomm AI Research Amsterdam. His research works have been published in top conferences such as NeurIPS, AAAI, ICML, ECML-PKDD and LoG. Additionally he has one US patent granted to his name. Sahil has been the recipient of the Qualcomm Innovation Fellowship for the year 2022."
  },
  {
    "objectID": "logml2024/projects2024/project13/index.html#sahil-manchanda",
    "href": "logml2024/projects2024/project13/index.html#sahil-manchanda",
    "title": "Learning to predict optimal solution value for NP-Hard Combinatorial problems",
    "section": "",
    "text": "Sahil Manchanda is a PhD scholar at the Department of Computer Science at the Indian Institute of Technology Delhi, working under the supervision of Prof. Sayan Ranu. Sahil works in the area of Learning to solve graph optimization problems with focus on Combinatorial Optimization, Graph Neural Networks, Lifelong Learning and Generative modeling. He is also interested in applications of Computer Science concepts in other fields such as Material Science and Hardware. He has interned at prestigious institutes such as the University of Tokyo, NAVER Labs- France and Qualcomm AI Research Amsterdam. His research works have been published in top conferences such as NeurIPS, AAAI, ICML, ECML-PKDD and LoG. Additionally he has one US patent granted to his name. Sahil has been the recipient of the Qualcomm Innovation Fellowship for the year 2022."
  },
  {
    "objectID": "logml2024/projects2024/project13/index.html#project",
    "href": "logml2024/projects2024/project13/index.html#project",
    "title": "Learning to predict optimal solution value for NP-Hard Combinatorial problems",
    "section": "Project",
    "text": "Project\nRecently, a lot of interest has been shown in the ML community to learn to solve NP-Hard Combinatorial Optimization(CO) problems. The prime reasons being:\n\nDeep Learning models offer the advantage of speed-up(with good quality) due to GPU based acceleration which is expected to further enhance as GPU hardware improves.\nHeuristics can be learned/distilled directly from the data distribution, thus minimizing or eliminating the need for manually crafted designs.\n\nIn this project we take a different approach and aim to learn to predict the optimal values of NP-Hard Combinatorial problems. Combinatorial optimization solvers can be augmented with machine learning-based optimal solution value predictors to reduce the search space during the quest for precise and high-quality solutions[1]. Further, in some problems such as Graph Edit Distance, directly estimating the optimal value between 2 graphs is also very useful[2].\nRecently, one work[1] attempts to learn to predict optimal values for TSP and job-shop scheduling problems using a Graph Transformer. The paper has interesting results giving hope that with further enhancements and better modelling it might be possible to learn to predict the optimal value more accurately. However, to understand its applicability in practical scenarios, results on crucial aspects such as inference time, percentage errors against optimal solutions and scalability to large CO problem instances are not discussed in the paper. Further, analysis is also required on how does the cost-accuracy tradeoff vary as the model capacity changes especially w.r.t to Graph Transformer layers etc.\nGoal of project:\nThe goal of this project will be initially implement the paper[1] and find out the cost(running time) and error(%) trade off with different model capacities on a set of CO problems. Further, if time permits analyze how does the method scale for larger instances of CO problems such as TSP 100 and TSP 200.\nExecution Plan:\nThe paper mentions that it uses GraphGPS[3] Transformer from PyTorch Geometric. It is easy to use.\nI will provide instances for CO problems(Eg:- TSP for different sizes, Job-Shop Scheduling Problem etc.) and their optimal values to the students. The students will write code to train the parameters of the Graph Transformer. Analyze results w.r.t cost vs prediction accuracy(error) trade off and explore the Pareto frontier etc. If time permits then understand the generalization w.r.t problem size.\nFinally, based upon the results we get, we together hope to formulate and investigate an interesting problem :).\nReferences:\n[1] Wang, Tianze, Amir H. Payberah, and Vladimir Vlassov. “Graph Representation Learning with Graph Transformers in Neural Combinatorial Optimization.”\n[2] Ranjan, Rishabh, et al. “Greed: A neural framework for learning graph distance functions.” Advances in Neural Information Processing Systems 35 (2022): 22518-22530.\n[3] Rampášek, Ladislav, et al. “Recipe for a general, powerful, scalable graph transformer.” Advances in Neural Information Processing Systems 35 (2022): 14501-14515."
  },
  {
    "objectID": "logml2024/speakers2024/keynote/pietro.html",
    "href": "logml2024/speakers2024/keynote/pietro.html",
    "title": "Pietro Liò",
    "section": "",
    "text": "Home\n    Speakers\n    Pietro Liò\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nI am Full Professor at the department of Computer Science and Technology of the University of Cambridge and I am a member of the Artificial Intelligence group. I am a member of the Cambridge Centre for AI in Medicine. My research interest focuses on developing Artificial Intelligence and Computational Biology models to understand diseases complexity and address personalised and precision medicine. Current focus is on Graph Neural Network modeling.\nI have a MA from Cambridge, a PhD in Complex Systems and Non Linear Dynamics (School of Informatics, dept of Engineering of the University of Firenze, Italy) and a PhD in (Theoretical) Genetics (University of Pavia, Italy). Other Affliations: I am member of CAMBRIDGE CENTRE FOR AI IN MEDICINE - the Integrate Cancer Medicine Institute, the committee of MPhil in Computational Biology (Stakeholder Group for the CCBI) , steering committee of Cambridge BIG data, VPH-UK (Virtual Physiological Human), Fellow and member of the Council of Clare Hall College , I am member of Ellis, the European Lab for Learning & Intelligent Systems, I am member of the Academia Europaea; I am listed in www.topitalianscientists.org/Top_italian_scientists_VIA-Academy.aspx"
  },
  {
    "objectID": "logml2024/speakers2024/other/vishnu.html",
    "href": "logml2024/speakers2024/other/vishnu.html",
    "title": "Vishnu Jejjala",
    "section": "",
    "text": "Home\n    Speakers\n    Vishnu Jejjala\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nEarly in his scientific career, Vishnu Jejjala switched from vertebrate paleontology to astronomy. He was seven years old at the time. His interest in explicating the origin of the Universe and playing with cool mathematics led him to string theory. He completed a Ph.D. in Physics at the University of Illinois at Urbana-Champaign in 2002 and subsequently held postdoctoral research appointments at Virginia Tech (2002–2004), Durham University (2004–2007), the Institut des Hautes Études Scientifiques (2007–2009), and Queen Mary, University of London (2009–2011). Since October 2011, Vishnu is the South African Research Chair in Theoretical Particle Cosmology at the University of the Witwatersrand in Johannesburg. He is also a Professor in the School of Physics.\nVishnu’s research interests are broad, but focus on exploring quantum gravity and the structure of quantum field theories with the goal of bringing string theory into contact with the real world. Black holes are one theoretical laboratory for investigating these issues. Another is string compactification on Calabi–Yau spaces. Vishnu has recently been applying techniques from machine learning to study large data sets in string theory and mathematics."
  },
  {
    "objectID": "logml2024/speakers2024/other/islem.html",
    "href": "logml2024/speakers2024/other/islem.html",
    "title": "Islem Rekik",
    "section": "",
    "text": "Home\n    Speakers\n    Islem Rekik\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nIslem Rekik is the Director of the Brain And SIgnal Research and Analysis (BASIRA) laboratory (http://basira-lab.com/) and an Associate Professor at Imperial College London (Innovation Hub I-X).\nTogether with BASIRA members, she conducted more than 90 cutting-edge research projects cross-pollinating AI and healthcare —with a sharp focus on brain imaging and neuroscience. She is also a co/chair/organizer of more than 20 international first-class conferences/workshops/competitions (e.g., Affordable AI 2021-22, Predictive AI 2018-2023, Machine Learning in Medical Imaging 2021-23, WILL competition 2021-22).\nDr Rekik has been awarded prestigious international research fellowships including the EU Marie-Curie Fellowship in 2019 and the TUBITAK 2232 for Outstanding Experienced Researchers during 2020-2022. In addition to her 130+ high-impact publications, she is a strong advocate of equity, inclusiveness and diversity in research.\nShe is the former president of the Women in MICCAI (WiM), the co-founder of the international RISE Network to Reinforce Inclusiveness & diverSity and Empower minority researchers in Low-Middle Income Countries (LMIC) and a committee member of the AFRICAI network."
  },
  {
    "objectID": "logml2024/speakers2024/other/michalis.html",
    "href": "logml2024/speakers2024/other/michalis.html",
    "title": "Michalis Vazirgiannis",
    "section": "",
    "text": "Home\n    Speakers\n    Michalis Vazirgiannis\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nM. Vazirgiannis is a Distinguished Professor at Ecole Polytechnique in France. He has been intensively involved in data science and AI related research. His broad research area is in methods for data mining and machine/deep learning methods for diverse data types and applications (including graphs, text, time series). Recently he is working of i. GNNs and aspects including expressiveness, efficiency, generation ii. Pretrained models and resources for multilingual NLP and Biomedical applications. His research and industrial impact is spanning different domains such as web advertising, social networks, online gambling, insurance, legal text applications, aviation and maritime industry and the bio/medical domain. He also pioneered at the teaching/training level having introduced new machine/deep learning and AI courses for academic and executive training studies. Pr. Vazirgiannis has published more than 250 papers in international journals and proceedings of international conferences and his work is highly cited. On the side of supervision he has supervised 30 completed PhD theses. Finally he has been able to attract significant funding for research from national and international sources, from research agencies and industrial partners (including Google, Airbus, Huawei, Deezer, BNP, LVMH). He lead(s) academic research chairs (DIGITEO 2013-15, ANR/HELAS 2020-26) and an industrial one (AXA, 2015-2018). He has received several awards and distinctions including i. Marie Curie Intra European Fellowship (2006-8) ii. “Rhino-Bird International Academic Expert Award” in recognition of his academic/professional work @ Tencent (2017), iii. best paper awards in international conferences (such as IJCAI 2018, CIKM2013). He has been invited to media interviews in France, USA and China and published popularized articles in French and Greek magazines/newspapers on Artificial Intelligence topics."
  },
  {
    "objectID": "logml2022/projects2022/project2/project2.html",
    "href": "logml2022/projects2022/project2/project2.html",
    "title": "Machine learning the fine interior",
    "section": "",
    "text": "Alexander Kasprzyk is  Associate Professor of Geometry and Head of Pure Mathematics at the  University of Nottingham. His research focuses on the intersection of  algebraic geometry, combinatorics, and Big Data. Through his work he has  pioneered the use of massively-parallel computational algebra and large databases to address fundamental  questions in geometry, applying tens of centuries of computing time to  make substantial and important mathematical advances in the  classification of Fano varieties, including the largest collections of Fano 3-fold and 4-fold varieties known. He maintains the online  Graded Ring Database, and is an editor for the new interdisciplinary  journal “Data Science in the Mathematical Sciences”."
  },
  {
    "objectID": "logml2022/projects2022/project2/project2.html#prof-alexander-kasprzyk",
    "href": "logml2022/projects2022/project2/project2.html#prof-alexander-kasprzyk",
    "title": "Machine learning the fine interior",
    "section": "",
    "text": "Alexander Kasprzyk is  Associate Professor of Geometry and Head of Pure Mathematics at the  University of Nottingham. His research focuses on the intersection of  algebraic geometry, combinatorics, and Big Data. Through his work he has  pioneered the use of massively-parallel computational algebra and large databases to address fundamental  questions in geometry, applying tens of centuries of computing time to  make substantial and important mathematical advances in the  classification of Fano varieties, including the largest collections of Fano 3-fold and 4-fold varieties known. He maintains the online  Graded Ring Database, and is an editor for the new interdisciplinary  journal “Data Science in the Mathematical Sciences”."
  },
  {
    "objectID": "logml2022/projects2022/project2/project2.html#project",
    "href": "logml2022/projects2022/project2/project2.html#project",
    "title": "Machine learning the fine interior",
    "section": "Project",
    "text": "Project\nFirst described in 1983, the Fine interior is a key combinatorial tool in Mirror Symmetry. Broadly speaking, a convex lattice polytope P corresponds to a hypersurface Z in a toric variety. Associate to P is the Fine interior F(P): the rational polytope given by moving all supporting hyperplanes of P in by lattice distance 1. Many geometric properties of Z can be deduced from combinatorial properties of F(P). For example, there exists a unique canonical model of Z if F(P) is non-empty, and the Kodaira dimension is determined by the dimension of F(P). Computing the Fine interior F(P) is computationally challenging and, despite being so important, almost nothing is known about how the combinatorics of P determines the dimension of F(P). This is an area perfect for investigation via Machine Learning.\nIn this project we will explore the classification of certain four-dimension lattice simplices – those containing a single interior lattice point. Each of these 338,752 simplices can be described uniquely by an integer-valued vector (a_0,…,a_4), and in nearly every case we know the Fine interior as a result of brute-force computations totalling many decades of CPU time. We will ask whether Machine Learning can predict the dimension of F(P) directly from the vector (a_0,…,a_4) and, if successful, attempt to understand how the machine is performing this calculation. This should present us with unique insights into the combinatorics of the Fine interior, which in turn will generate a richer understanding of the underlying geometry."
  },
  {
    "objectID": "logml2022/projects2022/project4/index.html",
    "href": "logml2022/projects2022/project4/index.html",
    "title": "Equivariant machine learning for vegetation dynamics",
    "section": "",
    "text": "Soledad Villar is an Assistant Professor in Applied Mathematics and Statistics at Johns Hopkins University. Prior to that she worked with Joan Bruna and Afonso Bandeira at New York University and was affiliated with the Simons Foundation in New York City and UC Berkeley. She received her PhD from UT Austin supervised by Rachel Ward. Her research interests include equivariant machine learning, graph neural networks and mathematical foundations of deep learning. She is also interested in applications to computational biology."
  },
  {
    "objectID": "logml2022/projects2022/project4/index.html#prof-soledad-villar",
    "href": "logml2022/projects2022/project4/index.html#prof-soledad-villar",
    "title": "Equivariant machine learning for vegetation dynamics",
    "section": "",
    "text": "Soledad Villar is an Assistant Professor in Applied Mathematics and Statistics at Johns Hopkins University. Prior to that she worked with Joan Bruna and Afonso Bandeira at New York University and was affiliated with the Simons Foundation in New York City and UC Berkeley. She received her PhD from UT Austin supervised by Rachel Ward. Her research interests include equivariant machine learning, graph neural networks and mathematical foundations of deep learning. She is also interested in applications to computational biology."
  },
  {
    "objectID": "logml2022/projects2022/project4/index.html#project",
    "href": "logml2022/projects2022/project4/index.html#project",
    "title": "Equivariant machine learning for vegetation dynamics",
    "section": "Project",
    "text": "Project\nPredicting vegetation dynamics is a fundamental problem in ecology, especially in the context of climate change. In this project we aim to learn the equations that rule the vegetation dynamics from data with machine learning.\nThe input features to the learning problem include observables such as rainfall, vegetation density, waterabsorbed into the soil, etc. Each of these observables have precise units (liters per day per square meter, grams per square meter, and liters per square meter, respectively for the examples above); therefore the learning should be units-equivariant. This means that if – for instance – we do a transformation where all the input features with units of meters are rescaled to inches, the predictions should transform accordingly. This symmetry is known as unit equivariance and it corresponds to group equivariance by an action by the (non-compact) group of scalings (see Section 3 of [3]).\nA few methods, based on classical dimensional analysis, have been recently proposed to model units-equivariant machine learning problems (see [3, 1]). In this project we propose to combine these ideas with symbolic regression and PDE integrators to learn the equations that produce the dynamics from data. For more information about the prediction problem and how the data is generated refer to Section 7 of [3] or contact Prof. Villar at soledad.villar@jhu.edu.\nReferences [1] Joseph Bakarji, Jared Callaham, Steven L Brunton, and J Nathan Kutz. Dimensionally consistent learning with buckingham Pi.arXiv:2202.04643, 2022. [2] Max Rietkerk, Maarten C Boerlijst, Frank van Langevelde, Reinier HilleRisLambers, Johan van de Kop-pel, Lalit Kumar, Herbert HT Prins, and Andr ́e M de Roos. Self-organization of vegetation in aridecosystems. The American Naturalist, 160(4):524–530, 2002. [3] Soledad Villar, Weichi Yao, David W Hogg, Ben Blum-Smith, and Bianca Dumitrascu. Dimensionless machine learning: Imposing exact units equivariance. arXiv preprint arXiv:2204.00887, 2022."
  },
  {
    "objectID": "logml2022/projects2022/project18/index.html",
    "href": "logml2022/projects2022/project18/index.html",
    "title": "Characterizing generalization and adversarial robustness for set networks",
    "section": "",
    "text": "Tolga Birdal is a prospective assistant professor in the Department of Computing of Imperial College London. Previously, he was a senior Postdoctoral Research Fellow at Stanford University within the Geometric Computing Group of Prof. Leonidas Guibas. Tolga has defended his masters and Ph.D. theses at the Computer Vision Group, Chair for Computer Aided Medical Procedures, Technical University of Munich led by Prof. Nassir Navab. He was also a Doktorand at Siemens AG under supervision of Dr. Slobodan Ilic working on “Geometric Methods for 3D Reconstruction from Large Point Clouds”. His current foci of interest involve geometric machine learning and 3D computer vision. More theoretical work is aimed at investigating and interrogating limits in geometric computing and non-Euclidean inference as well as principles of deep learning and artificial consciousness. Tolga has several publications at the well-respected venues such as NeurIPS, CVPR, ICCV, ECCV, T-PAMI, ICRA, IROS, ICASSP and 3DV. Aside from his academic life, Tolga is an entrepreneur. He has co-founded multiple companies including Befunky, a widely used web-based image editing platform."
  },
  {
    "objectID": "logml2022/projects2022/project18/index.html#prof-tolga-birdal",
    "href": "logml2022/projects2022/project18/index.html#prof-tolga-birdal",
    "title": "Characterizing generalization and adversarial robustness for set networks",
    "section": "",
    "text": "Tolga Birdal is a prospective assistant professor in the Department of Computing of Imperial College London. Previously, he was a senior Postdoctoral Research Fellow at Stanford University within the Geometric Computing Group of Prof. Leonidas Guibas. Tolga has defended his masters and Ph.D. theses at the Computer Vision Group, Chair for Computer Aided Medical Procedures, Technical University of Munich led by Prof. Nassir Navab. He was also a Doktorand at Siemens AG under supervision of Dr. Slobodan Ilic working on “Geometric Methods for 3D Reconstruction from Large Point Clouds”. His current foci of interest involve geometric machine learning and 3D computer vision. More theoretical work is aimed at investigating and interrogating limits in geometric computing and non-Euclidean inference as well as principles of deep learning and artificial consciousness. Tolga has several publications at the well-respected venues such as NeurIPS, CVPR, ICCV, ECCV, T-PAMI, ICRA, IROS, ICASSP and 3DV. Aside from his academic life, Tolga is an entrepreneur. He has co-founded multiple companies including Befunky, a widely used web-based image editing platform."
  },
  {
    "objectID": "logml2022/projects2022/project18/index.html#project",
    "href": "logml2022/projects2022/project18/index.html#project",
    "title": "Characterizing generalization and adversarial robustness for set networks",
    "section": "Project",
    "text": "Project\nDisobeying the classical wisdom of statistical learning theory, modern deep neural networks generalize well even though they typically contain millions of parameters. Hence, a new generation of learning theory has emerged to explain the characteristics of deep neural networks, such as generalization, overfitting or robustness. Empirical or theoretical, most of the works which prosper in bringing insights to the learning phenomenon, focus on convolutional networks, operating on the image domain. However, a vast majority of the computer vision problems involve either graphs or point clouds which live in unstructured domains. The goal of this project is to first empirically understand the generalization character of point cloud networks. To this end, we will deploy a series of state of the art measures. Guided by this empirical study, we aim to theorize how and why deep sets generalize. In particular, we will focus on topological data analysis as a unifying framework."
  },
  {
    "objectID": "logml2022/projects2022/project16/index.html",
    "href": "logml2022/projects2022/project16/index.html",
    "title": "Exploring network medicine principles encoded by graph neural networks",
    "section": "",
    "text": "Kexin Huang is a CS PhD student at Stanford with Prof. Jure Leskovec. His research interest lies in algorithmic challenges arising from real-world biomedicine, with a focus on graph learning and therapeutic discovery."
  },
  {
    "objectID": "logml2022/projects2022/project16/index.html#kexin-huang",
    "href": "logml2022/projects2022/project16/index.html#kexin-huang",
    "title": "Exploring network medicine principles encoded by graph neural networks",
    "section": "",
    "text": "Kexin Huang is a CS PhD student at Stanford with Prof. Jure Leskovec. His research interest lies in algorithmic challenges arising from real-world biomedicine, with a focus on graph learning and therapeutic discovery."
  },
  {
    "objectID": "logml2022/projects2022/project16/index.html#project",
    "href": "logml2022/projects2022/project16/index.html#project",
    "title": "Exploring network medicine principles encoded by graph neural networks",
    "section": "Project",
    "text": "Project\nGraph neural networks (GNNs) have enabled many successful biomedical applications when applying to biological networks - from prioritization of treatments, detection of side effects, to identification of protein function. Because of the impressive performance, it is hypothesized that GNN must capture fundamental biological principles [1] but it is unclear what they are and how GNNs encode these principles. There are decades of research in the field of network medicine [2] where researchers study the organizing principles of biological networks and described hypotheses of governing laws. In this project, we will explore what are these important network medicine principles, how they are manifested in the GNN embedding space. If GNN fails to encode some network medicine principles, then we can motivate a new method to tackle them.\nReferences [1] Li, Michelle M., Kexin Huang, and Marinka Zitnik. Graph Representation Learning in Biomedicine (2021). arXiv: 2104.04883 [2] Barabási, Albert-László, Natali Gulbahce, and Joseph Loscalzo. Network medicine: a network-based approach to human disease. Nature reviews genetics 12.1 (2011): 56-68."
  },
  {
    "objectID": "logml2022/projects2022/project10/index.html",
    "href": "logml2022/projects2022/project10/index.html",
    "title": "Deep functional map",
    "section": "",
    "text": "Abhishek is an incoming DL Research Scientist at Illumina AI lab Cambridge and before that, did his Ph.D. in the Shape Analysis Group at Ecole Polytechnique. His Ph.D. thesis is about Deep Functional Maps for 3D shape analysis. He is very interested in alignment problems in general and their applications in molecular biology."
  },
  {
    "objectID": "logml2022/projects2022/project10/index.html#dr-abhishek-sharma",
    "href": "logml2022/projects2022/project10/index.html#dr-abhishek-sharma",
    "title": "Deep functional map",
    "section": "",
    "text": "Abhishek is an incoming DL Research Scientist at Illumina AI lab Cambridge and before that, did his Ph.D. in the Shape Analysis Group at Ecole Polytechnique. His Ph.D. thesis is about Deep Functional Maps for 3D shape analysis. He is very interested in alignment problems in general and their applications in molecular biology."
  },
  {
    "objectID": "logml2022/projects2022/project10/index.html#project",
    "href": "logml2022/projects2022/project10/index.html#project",
    "title": "Deep functional map",
    "section": "Project",
    "text": "Project\n3D Shape matching is a fundamental problem in computer vision and graphics with significant applications on biological data. In this project, we will take a closer look at Deep Functional Map (DFM) [1,2] paradigm for 3D shape matching. You will become familiar with unsupervised DFM frameworks [2] and investigate a couple of directions less explored in the DFM literature. First, most DFM literature strongly relies on precomputed Laplacian Beltrami (LB) eigenbasis. There have been some recent attempts to learn an embedding [3,4] instead. However, it is not entirely clear in which circumstances learned embedding is more robust and useful than LB eigenbasis. Secondly, we will investigate cycle consistency constraints in DFM that provide a strong regularization by jointly optimizing the maps over a collection of shapes. By the end of the project, you should better understand these topics from both theoretical and practical perspectives.\nReferences [1] Litany et al., Deep Functional Map: Structure prediction for dense shape correspondence, ICCV 2017 [2] Roufosse et al., Unsupervised Deep Learning for 3D shape Matching, ICCV 2019 [3] Marin et al., Correspondence Learning via Linearly Invariant Embedding, Neurips 2020 [4] Sharma & Ovsjanikov, Joint Symmetry Detection and Shape Matching for Non-rigid Point Cloud, arXiv"
  },
  {
    "objectID": "logml2022/projects2022/project21/index.html",
    "href": "logml2022/projects2022/project21/index.html",
    "title": "PDE-inspired sheaf neural networks",
    "section": "",
    "text": "Cristian is a third-year PhD student in the Department of Computer Science, University of Cambridge, supervised by Prof Pietro Liò. His research uses applied Topology and Differential Geometry for understanding and developing Geometric Deep Learning models suitable for problems presenting an underlying combinatorial structure. He is also a Microsoft Research PhD Fellow (2021) and a former research intern at Twitter Cortex, Google Brain and Google X."
  },
  {
    "objectID": "logml2022/projects2022/project21/index.html#cristian-bodnar",
    "href": "logml2022/projects2022/project21/index.html#cristian-bodnar",
    "title": "PDE-inspired sheaf neural networks",
    "section": "",
    "text": "Cristian is a third-year PhD student in the Department of Computer Science, University of Cambridge, supervised by Prof Pietro Liò. His research uses applied Topology and Differential Geometry for understanding and developing Geometric Deep Learning models suitable for problems presenting an underlying combinatorial structure. He is also a Microsoft Research PhD Fellow (2021) and a former research intern at Twitter Cortex, Google Brain and Google X."
  },
  {
    "objectID": "logml2022/projects2022/project21/index.html#project",
    "href": "logml2022/projects2022/project21/index.html#project",
    "title": "PDE-inspired sheaf neural networks",
    "section": "Project",
    "text": "Project\nCellular sheaves [1, 2] equip graphs with a “geometrical” structure by attaching vector spaces and linear maps to their nodes and edges. It turns out that this additional structure can help mitigate some of the well-known problems of Graph Neural Networks. In a recent paper [3], we studied neural diffusion PDEs on sheaves and proved they have many desirable properties, such as better performance in heterophilic settings than GNNs and robust behaviour in the infinite-time limit (i.e. infinitely many layers).\nIn this project, we aim to extend this approach to other sheaf-based PDEs that behave differently from diffusion, leading to novel Sheaf Neural Network models. We will be studying the theoretical properties of these neural PDEs (e.g. Do they minimise some energy? Are the dynamics stable?) with the ultimate goal of building practical models for node classification and regression tasks.\nReferences [1] Jakob Hansen, Robert Ghrist, Toward a Spectral Theory of Cellular Sheaves, Journal of Applied and Computational Topology volume 3, pages 315–358 (2019) [2] Jakob Hansen, Thomas Gebhart, Sheaf Neural Networks, NeurIPS 2020 Workshop on TDA and Beyond [3] Bodnar et al., Neural Sheaf Diffusion: A Topological Perspective on Heterophily and Oversmoothing in GNNs, Preprint 2022"
  },
  {
    "objectID": "logml2022/projects2022/project6/index.html",
    "href": "logml2022/projects2022/project6/index.html",
    "title": "Learning graph rewiring using RL",
    "section": "",
    "text": "Eli Meirom is a research scientist at NVIDIA Research. His research interests include machine learning on graphs and reinforcement learning. He completed his Ph.D at the Technion, Israel. Before joining NVIDIA Research, he co-founded HearWize and Amooka-AI, the latter was acquired by Ford in 2018, where he worked as a senior research scientist, developing Ford’s autonomous vehicles driving policy."
  },
  {
    "objectID": "logml2022/projects2022/project6/index.html#dr-eli-meirom",
    "href": "logml2022/projects2022/project6/index.html#dr-eli-meirom",
    "title": "Learning graph rewiring using RL",
    "section": "",
    "text": "Eli Meirom is a research scientist at NVIDIA Research. His research interests include machine learning on graphs and reinforcement learning. He completed his Ph.D at the Technion, Israel. Before joining NVIDIA Research, he co-founded HearWize and Amooka-AI, the latter was acquired by Ford in 2018, where he worked as a senior research scientist, developing Ford’s autonomous vehicles driving policy."
  },
  {
    "objectID": "logml2022/projects2022/project6/index.html#project",
    "href": "logml2022/projects2022/project6/index.html#project",
    "title": "Learning graph rewiring using RL",
    "section": "Project",
    "text": "Project\nMost GNNs are based on the concept of message passing, which is by itself based on information diffusion. In diffusion dynamics, key information lies in closer objects, and distant nodes’ effect is decimated [1]. However, it is not clear that the topological graph structure must dictate the information transfer on the graph. In fact, in many cases, such as combinatorial optimization problems, nodes and edges that are distant from a node may have a major impact on the node’s value or class. To that end, graph rewiring allows adding edges, nodes, or other structures in order to assist information transfer. In practice, it decouples the information graph from the topological (input) graph. In this project, we will investigate how we can (meta) learn to build better information graphs using RL. Specifically, our agent will learn how to modify (add/remove) edges, i.e., perform graph rewiring, to improve learning. Our goal is to publish the results of this project in a top ML conference.\nReferences [1] Understanding over-squashing and bottlenecks on graphs via curvature, Topping et. al., 2022. [2] On the bottleneck of graph neural networks and its practical implications, Alon et. al., 2020"
  },
  {
    "objectID": "logml2022/projects2022/project8/index.html",
    "href": "logml2022/projects2022/project8/index.html",
    "title": "Graph-rewiring for GNNs from a geometric perspective",
    "section": "",
    "text": "Francesco is an ML Researcher currently working at Twitter with Michael Bronstein on geometry-inspired ideas applied in the context of graph machine learning. He completed a PhD in Riemannian geometry focussing on singularity formation of symmetric (cohomogeneity-1) Ricci flows at UCL under the supervision of Jason Lotay."
  },
  {
    "objectID": "logml2022/projects2022/project8/index.html#dr-francesco-di-giovanni",
    "href": "logml2022/projects2022/project8/index.html#dr-francesco-di-giovanni",
    "title": "Graph-rewiring for GNNs from a geometric perspective",
    "section": "",
    "text": "Francesco is an ML Researcher currently working at Twitter with Michael Bronstein on geometry-inspired ideas applied in the context of graph machine learning. He completed a PhD in Riemannian geometry focussing on singularity formation of symmetric (cohomogeneity-1) Ricci flows at UCL under the supervision of Jason Lotay."
  },
  {
    "objectID": "logml2022/projects2022/project8/index.html#project",
    "href": "logml2022/projects2022/project8/index.html#project",
    "title": "Graph-rewiring for GNNs from a geometric perspective",
    "section": "Project",
    "text": "Project\nIn this project we will investigate possible ways to formalize and understand the notion of graph-rewiring in the context of Graph Neural Networks from a geometric perspective. In certain regimes - existence of long-range dependencies that are crucial for the classification task or heterophily of the label (feature) distribution - the graph structure is known not to be beneficial and, in some cases, even harmful. However, a clear understanding of how classical GNNs behave with respect to specific topological transformations is still missing. Providing a clearer picture in this regard is intimately connected with the problem of stability of GNNs with respect to topological perturbations and might also shed some light on tackling expressivity from a different angle. The main goal of the project consists in studying notions of distances among graphs and associated classes of transformations that would allow us to better tackle the problem of graph-rewiring and analyse theoretically its effects on GNNs."
  },
  {
    "objectID": "logml2022/projects2022/project7/index.html",
    "href": "logml2022/projects2022/project7/index.html",
    "title": "Contrastive learning",
    "section": "",
    "text": "Melanie is a Hooke Research Fellow at the University of Oxford. Her research focuses on the mathematical foundations of Machine Learning and Data Science, with a special interest in understanding the geometric features of data and in developing machine learning methods that utilize such geometric knowledge. She received her PhD from Princeton University under the supervision of Charles Fefferman. She held visiting positions at MIT’s Laboratory for Information and Decision Systems and the Simons Institute in Berkeley and interned in the research labs of Facebook, Google and Microsoft. In addition to her academic work, she is the Chief Scientist of the Legal AI start up Claudius Legal Intelligence, where she leads a team of researchers in developing Trustworthy Machine Learning tools for legal analytics. Her awards include Princeton’s C.V. Starr Fellowship, a Simons-Berkeley Fellowship, a selection as Rising Star in EECS and an Alan Turing Post-Doctoral Enrichment Award."
  },
  {
    "objectID": "logml2022/projects2022/project7/index.html#dr-melanie-weber",
    "href": "logml2022/projects2022/project7/index.html#dr-melanie-weber",
    "title": "Contrastive learning",
    "section": "",
    "text": "Melanie is a Hooke Research Fellow at the University of Oxford. Her research focuses on the mathematical foundations of Machine Learning and Data Science, with a special interest in understanding the geometric features of data and in developing machine learning methods that utilize such geometric knowledge. She received her PhD from Princeton University under the supervision of Charles Fefferman. She held visiting positions at MIT’s Laboratory for Information and Decision Systems and the Simons Institute in Berkeley and interned in the research labs of Facebook, Google and Microsoft. In addition to her academic work, she is the Chief Scientist of the Legal AI start up Claudius Legal Intelligence, where she leads a team of researchers in developing Trustworthy Machine Learning tools for legal analytics. Her awards include Princeton’s C.V. Starr Fellowship, a Simons-Berkeley Fellowship, a selection as Rising Star in EECS and an Alan Turing Post-Doctoral Enrichment Award."
  },
  {
    "objectID": "logml2022/projects2022/project7/index.html#project",
    "href": "logml2022/projects2022/project7/index.html#project",
    "title": "Contrastive learning",
    "section": "Project",
    "text": "Project\nContrastive learning seeks to train a representation function that encodes the similarity structure in a data set based on pairs of positive samples (similar data points) and negative samples (dissimilar data points). This project will investigate ways of incorporating geometric information, such as equivariances or symmetries, into Contrastive Learning approaches. Depending on the interests and expertise of the group, both computational and theoretical avenues of investigation may be pursued."
  },
  {
    "objectID": "logml2022/projects2022/project15/index.html",
    "href": "logml2022/projects2022/project15/index.html",
    "title": "Learning non-geodesic submanifolds",
    "section": "",
    "text": "Nina Miolane is an Assistant Professor at the University of California, Santa Barbara, and an Affiliate Researcher with the SLAC National Laboratory, Stanford. She received her M.S. in Mathematics from Ecole Polytechnique (France) & Theoretical Physics from Imperial College (UK), and her Ph.D. in Computer Science from INRIA (France) in collaboration with Stanford University (US). She was a postdoc and lecturer in Statistics at Stanford University, and worked as a deep learning software engineer in the Silicon Valley. At UCSB, Nina directs the BioShape Lab, whose research investigates how the shapes of proteins, cells, and organs correlate with physiological conditions and pathologies. Her team co-develops Geomstats, a software for machine learning on geometric data such as biological shape data. The BioShape Lab was awarded a NIH R01 grant on Molecular Shape Reconstruction and the NSF SCALE MoDL grant on Mathematical Foundations of Deep Learning. Nina was the recipient of the L’Oréal-Unesco for Women in Science Award, of the Regents’ Junior Faculty Fellowship Award. In her free time, Nina pilots single-engine airplanes in the Californian skies."
  },
  {
    "objectID": "logml2022/projects2022/project15/index.html#prof-nina-miolane",
    "href": "logml2022/projects2022/project15/index.html#prof-nina-miolane",
    "title": "Learning non-geodesic submanifolds",
    "section": "",
    "text": "Nina Miolane is an Assistant Professor at the University of California, Santa Barbara, and an Affiliate Researcher with the SLAC National Laboratory, Stanford. She received her M.S. in Mathematics from Ecole Polytechnique (France) & Theoretical Physics from Imperial College (UK), and her Ph.D. in Computer Science from INRIA (France) in collaboration with Stanford University (US). She was a postdoc and lecturer in Statistics at Stanford University, and worked as a deep learning software engineer in the Silicon Valley. At UCSB, Nina directs the BioShape Lab, whose research investigates how the shapes of proteins, cells, and organs correlate with physiological conditions and pathologies. Her team co-develops Geomstats, a software for machine learning on geometric data such as biological shape data. The BioShape Lab was awarded a NIH R01 grant on Molecular Shape Reconstruction and the NSF SCALE MoDL grant on Mathematical Foundations of Deep Learning. Nina was the recipient of the L’Oréal-Unesco for Women in Science Award, of the Regents’ Junior Faculty Fellowship Award. In her free time, Nina pilots single-engine airplanes in the Californian skies."
  },
  {
    "objectID": "logml2022/projects2022/project15/index.html#project",
    "href": "logml2022/projects2022/project15/index.html#project",
    "title": "Learning non-geodesic submanifolds",
    "section": "Project",
    "text": "Project\nRepresentation learning aims to transform data x into a lower-dimensional variable z designed to be more efficient for any downstream machine learning task, such as classification. In this project, you will tackle representation learning for manifold-valued data x, and specifically delve into non-geodesic submanifold learning with algorithms of curve fitting and variational autoencoders (rVAE) on manifolds. You will contribute to the open-source package Geomstats by implementing a representation learning module which will unify and contrast the aforementioned methods.\nReferences () Miolane et al. Geomstats: A Python Package for Riemannian Geometry in Machine Learning (JMLR 2020). () Miolane, Holmes. Learning Weighted Submanifolds with Variational Autoencoders and Riemannian Variational Autoencoders (CVPR 2020)."
  },
  {
    "objectID": "logml2022/projects2022/project13/index.html",
    "href": "logml2022/projects2022/project13/index.html",
    "title": "Differential geometry for representation learning",
    "section": "",
    "text": "Georgios is a tenure-track assistant professor in the Section for Cognitive Systems at the Technical University of Denmark (DTU), from where he received his PhD in 2019 under the supervision of Søren Hauberg and Lars Kai Hansen. In between, he was a postdoctoral researcher at the Empirical Inference department at the Max Planck Institute for Intelligent Systems at Tübingen working with Bernhard Schölkopf. His research is mainly focused on geometrical methods in machine learning. In particular, he develops methods that learn the geometric structure of the data based on latent variable models, as well as the associated techniques that enable the analysis in these nonlinear spaces."
  },
  {
    "objectID": "logml2022/projects2022/project13/index.html#prof-georgios-arvanitidis",
    "href": "logml2022/projects2022/project13/index.html#prof-georgios-arvanitidis",
    "title": "Differential geometry for representation learning",
    "section": "",
    "text": "Georgios is a tenure-track assistant professor in the Section for Cognitive Systems at the Technical University of Denmark (DTU), from where he received his PhD in 2019 under the supervision of Søren Hauberg and Lars Kai Hansen. In between, he was a postdoctoral researcher at the Empirical Inference department at the Max Planck Institute for Intelligent Systems at Tübingen working with Bernhard Schölkopf. His research is mainly focused on geometrical methods in machine learning. In particular, he develops methods that learn the geometric structure of the data based on latent variable models, as well as the associated techniques that enable the analysis in these nonlinear spaces."
  },
  {
    "objectID": "logml2022/projects2022/project13/index.html#project",
    "href": "logml2022/projects2022/project13/index.html#project",
    "title": "Differential geometry for representation learning",
    "section": "Project",
    "text": "Project\nA common hypothesis in machine learning is that the data lie near a low dimensional manifold which is embedded in a high dimensional ambient space. This implies that shortest paths between points should respect the underlying geometric structure. In practice, we can capture the geometry of a data manifold through a Riemannian metric in the latent space of a stochastic generative model, relying on meaningful uncertainty estimation for the generative process. This enables us to compute identifiable distances, since the length of the shortest path remains invariant under re-parametrizations of the latent space. Consequently, we are able to study the learned latent representations beyond the classic Euclidean perspective.\nIn this project you will develop methods to learn Riemannian metrics in the latent space of deep generative models. You will then use the learned metrics for computing shortest paths in the representation space and for fitting a statistical model that respects the learned nonlinear geometry of the data.\nReferences () Latent Space Oddity: on the Curvature of Deep Generative Models, Georgios Arvanitidis, Lars Kai Hansen, Søren Hauberg, International Conference on Learning Representations (ICLR), 2018. () A prior-based approximate latent Riemannian metric, Georgios Arvanitidis, Bogdan Georgiev, Bernhard Schölkopf, International Conference on Artificial Intelligence and Statistics (AISTATS), 2022. () Fast and robust shortest paths on manifolds learned from data, Georgios Arvanitidis, Søren Hauberg, Philipp Hennig, Michael Tiemann, International Conference on Artificial Intelligence and Statistics (AISTATS), 2019. () A locally adaptive normal distribution, Georgios Arvanitidis, Lars Kai Hansen, Søren Hauberg, Advances in Neural Information Processing Systems (NeurIPS), 2016."
  },
  {
    "objectID": "logml2022/speakers2022/keynote/justin-solomon.html",
    "href": "logml2022/speakers2022/keynote/justin-solomon.html",
    "title": "Justin Solomon",
    "section": "",
    "text": "Home\n    Speakers\n    Justin Solomon\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nJustin Solomon is an associate professor of Electrical Engineering and Computer Science at MIT. He leads the Geometric Data Processing Group in the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL), which studies problems at the intersection of geometry, optimization, and applications."
  },
  {
    "objectID": "logml2022/speakers2022/other/smita-krishnaswamy.html",
    "href": "logml2022/speakers2022/other/smita-krishnaswamy.html",
    "title": "Smita Krishnaswamy",
    "section": "",
    "text": "Home\n    Speakers\n    Smita Krishnaswamy\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nSmita Krishnaswamy is an Associate Professor in the departments of Computer Science (SEAS) and Genetics (YSM). She is part of the programs in Applied Mathematics, Computational Biology & Bioinformatics and Interdisciplinary Neuroscience. She is also affiliated with the Yale Center for Biomedical Data Science, Yale Cancer Center, Wu-Tsai Institute. Smita’s lab works at the intersection of computer science, applied math, computational biology, and signal processing to develop representation-learning and deep learning methods that enable exploratory analysis, scientific inference and prediction from big biomedical datasets. She has applied her methods on datasets generated from single-cell sequencing, structural biology, biomedical imaging, brain activity recording, electronic health records on a wide variety of biological, cellular, and disease systems. Her techniques generally incorporate mathematical priors from graph spectral theory, manifold learning, signal processing, and topology into machine learning and deep learning frameworks, in order to denoise and model the underlying systems faithfully for predictive insight. Currently her methods are being widely used for data denoising, visualization, generative modeling, dynamics. modeling, comparative analysis and domain transfer.\nSmita teaches several courses including: Deep Learning Theory and Applications, Unsupervised learning, and Geometric and Topological Methods in Machine Learning. Prior to joining Yale, Smita completed her postdoctoral training at Columbia University in the systems biology department where she focused on learning computational models of cellular signaling from single-cell mass cytometry data. She obtained her Ph.D. from EECS department at University of Michigan where her research focused on algorithms for automated synthesis and probabilistic verification of nanoscale logic circuits. Following her time in Michigan, Smita spent 2 years at IBM’s TJ Watson Research Center as a researcher in the systems division where she worked on automated bug finding and error correction in logic. Smita’s work over the years has won several awards including the NSF CAREER Award, Sloan Faculty Fellowship, and Blavatnik fund for Innovation."
  },
  {
    "objectID": "logml2022/speakers2022/other/alice-le-brigant.html",
    "href": "logml2022/speakers2022/other/alice-le-brigant.html",
    "title": "Alice Le Brigant",
    "section": "",
    "text": "Home\n    Speakers\n    Alice Le Brigant\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nAlice Le Brigant is an Assistant Professor in the Applied Mathematics team SAMM at University Paris 1 Pantheon Sorbonne. Previously, she was a Postdoctoral Fellow at the French Civil Aviation School, in collaboration with the Toulouse Mathematics Institute. She obtained her PhD in Applied Mathematics from the University of Bordeaux in 2017. Her research interests are at the interface of statistics and applied Riemannian geometry."
  },
  {
    "objectID": "speaker.html",
    "href": "speaker.html",
    "title": "Speakers",
    "section": "",
    "text": "Keynote speakers\n\n\n\n\n\n\n\n\n\n\nHeather Harrington\n\n\nMPI-CBG Dresden\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nSpeakers\n\n\n\n\n\n\n\n\n\n\nBastian Rieck\n\n\nHelmholtz Munich\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people/team/daniel.html",
    "href": "people/team/daniel.html",
    "title": "Daniel Platt",
    "section": "",
    "text": "Daniel is a Chapman-Schmidt fellow at Imperial College London working on differential geometry and machine learning. He obtained his PhD from Imperial College London for his thesis on gauge theory in dimension 7 on problems that are motivated by string theory. Because of this, he is interested in big datasets of Calabi-Yau manifolds. Apart from this, Lie groups and  group actions are important for the kind of geometry that he studies and he is also interested in applications of differential geometry to group invariant machine learning."
  },
  {
    "objectID": "people/team/simone.html",
    "href": "people/team/simone.html",
    "title": "Simone Foti",
    "section": "",
    "text": "Simone is a Postdoctoral Research Associate in the Department of Computing at Imperial College London. Previously, he obtained a PhD at the University College London (UCL) by researching “Latent Disentanglement for the Analysis and Generation of Digital Human Shapes”, which found applications in computer vision, graphics and plastic surgery. During the PhD, he gained industry experience as a Research Scientist Intern at Disney Research Studios (Zurich) and Adobe Research (Paris). Simone also obtained an MRes in Medical Imaging from UCL, an MSc in Biomedical Engineering at Politecnico di Milano, and a BSc in Computer Science at the University of Trieste. Currently, his research interest lies at the intersection of geometric deep learning, computer graphics, and computer vision. Through his research, Simone strives to understand how to solve problems in non-Euclidean domains such as graphs and meshes. He is currently working on multiple projects ranging from shape and texture generation, latent disentanglement, mesh sampling, and 3D reconstruction."
  },
  {
    "objectID": "people/team/arne.html",
    "href": "people/team/arne.html",
    "title": "Arne Wolf",
    "section": "",
    "text": "Arne is doing his PhD at Imperial College London (LSGNT) under the supervision of Anthea Monod. He is working on adapting ideas from geometry and topology to discrete settings. Concretely, he is interested in different tools of topological data analysis like persistent homology and Hodge Laplacians for simplicial complexes and other discrete structures."
  },
  {
    "objectID": "people/advisors/pietro.html",
    "href": "people/advisors/pietro.html",
    "title": "Pietro Liò",
    "section": "",
    "text": "Home\n    Speakers\n    Pietro Liò\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nI am Full Professor at the department of Computer Science and Technology of the University of Cambridge and I am a member of the Artificial Intelligence group. I am a member of the Cambridge Centre for AI in Medicine. My research interest focuses on developing Artificial Intelligence and Computational Biology models to understand diseases complexity and address personalised and precision medicine. Current focus is on Graph Neural Network modeling.\nI have a MA from Cambridge, a PhD in Complex Systems and Non Linear Dynamics (School of Informatics, dept of Engineering of the University of Firenze, Italy) and a PhD in (Theoretical) Genetics (University of Pavia, Italy). Other Affliations: I am member of CAMBRIDGE CENTRE FOR AI IN MEDICINE - the Integrate Cancer Medicine Institute, the committee of MPhil in Computational Biology (Stakeholder Group for the CCBI) , steering committee of Cambridge BIG data, VPH-UK (Virtual Physiological Human), Fellow and member of the Council of Clare Hall College , I am member of Ellis, the European Lab for Learning & Intelligent Systems, I am member of the Academia Europaea; I am listed in www.topitalianscientists.org/Top_italian_scientists_VIA-Academy.aspx"
  },
  {
    "objectID": "people.html",
    "href": "people.html",
    "title": "LOGML 2025",
    "section": "",
    "text": "Home\n    People"
  },
  {
    "objectID": "people.html#organising-committee",
    "href": "people.html#organising-committee",
    "title": "LOGML 2025",
    "section": "Organising Committee",
    "text": "Organising Committee\n\n\n\n\n\n\n\n\n\n\nArne Wolf\n\n\nImperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\nDaniel Platt\n\n\nImperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\nPragya Singh\n\n\nImperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimone Foti\n\n\nImperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\nValentina Giunchiglia\n\n\nImperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\nZhengang Zhong\n\n\nUniversity of Warwick\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people.html#scientific-advisory-committee",
    "href": "people.html#scientific-advisory-committee",
    "title": "LOGML 2025",
    "section": "Scientific Advisory Committee",
    "text": "Scientific Advisory Committee\n\n\n\n\n\n\n\n\n\n\nAnthea Monod\n\n\nImperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\nMichael Bronstein\n\n\nUniversity of Oxford, Twitter\n\n\n\n\n\n\n\n\n\n\n\n\n\nPietro Liò\n\n\nUniversity of Cambridge\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "policies.html",
    "href": "policies.html",
    "title": "Policies",
    "section": "",
    "text": "The Organisers acknowledge that the workforce in STEM fields (including mathematics and computer science) does not reflect the diversity of the general population, and the importance of role models in encouraging talents to pursue careers in STEM regardless of their background. ​ This document outlines the steps the Organising Committee is taking in order to make the event accessible and welcoming to a diverse crowd, independently of ethnicity, gender, sexual orientation, religion, or disability.\nWe encourage anyone with concerns regarding accessibility, diversity, or inclusion to contact the Organising Committee.\nNote: this document is not final and will be subject to changes by the time of the event."
  },
  {
    "objectID": "policies.html#diversity-and-inclusion-policy",
    "href": "policies.html#diversity-and-inclusion-policy",
    "title": "Policies",
    "section": "",
    "text": "The Organisers acknowledge that the workforce in STEM fields (including mathematics and computer science) does not reflect the diversity of the general population, and the importance of role models in encouraging talents to pursue careers in STEM regardless of their background. ​ This document outlines the steps the Organising Committee is taking in order to make the event accessible and welcoming to a diverse crowd, independently of ethnicity, gender, sexual orientation, religion, or disability.\nWe encourage anyone with concerns regarding accessibility, diversity, or inclusion to contact the Organising Committee.\nNote: this document is not final and will be subject to changes by the time of the event."
  },
  {
    "objectID": "policies.html#speaker-and-mentors",
    "href": "policies.html#speaker-and-mentors",
    "title": "Policies",
    "section": "Speaker and mentors",
    "text": "Speaker and mentors\n​ The Organisers are committed to inviting speakers and mentors representative of all people. This includes ensuring a diverse representation of research subjects and fields, gender, and ethnic backgrounds among our speakers and mentors.\nIn particular, we attempt to counteract the underrepresentation of women in STEM by making sure that at least 20% of speakers and mentors are non-male. We also strongly encourage applications by non-men and minorities and will reserve 20% of the participant spots for underrepresented groups.\nThe Organisers will, to the best of their abilities, ensure traditionally under-represented groups in AI and STEM are aware of the event and encouraged to apply, e.g., by reaching out to university and department diversity officers."
  },
  {
    "objectID": "policies.html#disabilities",
    "href": "policies.html#disabilities",
    "title": "Policies",
    "section": "Disabilities",
    "text": "Disabilities\n​ The Organisers are committed to making the event accessible to people with disabilities. We will strive to provide accessible versions of the content of the summer school (e.g. plain-text pages with links to the videoconference meetings). We also have funding available for individualised assistance.\nPlease reach out to the organisers if you require help with accessibility."
  },
  {
    "objectID": "policies.html#caring-responsibilities",
    "href": "policies.html#caring-responsibilities",
    "title": "Policies",
    "section": "Caring responsibilities",
    "text": "Caring responsibilities\n​ Financial support will be available to relieve participants, mentors or speakers of caring responsibilities that might interfere with their participation in the Summer School. Applicants can state in the application form that they request financial assistance to cover caring expenses in order to attend the event. Attendance will also be provided for free to legal carers of participants if requested in the application form.\nPlease reach out to the organisers if you require help with caring responsibilities."
  },
  {
    "objectID": "policies.html#financial-support",
    "href": "policies.html#financial-support",
    "title": "Policies",
    "section": "Financial support",
    "text": "Financial support\n​ Limited financial support is available. Preference will be given to applicants who cannot obtain financial assistance by other means (e.g. from their university, department, or advisors). Applicants will have the possibility to apply for support in the application form. ​"
  },
  {
    "objectID": "policies.html#code-of-conduct",
    "href": "policies.html#code-of-conduct",
    "title": "Policies",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nThis document states the Code of Conduct of the LOGML 2025 Workshop (hereafter referred to as “the Event”).\nNote: This document is not final and will be subject to changes by the time of the event.\n\nIntroduction\n​ The Organisers are committed to making this conference productive and enjoyable for everyone, regardless of sex, sexual orientation, disability, age, physical appearance, body size, ethnicity, nationality, or religion. For this reason, we will not tolerate harassment of participants in any form, and will implement policies to promote an inclusive environment and support vectors for attendees who require assistance to participate in the workshop.\nAs part of the registration process, all attendees (i.e. all participants, mentors, speakers, any volunteer helpers, and any other person attending the event) are required to agree to adhere to this Code of Conduct. In particular, Sponsors are equally subject to this Code of Conduct, and may not use images, activities, or any other materials that are of sexual, racial, or otherwise offensive nature. This code applies both to official Sponsors as well as any organisation that uses the Workshop name, image, or identity as part of its activities at or around the Workshop.\n\n\nCode of Conduct\n​ Attendees are required to behave professionally. Harassment and sexist, racist, or exclusionary comments or jokes and any behaviour pertaining to bullying are not appropriate. Compliance with these principles and, in particular, with the present Code of Conduct is expected not just for virtual meetings, but also for interactions on social media.\nHarassment includes sustained disruption of talks or other events, sustained unwanted contact, sexual attention or innuendo, deliberate intimidation, pressuring, stalking, and photography or recording of an individual without consent. It also includes offensive or belittling comments related to gender, sexual orientation, disability, age, physical appearance, body size, ethnicity, or religion.\nAll communication should be appropriate for a professional audience including people of many different backgrounds. Sexual language and imagery are not appropriate."
  },
  {
    "objectID": "policies.html#incident-reporting-and-resolution",
    "href": "policies.html#incident-reporting-and-resolution",
    "title": "Policies",
    "section": "Incident reporting and resolution",
    "text": "Incident reporting and resolution\n​ If you observe someone making you or anyone else feel unsafe or unwelcome, please tell them so, and remind them of the Code of Conduct. Non-confrontational alternatives can be to create a distraction, or to refer the person to an Organiser.\nWhether or not you addressed the person yourself, report incidents as soon as possible to a member of the Organising Committee - see contact information below. The Committee is committed to addressing and resolving the matter to the best of their abilities and within the best possible delay. We are prepared to help participants in contacting relevant help services, and to escort them to a safe location.\nPlease use the following contact information, and explain what happened and who was involved so that we can investigate: logml.committee@gmail.com.\nThe phone number of the point of contact will be provided to all attendees as part of the electronic materials during the Event. ​ ## Sanctions ​ When someone is asked to stop any behaviour that violates the Code of Conduct, they are expected to comply immediately. In response to behaviour deemed inappropriate by the Organising Committee (e.g. sexual content, rudeness, unprofessional), the Organisers may take any action they deem appropriate, including warning the person in question, asking them to leave or banning them from the event, or removing them from a mailing list.\nSpecific actions may include but are not limited to:\n\nasking the person to cease the inappropriate behaviour, and warning them that any further reports will result in other sanctions\nrequiring the person makes conciliatory efforts that may include an apology, informal mediation, or other steps intended to facilitate restoration of relationships\nrequiring the person avoids any interaction with another person for the remainder of the event\nearly termination of a talk that violates the policy\nnot publishing the video or slides of a talk that violates the policy\nnot allowing a speaker who violated the policy to give (further) talks at the event\nimmediately ending any event responsibilities or privileges held\nrequiring that the person immediately leave the event and not return\nblocking the person on social media platforms\nbanning the person from future events\npublishing an anonymous account of the harassment\nreporting the incident to the person’s employer\nreporting the incident to competent authorities\n\nThe Organisers reserve the right to remove participants who fail to comply with the Code of Conduct if the situation requires it. The Organisers may decline to refund any cost incurred with attendance (including, e.g., registration fee) to participants who have been expelled for breaching the Code of Conduct.\nThank you for your participation in the LOGML community, and your efforts to keep our conference welcoming, respectful, and friendly for all participants!"
  },
  {
    "objectID": "policies.html#data-protection-statement",
    "href": "policies.html#data-protection-statement",
    "title": "Policies",
    "section": "Data Protection Statement",
    "text": "Data Protection Statement\nWho is collecting your data?\nThe London Geometry and Machine Learning Organisation Committee (LOGML) is collecting your data. The individual members of our initiative are listed on the “Organisers” page. Submitted data are controlled by LOGML. Data is shared with third parties, if necessary, for the organisation of the workshop. In particular, contact data of successful applicants is shared with project mentors, and participants’ CVs are shared with sponsors if the participant has opted-in for this during the application process.\nWhy will we be using your personal data?\nWe are using your personal data to make admissions decision for the summer school and to communicate with you about the summer school, for example to share access information or schedule changes. We share your contact information with the mentor of the project you work on during the workshop in order to enable them to contact you about matters regarding the project you will participate in.\nWhat are the categories of personal data concerned?\nWe only collect personal data when you share them directly with us through the application form, personally, via email. The categories of personal data concerned is the category of data that you are sharing directly with us.\nWhat is the legal justification for processing your data?\nWe are providing a task in the public interest linked to the core purpose “Education”.\nFor how long will we keep your data?\nWe will keep data of both successful and unsuccessful applicants for six months after the workshop.\nWho else might receive your data?\nIf you explicitly advise us to do this during the application process, we will share your CV with our sponsors who process your data according to their respective privacy policies. Our sponsors are listed on the “Sponsors” page of our website."
  },
  {
    "objectID": "policies.html#information-about-your-rights",
    "href": "policies.html#information-about-your-rights",
    "title": "Policies",
    "section": "Information about your rights",
    "text": "Information about your rights\nUnder certain circumstances, you may have the following rights in relation to your personal data:\nRight 1: A right to access personal data held by us about you.\nRight 2: A right to require us to rectify any inaccurate personal data held by us about you.\nRight 3: A right to require us to erase personal data held by us about you. This right will only apply where, for example, we no longer need to use the personal data to achieve the purpose we collected it for; or where you withdraw your consent if we are using your personal data based on your consent; or where you object to the way we process your data (in line with Right 6 below).\nRight 4: A right to restrict our processing of personal data held by us about you. This right will only apply where, for example, you dispute the accuracy of the personal data held by us; or where you would have the right to require us to erase the personal data but would prefer that our processing is restricted instead; or where we no longer need to use the personal data to achieve the purpose we collected it for, but we require the data for the purposes of dealing with legal claims.\nRight 5: A right to receive personal data, which you have provided to us, in a structured, commonly used and machine readable format. You also have the right to require us to transfer this personal data to another organisation.\nRight 6: A right to object to our processing of personal data held by us about you.\nRight 7: A right to withdraw your consent, where we are relying on it to use your personal data.\nRight 8: A right to ask us not to use information about you in a way that allows computers to make decisions about you and ask us to stop.\nIf you want to exercise these rights, please contact us at logml.committee@gmail.com.\nYou can read more about your rights in the Guidance from the UK Information Commissioner’s Office (ICO). You also have the right to make a complaint to the ICO about how we use your personal data. You can do this by contacting the ICO via their website or by calling 0303 123 1113."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Calabi-Yau Metrics with U(1)-invariant Neural Networks\n\n\n\n\n\n\n\n\n\n\n\nYidi Qi\n\n\n\n\n\n\n\n\n\n\n\n\nExploiting Graph Neural Networks for Prescriptive maintenance of CERN’s technical infrastructure\n\n\n\n\n\n\n\n\n\n\n\nLorenzo Giusti\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating Calabi-Yau Manifolds with Machine Learning\n\n\n\n\n\n\n\n\n\n\n\nElli Heyes\n\n\n\n\n\n\n\n\n\n\n\n\nGeometric GNNs for particle level reconstruction\n\n\n\n\n\n\n\n\n\n\n\nDolores Garcia\n\n\n\n\n\n\n\n\n\n\n\n\nGeometry for Distribution Learning\n\n\n\n\n\n\n\n\n\n\n\nZhengang Zhong\n\n\n\n\n\n\n\n\n\n\n\n\nGraph Learning for Uplift Modeling\n\n\n\n\n\n\n\n\n\n\n\nGeorge Panagopoulos\n\n\n\n\n\n\n\n\n\n\n\n\nInvariantly learning terminal singularities\n\n\n\n\n\n\n\n\n\n\n\nSara Veneziale\n\n\n\n\n\n\n\n\n\n\n\n\nLearning to predict optimal solution value for NP-Hard Combinatorial problems\n\n\n\n\n\n\n\n\n\n\n\nSahil Manchanda\n\n\n\n\n\n\n\n\n\n\n\n\nMatching graphs with spatial constrains\n\n\n\n\n\n\n\n\n\n\n\nAnna Calissano\n\n\n\n\n\n\n\n\n\n\n\n\nMixed Curvature Graph Neural Networks\n\n\n\n\n\n\n\n\n\n\n\nRishi Sonthalia\n\n\n\n\n\n\n\n\n\n\n\n\nMultimodal Protein Representation Learning\n\n\n\n\n\n\n\n\n\n\n\nMichail Chatzianastasis\n\n\n\n\n\n\n\n\n\n\n\n\nOn the Geometry of Relative Representations\n\n\n\n\n\n\n\n\n\n\n\nMarco Fumero\n\n\n\n\n\n\n\n\n\n\n\n\nPowerful Graph Neural Networks for Relational Databases\n\n\n\n\n\n\n\n\n\n\n\nJoshua Robinson\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting the pathogenicity of a (missense) mutation\n\n\n\n\n\n\n\n\n\n\n\nAbhishek Sharma\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-supervised learning for Topological Neural Networks\n\n\n\n\n\n\n\n\n\n\n\nClaudio Battiloro\n\n\n\n\n\n\n\n\n\n\n\n\nSpectral Signed GNNs for fMRI Connectomes\n\n\n\n\n\n\n\n\n\n\n\nRahul Singh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/Rishi-Sonthalia.html",
    "href": "projects/Rishi-Sonthalia.html",
    "title": "Mixed Curvature Graph Neural Networks",
    "section": "",
    "text": "Rishi Sonthalia is a Hedrick Assistant Adjunct Professor in the Math department at UCLA. He completed his Ph.D. from the University of Michigan where he won the Peter Smereka prize for the best Applied Math thesis. Rishi’s research interested is in mathematics for machine learning with a special focus on generalization, optimization, and the role of geometry."
  },
  {
    "objectID": "projects/Rishi-Sonthalia.html#rishi-sonthalia",
    "href": "projects/Rishi-Sonthalia.html#rishi-sonthalia",
    "title": "Mixed Curvature Graph Neural Networks",
    "section": "",
    "text": "Rishi Sonthalia is a Hedrick Assistant Adjunct Professor in the Math department at UCLA. He completed his Ph.D. from the University of Michigan where he won the Peter Smereka prize for the best Applied Math thesis. Rishi’s research interested is in mathematics for machine learning with a special focus on generalization, optimization, and the role of geometry."
  },
  {
    "objectID": "projects/Rishi-Sonthalia.html#project",
    "href": "projects/Rishi-Sonthalia.html#project",
    "title": "Mixed Curvature Graph Neural Networks",
    "section": "Project",
    "text": "Project\nRecent work has shown that hyperbolic geometry can be very useful in improving the performance of neural networks, including graph neural networks. There has also been recent work suggesting that using the correct geometry (based on curvature) can be used to alleviate oversquashing in GNNs. Hence it is currently of relevance to understand the performance of GNNs that use mix curvature geometries. For this a variety of different models have been proposed - product manifolds (Gu et al. 2019), hierarchical hyperbolic spaces (Sonthalia et al. 2022), the space of positive definite matrices (Lopez et al. 2021), as well neural networks that intertwine standard Euclidean and hyperbolic layers (Cui at al. 2022)."
  },
  {
    "objectID": "projects/Michail-Chatzianastasis.html",
    "href": "projects/Michail-Chatzianastasis.html",
    "title": "Multimodal Protein Representation Learning",
    "section": "",
    "text": "Michail Chatzianastasis is a PhD student at École Polytechnique in Paris, under the supervision of Prof. Michalis Vazirgiannis. He has a keen interest in machine learning on graph-structured data, focusing on developing neural network architectures for solving real-world problems in biology. His current research focuses on multimodal generative models for protein representation learning, combining both graph and text modalities using Graph Neural Networks and Large Language Models. Previously, he interned at Flatiron Institute of Simons Foundation in New York, working on graph neural networks for cancer gene prediction under the guidance of Prof. Zijun Frank Zhang."
  },
  {
    "objectID": "projects/Michail-Chatzianastasis.html#michail-chatzianastasis",
    "href": "projects/Michail-Chatzianastasis.html#michail-chatzianastasis",
    "title": "Multimodal Protein Representation Learning",
    "section": "",
    "text": "Michail Chatzianastasis is a PhD student at École Polytechnique in Paris, under the supervision of Prof. Michalis Vazirgiannis. He has a keen interest in machine learning on graph-structured data, focusing on developing neural network architectures for solving real-world problems in biology. His current research focuses on multimodal generative models for protein representation learning, combining both graph and text modalities using Graph Neural Networks and Large Language Models. Previously, he interned at Flatiron Institute of Simons Foundation in New York, working on graph neural networks for cancer gene prediction under the guidance of Prof. Zijun Frank Zhang."
  },
  {
    "objectID": "projects/Michail-Chatzianastasis.html#project",
    "href": "projects/Michail-Chatzianastasis.html#project",
    "title": "Multimodal Protein Representation Learning",
    "section": "Project",
    "text": "Project\nProteins are fundamental building blocks of life, playing crucial roles in various biological processes. Representing proteins is a multifaceted challenge due to their complex structural and functional characteristics. Traditionally, proteins have been represented in various ways, including as amino acid sequences, 2D graphs, and 3D graphs, each capturing different aspects of protein structure and function. However, these diverse representations often provide limited insight when considered in isolation. In this project, we aim to address this challenge by exploring multimodal protein representation learning methods[1,2], with the goal of discovering the most effective way to represent proteins or fusing these modalities to enhance downstream tasks. The primary goal of this project is to develop techniques that can effectively unify and harness the information contained in different modalities of protein representation. This involves understanding the complementary aspects of amino acid sequences, 2D graphs (e.g., contact maps), and 3D graphs (e.g., protein structures) and finding a way to merge and learn useful representations from them. By improving the representation learning process, we aim to boost the performance of downstream protein-related tasks such as function property prediction, and protein-protein interaction prediction. Our efforts will result in more accurate models that can have a profound impact on bioinformatics and drug discovery.\n[1] Xu, Minghao, et al. “Protst: Multi-modality learning of protein sequences and biomedical texts.”\n[2] Abdine, Hadi, et al. “Prot2Text: Multimodal Protein’s Function Generation with GNNs and Transformers.”"
  },
  {
    "objectID": "projects/Abhishek-Sharma.html",
    "href": "projects/Abhishek-Sharma.html",
    "title": "Predicting the pathogenicity of a (missense) mutation",
    "section": "",
    "text": "Abhishek is currently a senior research scientist in AI dept of Illumina, Cambridge. He did his PhD in computer science from Ecole polytechnique and masters in Applied Math from Ecole Centrale Paris. Before that, he also spent time as a researcher at EPFL, INRIA and MPI. In general, he is very interested in ML breakthroughs in real world problems with major societal impact."
  },
  {
    "objectID": "projects/Abhishek-Sharma.html#abhishek-sharma",
    "href": "projects/Abhishek-Sharma.html#abhishek-sharma",
    "title": "Predicting the pathogenicity of a (missense) mutation",
    "section": "",
    "text": "Abhishek is currently a senior research scientist in AI dept of Illumina, Cambridge. He did his PhD in computer science from Ecole polytechnique and masters in Applied Math from Ecole Centrale Paris. Before that, he also spent time as a researcher at EPFL, INRIA and MPI. In general, he is very interested in ML breakthroughs in real world problems with major societal impact."
  },
  {
    "objectID": "projects/Abhishek-Sharma.html#project",
    "href": "projects/Abhishek-Sharma.html#project",
    "title": "Predicting the pathogenicity of a (missense) mutation",
    "section": "Project",
    "text": "Project\nPredicting the effect of a (missense) mutation accurately is one of the central problems in biology. Currently, two paradigms dominate benchmarks for missense variant pathogenicity prediction, namely PrimateAI-3D and alpha-missense [1]. Interestingly, both follow totally different approaches in their learned model. While PrimateAI-3D is trained from scratch, alpha-missense [2] follows a pretraining-finetuning strategy. Both approaches rely on MSA, Protein 3D structures as input and attention mechanism in their model architecture. The goal of the project is to explore the alpha-missense pretraining-fine tuning strategy and find (simpler/better) alternatives E.g. [3] outlines a (simpler) alternative to the pre-training part (alphafold2.)\n1 . PrimateAI-3D outperforms AlphaMissense in real-world cohorts (Under Review) https://www.medrxiv.org/content/10.1101/2024.01.12.24301193v1\n\nAccurate proteome-wide missense variant effect prediction with AlphaMissense Cheng et al. Science, 2023\nEfficient and accurate prediction of protein structure using RoseTTAFold2. Minkyung Baek, Ivan Anishchenko, Ian Humphreys, Qian Cong, David Baker, Frank DiMaio Bio arxiv, 2023.\nhttps://structural-bioinformatics.netlify.app/index/"
  },
  {
    "objectID": "projects/George-Panagopoulos.html",
    "href": "projects/George-Panagopoulos.html",
    "title": "Graph Learning for Uplift Modeling",
    "section": "",
    "text": "George Panagopoulos is a postdoctoral scientist at the University of Luxembourg, researching graph neural networks and causal inference for biomedical applications. Before that, he was working on machine learning for operations research as an applied scientist in the Algorithms and Optimization lab of Amazon Transportation Services in Luxembourg. He received his PhD in machine learning from the Data Science and Mining Team of the École Polytechnique, specializing in graph learning for forecasting and combinatorial optimization. Previously, he was a research assistant and obtained his M.Sc. in computer science with a focus on digital signal processing for neural/physiological data from the University of Houston."
  },
  {
    "objectID": "projects/George-Panagopoulos.html#george-panagopoulos",
    "href": "projects/George-Panagopoulos.html#george-panagopoulos",
    "title": "Graph Learning for Uplift Modeling",
    "section": "",
    "text": "George Panagopoulos is a postdoctoral scientist at the University of Luxembourg, researching graph neural networks and causal inference for biomedical applications. Before that, he was working on machine learning for operations research as an applied scientist in the Algorithms and Optimization lab of Amazon Transportation Services in Luxembourg. He received his PhD in machine learning from the Data Science and Mining Team of the École Polytechnique, specializing in graph learning for forecasting and combinatorial optimization. Previously, he was a research assistant and obtained his M.Sc. in computer science with a focus on digital signal processing for neural/physiological data from the University of Houston."
  },
  {
    "objectID": "projects/George-Panagopoulos.html#project",
    "href": "projects/George-Panagopoulos.html#project",
    "title": "Graph Learning for Uplift Modeling",
    "section": "Project",
    "text": "Project\nFrom precision medicine and drug discovery to recommendation systems and online marketing, causal inference using randomized experiments is the gold standard for decision-making.\nHowever, these experiments require interventions that might be too costly, time-consuming, or simply impossible. Machine learning has provided promising solutions in predicting the conditional average treatment effect of an intervention, without actually making it [1]. In this project, we will examine the use of machine learning to facilitate a large-scale marketing campaign.\nDuring such a campaign, promotional codes are distributed to users to increase their activity. Given a specific promotional budget, the campaign should minimize the outreach to users who will not respond, or even worse respond negatively. Hence the aim is to build a model that can predict which users will respond positively to an intervention, commonly called uplift modelling. In uplift modeling, we run an experiment on a representative subset of the users and train a model to predict the average treatment effect on the rest of the user base.\nThe plan is to focus on the network of interactions between samples, which can hide potential confounders that introduce bias in the experiment [2]. We will tackle uplift modeling using graph learning on a heterogeneous graph of an e-commerce system with ground truth interventions and outcomes from an actual marketing campaign. We will then compare the performance with state-of-the-art methods using one of the libraries for ML-based causal inference [3].\n[1] Künzel, Sören R., et al. “Metalearners for estimating heterogeneous treatment effects using machine learning.” Proceedings of the national academy of sciences 116.10 (2019): 4156-4165.\n[2] Guo, Ruocheng, Jundong Li, and Huan Liu. “Learning individual causal effects from networked observational data.” Proceedings of the 13th international conference on web search and data mining. 2020.\n[3] Syrgkanis, Vasilis, et al. “Causal inference and machine learning in practice with econml and causalml: Industrial use cases at microsoft, tripadvisor, uber.” Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining. 2021."
  },
  {
    "objectID": "projects/Marco-Fumero.html",
    "href": "projects/Marco-Fumero.html",
    "title": "On the Geometry of Relative Representations",
    "section": "",
    "text": "Marco Fumero is an ELLIS Ph.D. student at Sapienza University of Rome (moving to IST Austria as PostDoc in prof. Locatello group) in the GLADIA research group led by Professor Emanuele Rodolà. His previous experiences includes a research internship at Autodesk AI LAB and Amazon AWS AI working on geometric deep earning and causal representation learning topics. His research stands at the intersection of geometry and deep learning with a focus on representation learning, disentanglement and out-of-distribution generalization. He has been recently focusing on the direction of latent space communication, and, more broadly, on the question of when how and why distinct learning processes yield similar representation, organizing also the UniReps workshop at NeurIPS 2023 on these topics."
  },
  {
    "objectID": "projects/Marco-Fumero.html#marco-fumero",
    "href": "projects/Marco-Fumero.html#marco-fumero",
    "title": "On the Geometry of Relative Representations",
    "section": "",
    "text": "Marco Fumero is an ELLIS Ph.D. student at Sapienza University of Rome (moving to IST Austria as PostDoc in prof. Locatello group) in the GLADIA research group led by Professor Emanuele Rodolà. His previous experiences includes a research internship at Autodesk AI LAB and Amazon AWS AI working on geometric deep earning and causal representation learning topics. His research stands at the intersection of geometry and deep learning with a focus on representation learning, disentanglement and out-of-distribution generalization. He has been recently focusing on the direction of latent space communication, and, more broadly, on the question of when how and why distinct learning processes yield similar representation, organizing also the UniReps workshop at NeurIPS 2023 on these topics."
  },
  {
    "objectID": "projects/Marco-Fumero.html#project",
    "href": "projects/Marco-Fumero.html#project",
    "title": "On the Geometry of Relative Representations",
    "section": "Project",
    "text": "Project\nNeural networks embed the geometric structure of a data manifold lying in a high-dimensional space into latent representations. Ideally, the distribution of the data points in the latent space should depend only on the task, the data, the loss, and other architecture-specific constraints. However, factors such as the random weights initialization, training hyperparameters, or other sources of randomness in the training phase may induce incoherent latent spaces that hinder any form of reuse. In a recent work [1] , it has been empirically observed that, under the same data and modeling choices, the angles between the encodings within distinct latent spaces do not change.To exploit this observation,we can compute a representation based on the latent similarity between each sample and a fixed set of training points, denoted as anchors. Given a specific choice of anchors and similarity function, the representation will retain different properties: choosing cosine similarity in the latent spaces enforces the desired invariance to angle preserving transformation of the latent space, without any additional training procedures. Neural architectures can leverage these relative representations to guarantee, in practice, invariance to latent isometries and rescalings, effectively enabling latent space communication: from zero-shot model stitching to latent space comparison between diverse settings, on different datasets, spanning various modalities (images, text, graphs), tasks (e.g., classification, reconstruction) and architectures (e.g., CNNs, GCNs, transformers).\nThe choice of similarity function should not be limited to capture invariances to angle preserving transformations. As shown in [2], other choices are good as well, and there’s not a clear best choice for capturing transformation across distinct latent spaces, depending on the setting and nuisance factors that affect the representations and cause the spaces to differ.\nIn this project the objective is to follow this direction by extending the notion of similarity function in two different ways aiming at enhancing the expressivity of this framework using differential geometry and topological analysis tools.\nHigher Order Relative Representations: The first direction aims to enhance the scope of the relative representation function by considering a similarity function that takes three or more arguments in input, switching from considering binary relations between data points to n-way relations. From a geometrical perspective, the standard relative representation corresponds to constructing a bipartite graph between anchors and sample: considering n-way relations is analogous to constructing a simplicial complex in the latent space. For instance, three-way relationships correspond to triangles, four-way to tetrahedra, and so on. The objective here is to assess whether this extension enriches the framework’s expressiveness from both practical and theoretical standpoints.\nRelative Geodesic Representations: Our second area of exploration involves utilizing geodesic distance as a similarity function, calculated within latent spaces, as a metric for relative representation. This approach ensures that the relative space remains invariant to the isometries of the data’s manifold, defined by geodesic distance. The primary objective of this project is to experimentally determine whether this set of transformations results in a more expressive and efficient relative space, one that more accurately encapsulates the relationships and transformations across different latent spaces. A secondary goal is to devise effective approximation techniques for computing geodesics, thereby enhancing the practical efficiency of the method.\n[1] Moschella, L., Maiorca, V., Fumero, M., Norelli, A., Locatello, F., & Rodola, E. (2022). Relative representations enable zero-shot latent space communication.ICLR 2023\n[2] Cannistraci, I., Moschella, L., Fumero, M., Maiorca, V., & Rodolà, E. (2023). From Bricks to Bridges: Product of Invariances to Enhance Latent Space Communication. arXiv\n[3] Shao, H., Kumar, A., & Thomas Fletcher, P. (2018). The Riemannian geometry of deep generative models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (pp. 315-323)."
  },
  {
    "objectID": "projects/Elli-Heyes.html",
    "href": "projects/Elli-Heyes.html",
    "title": "Generating Calabi-Yau Manifolds with Machine Learning",
    "section": "",
    "text": "I am final year PhD student at City, University of London and soon to be postdoctoral researcher at Imperial College London. Broadly speaking my research involves using techniques from data science and machine learning to study compactification spaces in string theory, namely Calabi-Yau manifolds and G2 manifolds. I attended LOGML just before I started my PhD and learnt a lot, so I look forward to being part of LOGML once again this time as a mentor."
  },
  {
    "objectID": "projects/Elli-Heyes.html#elli-heyes",
    "href": "projects/Elli-Heyes.html#elli-heyes",
    "title": "Generating Calabi-Yau Manifolds with Machine Learning",
    "section": "",
    "text": "I am final year PhD student at City, University of London and soon to be postdoctoral researcher at Imperial College London. Broadly speaking my research involves using techniques from data science and machine learning to study compactification spaces in string theory, namely Calabi-Yau manifolds and G2 manifolds. I attended LOGML just before I started my PhD and learnt a lot, so I look forward to being part of LOGML once again this time as a mentor."
  },
  {
    "objectID": "projects/Elli-Heyes.html#project",
    "href": "projects/Elli-Heyes.html#project",
    "title": "Generating Calabi-Yau Manifolds with Machine Learning",
    "section": "Project",
    "text": "Project\nCalabi-Yau manifolds are of interest in string theory as they describe the shape of the extra dimensions that arise in the theory. It is an active area of research to construct new Calabi-Yau manifolds, and in this project we will use genetic algorithms to generate Calabi-Yau manifolds arising from the construction of hypersurfaces in toric varieties. This was pioneered in the preprint “New Calabi-Yau Manifolds from Genetic Algorithms” (https://arxiv.org/abs/2306.06159). Using our algorithm we will search for Calabi-Yau manifolds with the set of properties that are required in order to give rise to the physics of the Standard Model. Time permitting, we may experiment with more complicated algorithms, for example by combining genetic algorithms with neural networks to optimise the evolutionary process."
  },
  {
    "objectID": "projects/Claudio-Battiloro.html",
    "href": "projects/Claudio-Battiloro.html",
    "title": "Self-supervised learning for Topological Neural Networks",
    "section": "",
    "text": "Claudio Battiloro is a PhD candidate in ICT at Sapienza University of Rome, a Visiting Scholar at Harvard SPH, and a former Visiting Associate at the SEAS of University of Pennsylvania. He is a prospective co-appointed postdoctoral fellow at Harvard University and University of Pennsylvania. Claudio’s research interests include theory and methods for topological and algebraic signal processing, topological deep learning, and distributed optimization. He has several publications in top-tier journals and conferences. Claudio received different awards such as the IEEE SPS Italian Chapter Best M.Sc. Thesis Award (2020). In 2020, he graduated with distinction in the M.Sc. in Data Science with a (university-overall) Top 400 Students award at Sapienza University."
  },
  {
    "objectID": "projects/Claudio-Battiloro.html#claudio-battiloro",
    "href": "projects/Claudio-Battiloro.html#claudio-battiloro",
    "title": "Self-supervised learning for Topological Neural Networks",
    "section": "",
    "text": "Claudio Battiloro is a PhD candidate in ICT at Sapienza University of Rome, a Visiting Scholar at Harvard SPH, and a former Visiting Associate at the SEAS of University of Pennsylvania. He is a prospective co-appointed postdoctoral fellow at Harvard University and University of Pennsylvania. Claudio’s research interests include theory and methods for topological and algebraic signal processing, topological deep learning, and distributed optimization. He has several publications in top-tier journals and conferences. Claudio received different awards such as the IEEE SPS Italian Chapter Best M.Sc. Thesis Award (2020). In 2020, he graduated with distinction in the M.Sc. in Data Science with a (university-overall) Top 400 Students award at Sapienza University."
  },
  {
    "objectID": "projects/Claudio-Battiloro.html#project",
    "href": "projects/Claudio-Battiloro.html#project",
    "title": "Self-supervised learning for Topological Neural Networks",
    "section": "Project",
    "text": "Project\nSelf-supervised methods can be broadly categorized into contrastive approaches and predictive approaches. In the context of GNNs, self-supervision is particularly useful for learning graph encoders by utilizing information from the distribution of unlabeled graphs and minimizing a self-supervised loss. However, literature about self-supervised techniques for Topological Deep Learning is extremely sparse, and only a couple of works for simplicial complexes have been presented. For this reason, I propose to design novel self-predictive and contrastive objectives for training TDL architectures.\nIn particular, I will mentor the attendants with the aim of investigating generative objectives based on the predictive approach for architectures defined over regular cell complexes. We will explore cell complex-based reconstruction tasks that can properly generalize the well-studied node property prediction and attribute denoising tasks in the space of attributed graphs.\nWe will go through the actual SoA to design novel techniques able to combine my personal signal processing-grounded approach with cutting edge ML research on Topological and Geometric DL."
  },
  {
    "objectID": "projects/Rahul-Singh.html",
    "href": "projects/Rahul-Singh.html",
    "title": "Spectral Signed GNNs for fMRI Connectomes",
    "section": "",
    "text": "Rahul is a Wu Tsai postdoctoral fellow at Yale University where he is mentored by Smita Krishnaswamy and Joy Hirsch. He received his PhD in Machine Learning in under the mentorship of Yongxin Chen from the Georgia Institute of Technology. His research interests are in the areas of signal processing, graph neural networks, and machine learning applications in bioinformatics and neuroscience. One of his current research focuses is to explore the neural complexes of brain-to-brain communication when two humans interact."
  },
  {
    "objectID": "projects/Rahul-Singh.html#rahul-singh",
    "href": "projects/Rahul-Singh.html#rahul-singh",
    "title": "Spectral Signed GNNs for fMRI Connectomes",
    "section": "",
    "text": "Rahul is a Wu Tsai postdoctoral fellow at Yale University where he is mentored by Smita Krishnaswamy and Joy Hirsch. He received his PhD in Machine Learning in under the mentorship of Yongxin Chen from the Georgia Institute of Technology. His research interests are in the areas of signal processing, graph neural networks, and machine learning applications in bioinformatics and neuroscience. One of his current research focuses is to explore the neural complexes of brain-to-brain communication when two humans interact."
  },
  {
    "objectID": "projects/Rahul-Singh.html#project",
    "href": "projects/Rahul-Singh.html#project",
    "title": "Spectral Signed GNNs for fMRI Connectomes",
    "section": "Project",
    "text": "Project\nThe existing GNN architectures have focused almost exclusively on graphs with nonnegative edges, which encode some kind of similarity relation between the incident nodes. In contrast, negative edges are often useful to model dissimilarity relations: for instance, in social networks, users may have common/opposite political views or like/dislike each other. Such negative correlations also arise in functional magnetic resonance imaging (fMRI)-derived brain networks or connectomes [1]. These dissimilarity relations are modeled using signed graphs allowing the edges to take both positive or negative values.\nWe will build upon the recently proposed spectral signed graph neural network (GNN) architechtures [2] that can handle (directed) signed graphs and provide interpretable models backed by frequency analysis of signed graphs. Existing representation learning based methods such as BrainGNN [3] for fMRI data do not take these negative correlations into account. The focus of the project will be to utilize interpretable spectral signed GNNs to understand the neurobiological information of negative edges in fMRI connectomes. We will use open-source fMRI datasets human connectome project (HCP) and autism brain imaging data exchange (ABIDE) for our analysis. The brain will be modeled as a signed graph with nodes representing brain regions of interest (ROIs) and edges representing the functional connectivity between those ROIs computed as the pairwise (positive as well as negative) correlations of fMRI time series. The goal will be to identify and explain the effect of negative correlations between different brain regions on learned representations over fMRI connectomes.\n[1] Liang Zhan et al. The significance of negative correlations in brain connectivity, Journal of Comparative Neurology 2017.\n[2] Rahul Singh and Yongxin Chen, Signed graph neural networks: A frequency perspective, Transactions on Machine Learning Research 2023.\n[3] Xiaoxiao Li et al. BrainGNN: Interpretable brain graph neural network for fMRI analysis, Medical Image Analysis 2021."
  },
  {
    "objectID": "logml2021/projects2021/project2/index.html",
    "href": "logml2021/projects2021/project2/index.html",
    "title": "Characterising Universes in String Theory using Geometric Learning",
    "section": "",
    "text": "Challenger Mishra is a Departmental Eary Career Academic Fellow at the Computer Lab, University of Cambridge, and a Stipendiary Lecturer of Applied Mathematics at Lady Margaret Hall, University of Oxford. His research interests include Machine Learning, Calabi-Yau manifolds in String theory, String compactifications, and their interplay. Prior to this, he did a doctorate as a Rhodes Scholar in String theory at the Rudolf Peierls Center for Theoretical Physics, University of Oxford. Challenger completed his undergraduate work in Physics from the Indian Institute of Science Education and Research, Kolkata."
  },
  {
    "objectID": "logml2021/projects2021/project2/index.html#challenger-mishra",
    "href": "logml2021/projects2021/project2/index.html#challenger-mishra",
    "title": "Characterising Universes in String Theory using Geometric Learning",
    "section": "",
    "text": "Challenger Mishra is a Departmental Eary Career Academic Fellow at the Computer Lab, University of Cambridge, and a Stipendiary Lecturer of Applied Mathematics at Lady Margaret Hall, University of Oxford. His research interests include Machine Learning, Calabi-Yau manifolds in String theory, String compactifications, and their interplay. Prior to this, he did a doctorate as a Rhodes Scholar in String theory at the Rudolf Peierls Center for Theoretical Physics, University of Oxford. Challenger completed his undergraduate work in Physics from the Indian Institute of Science Education and Research, Kolkata."
  },
  {
    "objectID": "logml2021/projects2021/project2/index.html#project",
    "href": "logml2021/projects2021/project2/index.html#project",
    "title": "Characterising Universes in String Theory using Geometric Learning",
    "section": "Project",
    "text": "Project\nOne of the holy grails of modern theoretical physics is the unification of Quantum Mechanics with Einstein’s relativity. String theory is the only known consistent theory of quantum gravity, and arguably the most promising candidate for a unified theory of physics. Since its inception in the late 1960s, it has provided tremendous insights into our understanding of the physical world, and has overseen many interesting developments in various branches of pure mathematics and theoretical physics. Despite string theory’s many successes, a string model that explains all observed data from cosmology and particle physics experiments, has eluded discovery. This is owing to the particularly large landscape of valid string theory solutions, estimated to be of the size 10^{270,000}. Most of these solutions are thought to lead to descriptions of universes that do not resemble ours in detail.\nString theory posits extra-dimensions of space. These are often described by complex geometries called Calabi—Yau manifolds. A class of string theory solutions (or vacua) is characterised by Complete Intersection Calabi–Yau manifolds, and bundles over them. The data corresponding to these are encoded as bipartite graphs and integer matrices, whose size is governed by the (topological) properties of the bundles and Calabi–Yau geometries. The resulting dataset is of size tens of thousands. The objective of this project is to characterise these different solutions (Universes) using Machine Learning. More concretely, the aim is to obtain a suitable metric on this space of solutions that ‘scores’ stringy solutions based on their closeness to reality, i.e., observations from particle accelerators like the LHC. Such a metric could be approximated by a sufficiently deep neural network. Insights from such a metric will allow the construction of even more realistic string solutions, ESP on geometries that have been out of current computational reach.\nThe project will allow the participant(s) to delve into fundamentals of complex geometry, learn about effective representations of geometric data in Machine Learning, and develop an empirical understanding of the ML tools that are effective in such geometric problems.\nReferences: [1] Calabi-Yau Spaces in the String Landscape – Yang-Hui He, https://arxiv.org/abs/2006.16623\n[2] Calabi-Yau manifolds, Discrete Symmetries, and String theory – Challenger Misha, https://ora.ox.ac.uk/objects/uuid:4a174981-085e-4e81-8f27-b48533f08315\n[3] Heterotic Line Bundle Standard Models – Lara B. Anderson, James Gray, Andre Lukas, Eran Palti\nhttps://arxiv.org/abs/1202.1757"
  },
  {
    "objectID": "logml2021/projects2021/project4/index.html",
    "href": "logml2021/projects2021/project4/index.html",
    "title": "Surface reconstruction from point clouds",
    "section": "",
    "text": "Rana Hanocka is a Ph.D. candidate at Tel Aviv University under the supervision of Daniel Cohen-Or and Raja Giryes. Rana obtained an M.Sc. in Electrical Engineering from Tel Aviv University and a B.Sc. in Electrical Engineering from Rensselaer Polytechnic Institute. Rana is the recipient of the Dan David Prize’s 2020 Scholarship in Artificial Intelligence, and was awarded the Outstanding Data Science Fellowship by Israel’s Council for Higher Education."
  },
  {
    "objectID": "logml2021/projects2021/project4/index.html#rana-hanocka",
    "href": "logml2021/projects2021/project4/index.html#rana-hanocka",
    "title": "Surface reconstruction from point clouds",
    "section": "",
    "text": "Rana Hanocka is a Ph.D. candidate at Tel Aviv University under the supervision of Daniel Cohen-Or and Raja Giryes. Rana obtained an M.Sc. in Electrical Engineering from Tel Aviv University and a B.Sc. in Electrical Engineering from Rensselaer Polytechnic Institute. Rana is the recipient of the Dan David Prize’s 2020 Scholarship in Artificial Intelligence, and was awarded the Outstanding Data Science Fellowship by Israel’s Council for Higher Education."
  },
  {
    "objectID": "logml2021/projects2021/project4/index.html#project",
    "href": "logml2021/projects2021/project4/index.html#project",
    "title": "Surface reconstruction from point clouds",
    "section": "Project",
    "text": "Project\nIn this project, we will take a closer look into surface reconstruction from point clouds. This project is a hands-on project in PyTorch. You will gain familiarity with 3D surface reconstruction and applying deep neural networks to meshes. The project is mainly focused on the Point2Mesh [1] technique, which optimizes the weights of a CNN to deform some initial mesh to shrink-wrap the input point cloud. You will apply this technique to scans and a wider variety of data in order to explore the current strengths and limitations of such a technique, and how it can be further improved. One interesting direction is to learn which edge to split, thereby defining where to add additional mesh connectivity. By the end of the project, you should better understand these topics from a theoretical and practical perspective, and gain insights with respect to how to incorporate similar concepts into your own research.\n[1] Point2Mesh: A Self-Prior for Deformable Meshes. Rana Hanocka, Gal Metzer, Raja Giryes, and Daniel Cohen-Or. SIGGRAPH 2020."
  },
  {
    "objectID": "logml2021/projects2021/project18/index.html",
    "href": "logml2021/projects2021/project18/index.html",
    "title": "Uncovering and correcting biases in neuroimaging studies",
    "section": "",
    "text": "Ira is currently a Research Engineer at DeepMind working on Incubation of cutting edge research. Previously, she was a Senior Machine Learning Researcher at Twitter, focusing on real-time personalisation and causal analysis for recommendations. She completed a Doctoral degree in Medical Image Computing at Imperial College London. Her research focused on developing methods for modelling and analysing graph-structured neuroimaging data at an individual or population level using traditional graph theoretical approaches and geometric deep learning. During her PhD, she spent some time at the Massachusetts General Hospital, Harvard Medical School, where she worked on outcome prediction for ischemic stroke patients."
  },
  {
    "objectID": "logml2021/projects2021/project18/index.html#ira-ktena",
    "href": "logml2021/projects2021/project18/index.html#ira-ktena",
    "title": "Uncovering and correcting biases in neuroimaging studies",
    "section": "",
    "text": "Ira is currently a Research Engineer at DeepMind working on Incubation of cutting edge research. Previously, she was a Senior Machine Learning Researcher at Twitter, focusing on real-time personalisation and causal analysis for recommendations. She completed a Doctoral degree in Medical Image Computing at Imperial College London. Her research focused on developing methods for modelling and analysing graph-structured neuroimaging data at an individual or population level using traditional graph theoretical approaches and geometric deep learning. During her PhD, she spent some time at the Massachusetts General Hospital, Harvard Medical School, where she worked on outcome prediction for ischemic stroke patients."
  },
  {
    "objectID": "logml2021/projects2021/project18/index.html#project",
    "href": "logml2021/projects2021/project18/index.html#project",
    "title": "Uncovering and correcting biases in neuroimaging studies",
    "section": "Project",
    "text": "Project\nRecent work [1] on neuroimaging has demonstrated significant benefits of using population graphs to capture non-imaging information in the prediction of neurodegenerative and neurodevelopmental disorders. These non-imaging attributes may contain demographic information about the individuals, e.g. age or sex, but also the acquisition site, as imaging protocols and hardware might significantly differ across sites in large-scale studies. The effect of the latter is particularly prevalent in functional connectomics studies, where it’s unclear how to sufficiently homogenise fMRI signals across the different sites. A recent study [2] has highlighted the need to investigate potential biases in the classifiers devised using large-scale datasets, which might be imbalanced in terms of one or more sensitive attributes (like gender and race). This can be exacerbated when employing these attributes in a population graph and lead to disparate predictive performance across sub-populations. This project aims to uncover any potential biases of a semi-supervised classifier that relies on a population graph and explores methods to mitigate such biases to produce fairer predictions across the population.\n[1] Parisot, S., Ktena, S. I., Ferrante, E., Lee, M., Moreno, R. G., Glocker, B., & Rueckert, D. Disease Prediction using Graph Convolutional Networks: Application to Autism Spectrum Disorder and Alzheimer’s Disease. Medical Image Analysis, 2018\n[2] Larrazabal, Agostina J., et al. “Gender imbalance in medical imaging datasets produces biased classifiers for computer-aided diagnosis.” Proceedings of the National Academy of Sciences, 2020"
  },
  {
    "objectID": "logml2021/projects2021/project16/index.html",
    "href": "logml2021/projects2021/project16/index.html",
    "title": "Manifold optimization and recent applications",
    "section": "",
    "text": "Bamdev Mishra is Senior Applied Scientist at Microsoft India in the Office India Intelligence team. He develops machine learning (ML) solutions for Kaizala, Office Lens, Operations, and Information Protection for Office365, to name a few. Prior to this, he worked on various ML problems in the retail domain including competitive price prediction, review abuse detection, and style recommendations.\nBamdev looks into problem domains that allow to use and build ML solutions for industrial applications. On the research side, his primary research interests include nonlinear optimization, stochastic learning, and matrix and tensor learning methods. He has published many technical papers in ML, NLP, and numerical optimization.\nBamdev received the BTech and MTech degrees in Electrical Engineering from the Indian Institute of Technology Bombay, India, in 2010, and the Ph.D. degree from the University of Liège, Belgium, in 2014. He was a Postdoctoral Researcher at the University of Liège and a Visiting Research Associate at the University of Cambridge from 2014 to 2015."
  },
  {
    "objectID": "logml2021/projects2021/project16/index.html#bamdev-mishra",
    "href": "logml2021/projects2021/project16/index.html#bamdev-mishra",
    "title": "Manifold optimization and recent applications",
    "section": "",
    "text": "Bamdev Mishra is Senior Applied Scientist at Microsoft India in the Office India Intelligence team. He develops machine learning (ML) solutions for Kaizala, Office Lens, Operations, and Information Protection for Office365, to name a few. Prior to this, he worked on various ML problems in the retail domain including competitive price prediction, review abuse detection, and style recommendations.\nBamdev looks into problem domains that allow to use and build ML solutions for industrial applications. On the research side, his primary research interests include nonlinear optimization, stochastic learning, and matrix and tensor learning methods. He has published many technical papers in ML, NLP, and numerical optimization.\nBamdev received the BTech and MTech degrees in Electrical Engineering from the Indian Institute of Technology Bombay, India, in 2010, and the Ph.D. degree from the University of Liège, Belgium, in 2014. He was a Postdoctoral Researcher at the University of Liège and a Visiting Research Associate at the University of Cambridge from 2014 to 2015."
  },
  {
    "objectID": "logml2021/projects2021/project16/index.html#project",
    "href": "logml2021/projects2021/project16/index.html#project",
    "title": "Manifold optimization and recent applications",
    "section": "Project",
    "text": "Project\nOptimization over smooth manifolds or manifold optimization involves minimizing an objective function over a smooth constrained set. Many such sets have usually a manifold structure. Some particularly useful manifolds include the set of orthogonal matrices, the set of symmetric positive definite matrices, the set of subspaces, the set of fixed-rank matrices/tensors, and the set of doubly stochastic matrices (optimal transport plans), to name a few [1]. Consequently, there has been a development of a number of manifold optimization toolboxes [2].\nIn this project, we make use of these wonderful tools to solve a few machine learning problems with manifold optimization. The aim would be to get a hands-on experience of manifold optimization.\n[1] Boumal, N., 2020. An introduction to optimization on smooth manifolds. Web: http://sma.epfl.ch/~nboumal/book/index.html.\n[2] Manopt, pymanopt, Manopt.jl, McTorch, Geomstats, ROPTLIB, and so on. The links to many of these toolboxes are available on https://www.manopt.org/about.html."
  },
  {
    "objectID": "logml2021/projects2021/project10/index.html",
    "href": "logml2021/projects2021/project10/index.html",
    "title": "Geometric Learning on Shapes and Distributions with Optimal Transport",
    "section": "",
    "text": "Jean Feydy graduated from the ENS Paris in 2016, writing his master’s thesis on the denoising of CT images at Siemens Healthcare (Princeton, NJ). He then pursued a PhD under the supervision of Alain Trouvé (ENS Paris-Saclay) at the intersection between computational anatomy, optimal transport theory and geometric machine learning. He is currently doing a post-doc with Michael Bronstein (Imperial College London) and will soon take up a faculty position at INRIA Paris, working on building new bridges between medical imaging, computer vision and geometric statistics.\nHis research focuses on flexible geometric methods for scalable data analysis. His most significant works are distributed through: - the KeOps library for fast geometric computations (www.kernel-operations.io), - the GeomLoss library for scalable optimal transport (www.kernel-operations.io/geomloss/), - his accessible textbook “Geometric data analysis, beyond convolutions” (www.jeanfeydy.com/geometric_data_analysis.pdf)."
  },
  {
    "objectID": "logml2021/projects2021/project10/index.html#jean-feydy",
    "href": "logml2021/projects2021/project10/index.html#jean-feydy",
    "title": "Geometric Learning on Shapes and Distributions with Optimal Transport",
    "section": "",
    "text": "Jean Feydy graduated from the ENS Paris in 2016, writing his master’s thesis on the denoising of CT images at Siemens Healthcare (Princeton, NJ). He then pursued a PhD under the supervision of Alain Trouvé (ENS Paris-Saclay) at the intersection between computational anatomy, optimal transport theory and geometric machine learning. He is currently doing a post-doc with Michael Bronstein (Imperial College London) and will soon take up a faculty position at INRIA Paris, working on building new bridges between medical imaging, computer vision and geometric statistics.\nHis research focuses on flexible geometric methods for scalable data analysis. His most significant works are distributed through: - the KeOps library for fast geometric computations (www.kernel-operations.io), - the GeomLoss library for scalable optimal transport (www.kernel-operations.io/geomloss/), - his accessible textbook “Geometric data analysis, beyond convolutions” (www.jeanfeydy.com/geometric_data_analysis.pdf)."
  },
  {
    "objectID": "logml2021/projects2021/project10/index.html#project",
    "href": "logml2021/projects2021/project10/index.html#project",
    "title": "Geometric Learning on Shapes and Distributions with Optimal Transport",
    "section": "Project",
    "text": "Project\nOptimal transport generalizes sorting to spaces of dimension D&gt;1. It induces the Wasserstein metric (aka. Earth Mover’s Distance) between probability distributions, which allows us to work with unlabelled point clouds using a simple and intuitive particle-based model.\nIn this project, we will build upon the fast numerical routines of the GeomLoss library (https://www.kernel-operations.io/geomloss/) to explore the use of the Wasserstein metric in geometric data analysis. We will first start with a short lecture on the definition and main properties of optimal transport. Then, we will rely on simple experiments with Wasserstein barycenters and gradient flows to get an intuitive understanding of the optimal transport distance. Finally, we will study the impact of this metric on several standard tasks, from 3D shape registration to the UMAP visualization of a dataset of histograms.\nThis project will allow you to get a hands-on experience of optimal transport tools in realistic application scenarios. Notably, we will highlight both the strengths and the limitations of this theory in data sciences: by the end of the week, you should have a clear picture of what optimal transport can (and cannot) bring to your own research work."
  },
  {
    "objectID": "logml2021/projects2021/project19/index.html",
    "href": "logml2021/projects2021/project19/index.html",
    "title": "Geometry of HMC and Geometric Integration for Sampling and Optimization",
    "section": "",
    "text": "Alessandro is a Research Associate in the Engineering Department at the University of Cambridge and a visiting researcher at the Alan Turing Institute. His research focuses on the empowerment of measures via the geometrisation of statistical methods, such as Hamiltonian Monte Carlo and kernel algorithms built with Stein operators. After having completed the MMathPhys of the University of Warwick and Part III of the Mathematical Tripos at Cambridge, he received his PhD from Imperial College London."
  },
  {
    "objectID": "logml2021/projects2021/project19/index.html#alessandro-barp",
    "href": "logml2021/projects2021/project19/index.html#alessandro-barp",
    "title": "Geometry of HMC and Geometric Integration for Sampling and Optimization",
    "section": "",
    "text": "Alessandro is a Research Associate in the Engineering Department at the University of Cambridge and a visiting researcher at the Alan Turing Institute. His research focuses on the empowerment of measures via the geometrisation of statistical methods, such as Hamiltonian Monte Carlo and kernel algorithms built with Stein operators. After having completed the MMathPhys of the University of Warwick and Part III of the Mathematical Tripos at Cambridge, he received his PhD from Imperial College London."
  },
  {
    "objectID": "logml2021/projects2021/project19/index.html#project",
    "href": "logml2021/projects2021/project19/index.html#project",
    "title": "Geometry of HMC and Geometric Integration for Sampling and Optimization",
    "section": "Project",
    "text": "Project\nGeometric integration plays a central role in many applications. In this project, we will discuss its applications to sampling and optimisation. For sampling, we will uncover the canonical geometry of measures and apply it construct diffusions and dynamics preserving measures, symmetries and constraints. We will then discuss general strategies to construct Hamiltonian-based geometric integrators maintaining some critical properties, in particular volume preservation and conservation of a shadow energy, and hence obtain the family of Hamiltonian Monte Carlo samplers on vector spaces and manifolds.\nWe will then apply similar techniques to optimization in order to obtain rate-matching integrators that preserve the decay rate of dissipative dynamics."
  },
  {
    "objectID": "logml2021/projects2021/project1/index.html",
    "href": "logml2021/projects2021/project1/index.html",
    "title": "Stability or Collapse: Topological Properties of Deep Autoencoders",
    "section": "",
    "text": "Kelly Spendlove is a postdoctoral researcher at the University of Oxford in the Centre for Topological Data Analysis. His research primarily concerns applied algebraic topological approaches to dynamical systems. He completed his PhD at Rutgers University under Konstantin Mischaikow, with the support of a NSF Graduate Fellowship, while spending time as a long term visitor at VU Amsterdam, Ohio State University, and Kyoto University."
  },
  {
    "objectID": "logml2021/projects2021/project1/index.html#kelly-spendlove",
    "href": "logml2021/projects2021/project1/index.html#kelly-spendlove",
    "title": "Stability or Collapse: Topological Properties of Deep Autoencoders",
    "section": "",
    "text": "Kelly Spendlove is a postdoctoral researcher at the University of Oxford in the Centre for Topological Data Analysis. His research primarily concerns applied algebraic topological approaches to dynamical systems. He completed his PhD at Rutgers University under Konstantin Mischaikow, with the support of a NSF Graduate Fellowship, while spending time as a long term visitor at VU Amsterdam, Ohio State University, and Kyoto University."
  },
  {
    "objectID": "logml2021/projects2021/project1/index.html#project",
    "href": "logml2021/projects2021/project1/index.html#project",
    "title": "Stability or Collapse: Topological Properties of Deep Autoencoders",
    "section": "Project",
    "text": "Project\nRecent ideas [1,2] have considered how the behavior of different activation functions may depend upon their ‘topological’ properties, e.g., homeomorphism vs continuity. The work of Naitzat et al [1] seems to suggest that ReLU activations exhibit a stronger effect upon the topology, collapsing it in earlier layers and more quickly than homeomorphic activations such as Tanh or Leaky ReLU.\nOn the other hand, auto-encoders are neural networks which minimize the distance between a reconstruction and the original data, producing both an ‘encoder’ and ‘decoder’. The stability theorem of persistent homology suggests that if an autoencoder is trained to reconstruct the data within epsilon, the resulting persistence diagrams (a proxy for the topology) are also within distance epsilon. This would seem to suggest that the topology cannot be changed by much, even with the use of ReLU and a deep autoencoder. The goal of this project is to investigate and understand how these two observations can be reconciled; and once properly understood, to determine whether these observations can be used to design topologically-faithful dimensionality reduction techniques.\n[1] Naitzat, G., Zhitnikov, A., & Lim, L. H. (2020). Topology of deep neural networks. Journal of Machine Learning Research, 21(184), 1-40. [2] C. Olah. Neural networks, manifolds, and topology.http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/, 2014."
  },
  {
    "objectID": "logml2021/projects2021/project9/index.html",
    "href": "logml2021/projects2021/project9/index.html",
    "title": "Self-supervised non-rigid correspondence by geodesic distortion minimization using the deformation field",
    "section": "",
    "text": "Søren Hauberg is a professor in the Section for Cognitive Systems at the Technical University of Denmark. He received his PhD in computer science from the University of Copenhagen in 2011. Prior to pursuing a PhD he worked as a “digital lumberjack” in the startup Dralle A/S. He was a post doc for two years at Perceiving Systems at the Max Planck Institute for Intelligent Systems working with Michael Black. In 2013, he was the sole computer science recipient of the Sapere Aude Research Talent award from the Danish Council for Independent Research, and in 2016 he was the sole computer science Villum Young Investigator. In 2017 he was further awarded a starting grant from the European Research Council. In 2018, he joined the Young Scientists community under the World Economic Forum, and was in the process named one of “10 of the most exciting young scientists working in the world today.”\nHis research interest lie in the span of geometry and statistics. He develops machine learning techniques using geometric constructions, and works on the related numerical challenges. He is particularly interested in random geometries as they naturally appear in learning."
  },
  {
    "objectID": "logml2021/projects2021/project9/index.html#søren-hauberg",
    "href": "logml2021/projects2021/project9/index.html#søren-hauberg",
    "title": "Self-supervised non-rigid correspondence by geodesic distortion minimization using the deformation field",
    "section": "",
    "text": "Søren Hauberg is a professor in the Section for Cognitive Systems at the Technical University of Denmark. He received his PhD in computer science from the University of Copenhagen in 2011. Prior to pursuing a PhD he worked as a “digital lumberjack” in the startup Dralle A/S. He was a post doc for two years at Perceiving Systems at the Max Planck Institute for Intelligent Systems working with Michael Black. In 2013, he was the sole computer science recipient of the Sapere Aude Research Talent award from the Danish Council for Independent Research, and in 2016 he was the sole computer science Villum Young Investigator. In 2017 he was further awarded a starting grant from the European Research Council. In 2018, he joined the Young Scientists community under the World Economic Forum, and was in the process named one of “10 of the most exciting young scientists working in the world today.”\nHis research interest lie in the span of geometry and statistics. He develops machine learning techniques using geometric constructions, and works on the related numerical challenges. He is particularly interested in random geometries as they naturally appear in learning."
  },
  {
    "objectID": "logml2021/projects2021/project9/index.html#project",
    "href": "logml2021/projects2021/project9/index.html#project",
    "title": "Self-supervised non-rigid correspondence by geodesic distortion minimization using the deformation field",
    "section": "Project",
    "text": "Project\nLatent variable models, such as the variational autoencoder, suffer from the identifiability problem: there is no unique configuration of the latent variables. This is problematic as latent variables are often inspected, e.g. through visualization, to gain insights into the data generating process. The lack of identifiability then raise the risk of misinterpreting the data as conclusions may be drawn from arbitrary latent instantiations.\nIn this project you will investigate a geometric solution to the identifiability problem that amounts to endowing the latent space with a particular Riemannian metric. You will learn latent representations and compute geodesics accordingly.\nReferences: () Latent Space Oddity: on the Curvature of Deep Generative Models Georgios Arvanitidis, Lars Kai Hansen and Søren Hauberg. In International Conference on Learning Representations (ICLR), 2018. () Only Bayes should learn a manifold (on the estimation of differential geometric structure from data) Søren Hauberg."
  },
  {
    "objectID": "logml2021/projects2021/project12/index.html",
    "href": "logml2021/projects2021/project12/index.html",
    "title": "Implicit Node and Edge Features for More Expressive Graph Neural Networks",
    "section": "",
    "text": "Octabian is broadly interested in representation learning for unstructured data (graphs), 3D objects (e.g. molecules), text or images through statistical or geometric models that could be devised and understood in a mathematically principled and elegant manner. In particular, he explored non-Euclidean geometries in Machine Learning to overcome some of the current difficulties in graph representation learning and generation, e.g. finding and learning latent hierarchical structures in data via hyperbolic geometry, as well as combining optimal transport and graph neural networks for better models that deal with graphs. He is currently applying my models to problems related to computational chemistry such as drug discovery"
  },
  {
    "objectID": "logml2021/projects2021/project12/index.html#octavian-eugen-ganea",
    "href": "logml2021/projects2021/project12/index.html#octavian-eugen-ganea",
    "title": "Implicit Node and Edge Features for More Expressive Graph Neural Networks",
    "section": "",
    "text": "Octabian is broadly interested in representation learning for unstructured data (graphs), 3D objects (e.g. molecules), text or images through statistical or geometric models that could be devised and understood in a mathematically principled and elegant manner. In particular, he explored non-Euclidean geometries in Machine Learning to overcome some of the current difficulties in graph representation learning and generation, e.g. finding and learning latent hierarchical structures in data via hyperbolic geometry, as well as combining optimal transport and graph neural networks for better models that deal with graphs. He is currently applying my models to problems related to computational chemistry such as drug discovery"
  },
  {
    "objectID": "logml2021/projects2021/project12/index.html#project",
    "href": "logml2021/projects2021/project12/index.html#project",
    "title": "Implicit Node and Edge Features for More Expressive Graph Neural Networks",
    "section": "Project",
    "text": "Project\nGraph Neural Networks (GNN) have been achieving state-of-the-art performance in various graph related tasks, being the first adopted solution especially when graphs have node and edge features. However, GNNs have several difficulties [4], such as capturing long-range graph interactions (due to the oversquashing effect) or differentiating locally isomorphic nodes [5] (e.g. based on the WL test). Moreover, GNNs haven’t yet been reconciled or combined with positional independent node embedding (PINE) approaches such as Node2Vec [1] or distortion based embeddings (e.g. Poincare embeddings [2]). The latter are known to capture well long-range graph interactions, can be trained fully unsupervised, but are not uniquely defined as they can be arbitrarily transformed with a shared invertible matrix while keeping the loss value unchanged. In this project, we propose to explore combining GNNs and PINEs in a joint end-to-end supervised trainable method by leveraging the power of implicit differentiation (ID) [3] traditionally used in meta-learning approaches. Given an input graph, we will create an implicit layer that learns PINEs based on an unsupervised objective (e.g. distortion loss), and these will in turn become node features that will be the input of a GNN. Importantly, using ID, we can backpropagate through the PINE training procedure and, thus, obtain meaningful PINE features for the downstream task at hand. This would allow us to obtain globally (at graph level) correlated node features for GNNs, to differentiate non-isomorphic graphs otherwise indistinguishable by the WL test, and to reflect on other (unsupervised) inductive biases useful for specific downstream graph problems.\n[1] Node2Vec: https://snap.stanford.edu/node2vec/ [2] Poincare Embeddings: https://arxiv.org/pdf/1705.08039.pdf [3] ID Neurips 2020 tutorial: https://www.youtube.com/watch?v=MX1RJELWONc [4] GNN difficulties: https://arxiv.org/pdf/2006.05205.pdf , https://arxiv.org/abs/2006.13318, rb.gy/quo3n6 [5] https://www.mit.edu/~vgarg/GNNs_FinalVersion.pdf"
  },
  {
    "objectID": "logml2021/projects2021/project14/index.html",
    "href": "logml2021/projects2021/project14/index.html",
    "title": "Morphing of manifold-valued images",
    "section": "",
    "text": "Marie-Julie Rakotosaona is a PhD candidate at Ecole Polytechnique in the GeoVic team where she is advised by Maks Ovsjanikov. Her research focuses on 3D shape analysis and processing. She is working on finding well suited deep learning methods and representations for processing, understanding, and creating 3D shapes."
  },
  {
    "objectID": "logml2021/projects2021/project14/index.html#marie-julie-rakotosaona",
    "href": "logml2021/projects2021/project14/index.html#marie-julie-rakotosaona",
    "title": "Morphing of manifold-valued images",
    "section": "",
    "text": "Marie-Julie Rakotosaona is a PhD candidate at Ecole Polytechnique in the GeoVic team where she is advised by Maks Ovsjanikov. Her research focuses on 3D shape analysis and processing. She is working on finding well suited deep learning methods and representations for processing, understanding, and creating 3D shapes."
  },
  {
    "objectID": "logml2021/projects2021/project14/index.html#project",
    "href": "logml2021/projects2021/project14/index.html#project",
    "title": "Morphing of manifold-valued images",
    "section": "Project",
    "text": "Project\nExisting learning-based methods for mesh reconstruction from fixed point clouds mostly generate triangles individually, making it hard to create consistent manifold meshes. In this project, we will build a novel method aimed at estimating local surfaces from point clouds and constructing high quality meshes. We are interested in leveraging the properties of 2D Delaunay triangulations to construct a mesh from small manifold surface elements. The method first learns local logarithmic maps around estimated geodesic neighborhoods centered at each point, from which we can compute manifold Delaunay triangulation. The local 2D projections are then synchronized to maximize the manifoldness of the global reconstructed mesh.\nDuring this week, we will first build a robust learning-based pipeline to mesh point clouds. Throughout this process, you will gain significant familiarity with or get a deeper understanding of basic concepts in geometry for representing 3D shapes as well as existing tools for machine learning on point clouds such as PointNet or FoldingNet. In the second stage of this project, we will explore novel directions to improve the proposed method and build tools for both meshing and analysis of 3D surfaces."
  },
  {
    "objectID": "logml2021/speaker.html",
    "href": "logml2021/speaker.html",
    "title": "Speakers",
    "section": "",
    "text": "Keynote speakers\n\n\n\n\n\n\n\n\n\n\nMichael Bronstein\n\n\nImperial College London, Twitter\n\n\n\n\n\n\n\n\n\n\n\n\n\nUlrike Tillmann FRS\n\n\nUniversity of Oxford\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nSpeakers\n\n\n\n\n\n\n\n\n\n\nGabriele Steidl\n\n\nTechnische Universität Berlin\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaks Ovsjanikov\n\n\nÉcole Polytechnique\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarinka Zitnik\n\n\nHarvard University\n\n\n\n\n\n\n\n\n\n\n\n\n\nMartin Rumpf\n\n\nUniversität Bonn\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaximilian Nickel\n\n\nFacebook AI Research\n\n\n\n\n\n\n\n\n\n\n\n\n\nMichael Betancourt\n\n\nImperial College London, Twitter\n\n\n\n\n\n\n\n\n\n\n\n\n\nNiloy J. Mitra\n\n\nUniversity College London\n\n\n\n\n\n\n\n\n\n\n\n\n\nRuriko Yoshida\n\n\nNaval Postgraduate School, California\n\n\n\n\n\n\n\n\n\n\n\n\n\nSanja Fidler\n\n\nUniversity of Toronto, Vector Institute, Nvidia\n\n\n\n\n\n\n\n\n\n\n\n\n\nThomas Kipf\n\n\nGoogle Brain\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Archives",
      "LOGML 2021",
      "Speakers"
    ]
  },
  {
    "objectID": "logml2021/speakers2021/keynote/ulrike-tillmann.html",
    "href": "logml2021/speakers2021/keynote/ulrike-tillmann.html",
    "title": "Ulrike Tillmann FRS",
    "section": "",
    "text": "Home\n    Speakers\n    Ulrike Tillmann FRS\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nUlrike Tillmann FRS is a mathematician working on topology and geometry. Much of her recent work in topology is motivated by questions from topological data analysis and data science.\nShe obtained her PhD at Stanford University, Habilitation at Universität Bonn, and now works as a professor of mathematics at the university of Oxford. She is on the editorial boards for a range of mathematical journals, including being the founding and managing editor for the Journal of Topology. She was awarded the Whitehead Prize by the London Mathematical Society (LMS) in 2004, a Bessel Forschungspreis in 2008, was elected an inaugural fellow of the American Mathematical Society in 2012, and has been a Fellow of the Royal Society since 2008.\nApart from that, she is active in outreach and for women in mathematics, for example acting as EMS–EWM Steering Committee Chair for summer schools at the Mittag-Leffler Institute 2015–20 and leading groups at Women in Topology and Women in Mathematical Physics."
  },
  {
    "objectID": "logml2021/speakers2021/other/ruriko-yoshida.html",
    "href": "logml2021/speakers2021/other/ruriko-yoshida.html",
    "title": "Ruriko Yoshida",
    "section": "",
    "text": "Home\n    Speakers\n    Ruriko Yoshida\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nDr. Ruriko Yoshida is an Associate Professor of Operations Research at the Naval Postgraduate School. Her research topics cover a wide variety of areas: applications of algebraic combinatorics to statistical problems, such as goodness of fit tests, optimized camera placement in sensor networks, phylogenetics, and phylogenomics. Dr. Yoshida received her Ph.D. (2004) in Mathematics from the University of California, Davis. She then went to the University of California, Berkeley as a postdoctoral researcher, and then Duke University for her postdoctoral research from 2004 to 2006. She was at the University of Kentucky from 2006 to 2016 as an assistant and then as associate professor. In 2016, she joined the operations research department at the Naval Postgraduate School."
  },
  {
    "objectID": "logml2021/speakers2021/other/maximilian-nickel.html",
    "href": "logml2021/speakers2021/other/maximilian-nickel.html",
    "title": "Maximilian Nickel",
    "section": "",
    "text": "Home\n    Speakers\n    Maximilian Nickel\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nMax is a Research Scientist at Facebook AI Research in New York, working on foundational methods for learning from structured and temporal information and their applications in artificial intelligence, network science, and computational social science. Before joining FAIR in 2016, he was a postdoctoral fellow at MIT where he was with the Laboratory for Computational and Statistical Learning and the Center for Brains, Minds and Machines. In 2013, Max earned his PhD with summa cum laude from the Ludwig Maximilian University in Munich. From 2010 to 2013 he worked as a research assistant at Siemens Corporate Technology."
  },
  {
    "objectID": "logml2021/speakers2021/other/sanja-fidler.html",
    "href": "logml2021/speakers2021/other/sanja-fidler.html",
    "title": "Sanja Fidler",
    "section": "",
    "text": "Home\n    Speakers\n    Sanja Fidler\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nSanja is an Associate Professor at University of Toronto, and a Director of AI at NVIDIA, leading a research lab in Toronto. Prior coming to Toronto, in 2012/2013, she was a Research Assistant Professor at Toyota Technological Institute at Chicago, an academic institute located in the campus of University of Chicago. She did my postdoc with Prof. Sven Dickinson at University of Toronto in 2011/2012. She finished her PhD in 2010 at University of Ljubljana in Slovenia in the group of Prof. Ales Leonardis. In 2010, she was visiting Prof. Trevor Darrell’s group at UC Berkeley and ICSI. She got my BSc degree in Applied Math at University of Ljubljana."
  },
  {
    "objectID": "logml2021/speakers2021/other/martin-rumpf.html",
    "href": "logml2021/speakers2021/other/martin-rumpf.html",
    "title": "Martin Rumpf",
    "section": "",
    "text": "Home\n    Speakers\n    Martin Rumpf\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nMartin Rumpf studied mathematics at Bonn University. He did his PhD in 1992 and was scientific assistant from 1993 to 1996 at the University of Freiburg. In 1996 he became associate professor at Bonn University and in 2001 full professor at Duisburg university. Since 2004 he is full professor for mathematics at Bonn University. He is studying Riemannian calculus on shape spaces with applications in imaging and in computer graphics. Furthermore, he is interested in microstructures in material science, in shape optimization and in adaptive finite element methods."
  },
  {
    "objectID": "logml2021/speakers2021/other/gabriele-steidl.html",
    "href": "logml2021/speakers2021/other/gabriele-steidl.html",
    "title": "Gabriele Steidl",
    "section": "",
    "text": "Home\n    Speakers\n    Gabriele Steidl\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nGabriele Steidl received her PhD and Habilitation in Mathematics from the University of Rostock (Germany), in 1988 and 1991, respectively. From 1992 to 1993 she worked as a consultant at the Verband Deutscher Rentenversicherungsträger in Frankfurt am Main. Between 1993 and 2020 she held professorships at the Departments of Mathematics at the TU Darmstadt, University of Mannheim, and TU Kaiserslautern, and was Consultant of the Fraunhofer Institute for Industrial Mathematics. Since 2020, she is Professor at the Department of Mathematics at the TU Berlin. She worked as a Postdoc at the University of Debrecen (Hungary), the Banach Center Warsaw and the University of Zürich and was a Visiting Professor at the ENS Paris/Cachan and the Université Paris East Marne-la-Vallée and the Sorbonne. Since 2020 she is a member of the DFG Fachkollegium Mathematik and the Program Director of SIAG-IS (SIAM). She is a member of the Editorial board of Journal of Mathematical Imaging and Vision, SIAM Journal on Imaging Sciences, The Journal of Fourier Analysis, Inverse Problems and Imaging, Journal of Optimization Theory and Applications, Transactions in Mathematics and its Applications, Acta Applicandae Mathematicae (ACAP), and Sampling Theory, Signal Processing and Data Analysis."
  },
  {
    "objectID": "speakers/other/bastian.html",
    "href": "speakers/other/bastian.html",
    "title": "Bastian Rieck",
    "section": "",
    "text": "Home\n    People\n    Bastian Reck \n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website"
  },
  {
    "objectID": "speakers/keynote/heather.html",
    "href": "speakers/keynote/heather.html",
    "title": "Heather Harrington",
    "section": "",
    "text": "Home\n    People\n    Heather Harrington\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website"
  },
  {
    "objectID": "logml2021/projects.html",
    "href": "logml2021/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Characterising Universes in String Theory using Geometric Learning\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nChallenger Mishra\n\n\n\n\n\n\n\n\n\n\n\n\nCoarsening disassortative graphs\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nDaniele Grattarola\n\n\n\n\n\n\n\n\n\n\n\n\nEfficient Fully Fourier Spherical Convolutional Networks\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nShubhendu Trivedi\n\n\n\n\n\n\n\n\n\n\n\n\nGeometric Learning on Shapes and Distributions with Optimal Transport\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nJean Feydy\n\n\n\n\n\n\n\n\n\n\n\n\nGeometry of HMC and Geometric Integration for Sampling and Optimization\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nAlessandro Barp\n\n\n\n\n\n\n\n\n\n\n\n\nImplicit Node and Edge Features for More Expressive Graph Neural Networks\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nOctavian-Eugen Ganea\n\n\n\n\n\n\n\n\n\n\n\n\nImplicit planner GNNs for continuous control\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nAndreea Deac\n\n\n\n\n\n\n\n\n\n\n\n\nImproved expressive power for message-passing networks via subgraph aggregation\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nHaggai Maron\n\n\n\n\n\n\n\n\n\n\n\n\nInvestigating Differentiable Graph Module\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nAnees Kazi\n\n\n\n\n\n\n\n\n\n\n\n\nManifold optimization and recent applications\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nBamdev Mishra\n\n\n\n\n\n\n\n\n\n\n\n\nMorphing of manifold-valued images\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nSebastian Neumayer\n\n\n\n\n\n\n\n\n\n\n\n\nMorphing of manifold-valued images\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nMarie-Julie Rakotosaona\n\n\n\n\n\n\n\n\n\n\n\n\nNavigating text adventures with algorithmic reasoners\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nPetar Veličković\n\n\n\n\n\n\n\n\n\n\n\n\nPlatonic CNNs\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nTaco Cohen\n\n\n\n\n\n\n\n\n\n\n\n\nPretraining graph neural networks with ELECTRA\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nWengong Jin\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-supervised non-rigid correspondence by geodesic distortion minimization using the deformation field\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nSøren Hauberg\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-supervised non-rigid correspondence by geodesic distortion minimization using the deformation field\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nOshri Halimi\n\n\n\n\n\n\n\n\n\n\n\n\nStability or Collapse: Topological Properties of Deep Autoencoders\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nKelly Spendlove\n\n\n\n\n\n\n\n\n\n\n\n\nSurface reconstruction from point clouds\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nRana Hanocka\n\n\n\n\n\n\n\n\n\n\n\n\nUncovering and correcting biases in neuroimaging studies\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nIra Ktena\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Archives",
      "LOGML 2021",
      "Projects"
    ]
  },
  {
    "objectID": "logml2021/speakers2021/other/niloy-mitra.html",
    "href": "logml2021/speakers2021/other/niloy-mitra.html",
    "title": "Niloy J. Mitra",
    "section": "",
    "text": "Home\n    Speakers\n    Niloy J. Mitra\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nNiloy Mitra is a Professor of Geometry Processing in the Department of Computer Science, University College London (UCL). He received his MS and PhD in Electrical Engineering from Stanford University under the guidance of Leonidas Guibas and Marc Levoy, and was a postdoctoral scholar with Helmut Pottmann at Technical University Vienna. His research interests include shape analysis, computational design and fabrication, and geometry processing. For details, please visit the SmartGeometryProcessing page. Niloy received the 2013 ACM Siggraph Significant New Researcher Award for “his outstanding work in discovery and use of structure and function in 3D objects” (UCL press release), the BCS Roger Needham award (BCS press release) in 2015, and the Eurographics Outstanding Technical Contributions Award in 2019. He received the ERC Starting Grant on SmartGeometry in 2013. His work has twice been featured as research highlights in the Communications of the ACM, twice been selected by ACM Siggraph/Siggraph Asia (both in 2017) for press release as research highlight. Besides research, Niloy is an active DIYer and loves reading, bouldering, and cooking."
  },
  {
    "objectID": "logml2021/speakers2021/other/maks-ovsjanikov.html",
    "href": "logml2021/speakers2021/other/maks-ovsjanikov.html",
    "title": "Maks Ovsjanikov",
    "section": "",
    "text": "Home\n    Speakers\n    Maks Ovsjanikov\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nMaks Ovsjanikov is a professor of computer science at Ecole Polytechnique. Prior to this, he worked for Google in their Image Search team in Mountain View. In 2011, he graduated from Stanford University with a Ph.D. in Computational Mathematics (from the ICME department), having done work on shape analysis in the geometric computing lab headed by Prof. Leonidas Guibas. His research interests are in geometric data analysis, and especially in the analysis and processing of deformable 3D shapes, with an emphasis on Deep Learning for non-rigid shape comparison and processing. Over the course of his career, Prof. Ovjsanikov introduced several key concepts in shape analysis, including the Heat Kernel Signature (cited over 1400 times, led to a Wikipedia article), algorithms for isometric shape matching, and Functional Maps. His research is the object of several international patents widely adopted in the industry. He received the Eurographics Young Researcher Award in 2014, and a Bronze medal from the CNRS in 2018."
  },
  {
    "objectID": "logml2021/speakers2021/other/thomas-kipf.html",
    "href": "logml2021/speakers2021/other/thomas-kipf.html",
    "title": "Thomas Kipf",
    "section": "",
    "text": "Home\n    Speakers\n    Thomas Kipf\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nThomas Kipf is a Research Scientist at Google Research in the Brain Team in Amsterdam. Prior to joining Google, he completed his PhD at University of Amsterdam under Prof. Max Welling on the topic “Deep Learning with Graph-Structured Representations”. His research interests lie in the area of relational learning and in developing models that can reason about the world in terms of structured abstractions such as objects or events."
  },
  {
    "objectID": "logml2021/speakers2021/other/michael-betancourt.html",
    "href": "logml2021/speakers2021/other/michael-betancourt.html",
    "title": "Michael Betancourt",
    "section": "",
    "text": "Home\n    Speakers\n    Michael Betancourt\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nMichael Betancourt is the principal research scientist with Symplectomorphic, LLC where he develops theoretical and methodological tools to support practical Bayesian inference. He is also a core developer of Stan, where he implements and tests these tools. In addition to hosting tutorials and workshops on Bayesian inference with Stan he also collaborates on analyses in epidemiology, pharmacology, and physics, amongst others. Before moving into statistics, Michael earned a B.S. from the California Institute of Technology and a Ph.D. from the Massachusetts Institute of Technology, both in physics."
  },
  {
    "objectID": "logml2021/speakers2021/other/marinka-zitnik.html",
    "href": "logml2021/speakers2021/other/marinka-zitnik.html",
    "title": "Marinka Zitnik",
    "section": "",
    "text": "Home\n    Speakers\n    Marinka Zitnik\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nMarinka Zitnik is an Assistant Professor at Harvard with appointments in the Department of Biomedical Informatics, Blavatnik Institute, Broad Institute of MIT and Harvard, and Harvard Data Science Initiative. Dr. Zitnik is a computer scientist studying applied machine learning with a focus on challenges brought forward by data in science, medicine, and health. She has published extensively on the topics of representation learning, knowledge graphs, network science, and graph ML in top-tier AI venues (NeurIPS, JMLR, IEEE TPAMI, KDD, ICLR), top-tier bioinformatics venues (Bioinformatics, ISMB, RECOMB), and journals (Nature Methods, Nature Communications, PNAS). Some of her methods are used by major institutions, including Baylor College of Medicine, Karolinska Institute, Stanford Medical School, and Massachusetts General Hospital. Her work received several best paper, poster, and research awards from the International Society for Computational Biology. She has recently been named a Rising Star in EECS by MIT and also a Next Generation in Biomedicine by the Broad Institute, being the only young scientist who received such recognition in both EECS and Biomedicine."
  },
  {
    "objectID": "logml2021/speakers2021/keynote/michael-bronstein.html",
    "href": "logml2021/speakers2021/keynote/michael-bronstein.html",
    "title": "Michael Bronstein",
    "section": "",
    "text": "Home\n    Speakers\n    Michael Bronstein\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nScientist to the World Economic Forum, an honor bestowed on forty world’s leading scientists under the age of forty. Michael is a Fellow of IEEE and IAPR, alumnus of the Technion Excellence Program and the Academy of Achievement, ACM Distinguished Speaker, and a member of the Young Academy of Europe. In addition to academic work, Michael’s industrial experience includes technological leadership in multiple startup companies, including Novafora, Videocites, and Invision (acquired by Intel in 2012), and Fabula AI (acquired by Twitter in 2019). Following the acquisition of Fabula, he joined Twitter as Head of Graph Learning Research. He previously served as Principal Engineer at Intel Perceptual Computing (2012-2019) and was one of the key developers of the Intel RealSense 3D camera technology."
  },
  {
    "objectID": "logml2021/projects2021/project13/index.html",
    "href": "logml2021/projects2021/project13/index.html",
    "title": "Morphing of manifold-valued images",
    "section": "",
    "text": "Sebastian Neumayer is a researcher at TU Berlin in the final phase of his PhD under the supervision of Gabriele Steidl. Before coming to Berlin, he worked as a researcher at TU Kaiserslautern, where he also did his Bachelor and Master studies. His research interests are focused around image analysis with a special focus on motion models, manifold valued images and also inverse problems. Recently, he began to investigate the relation of such classical tasks with machine learning, which seems like a very promising direction of future research."
  },
  {
    "objectID": "logml2021/projects2021/project13/index.html#sebastian-neumayer",
    "href": "logml2021/projects2021/project13/index.html#sebastian-neumayer",
    "title": "Morphing of manifold-valued images",
    "section": "",
    "text": "Sebastian Neumayer is a researcher at TU Berlin in the final phase of his PhD under the supervision of Gabriele Steidl. Before coming to Berlin, he worked as a researcher at TU Kaiserslautern, where he also did his Bachelor and Master studies. His research interests are focused around image analysis with a special focus on motion models, manifold valued images and also inverse problems. Recently, he began to investigate the relation of such classical tasks with machine learning, which seems like a very promising direction of future research."
  },
  {
    "objectID": "logml2021/projects2021/project13/index.html#project",
    "href": "logml2021/projects2021/project13/index.html#project",
    "title": "Morphing of manifold-valued images",
    "section": "Project",
    "text": "Project\nIn this project, we deal with manifold-valued images, which occur, e.g., in DT-MRI (values are symmetric positive definite matrices) or material science (EBSD data describing grain orientation in materials). As the potential applications are becoming more and more, many classical imaging problems have also been investigated for the manifold setting. While some problems are easy to generalize, others require more attention. One specific issue popping up is that the values at each pixel are not rotation invariant anymore (rotating a point by 90 degrees changes nothing, rotating an arrow pointing in some direction by 90 degrees also changes its direction). Hence, some applications can not be tackled with the straight-forward extension of Euclidean transformation models. Originally, we have proposed a metamorphosis (or morphing) model for manifold-valued images which does note take this observation into account. Extending the model and deriving similar theoretical results as for the original one incorporating this observation will be the main goal of this project. Part of the project is devoted to the numerical implementation of the model. There is some Matlab Code available for the original version, but we can clearly also start from scratch in Python if you prefer. Being already a bit familiar with one of the available manifold toolboxes is definitely beneficial for the project."
  },
  {
    "objectID": "logml2021/projects2021/project15/index.html",
    "href": "logml2021/projects2021/project15/index.html",
    "title": "Improved expressive power for message-passing networks via subgraph aggregation",
    "section": "",
    "text": "Haggai is a Research Scientist at NVIDIA Research. His main fields of interest are machine learning, optimization and shape analysis. More specifically, he is working on applying deep learning to irregular domains (e.g., graphs, point clouds, and surfaces) and graph/shape matching problems. He completed his Ph.D. in 2019 at the Weizmann Institute of Science under the supervision of Prof. Yaron Lipman."
  },
  {
    "objectID": "logml2021/projects2021/project15/index.html#haggai-maron",
    "href": "logml2021/projects2021/project15/index.html#haggai-maron",
    "title": "Improved expressive power for message-passing networks via subgraph aggregation",
    "section": "",
    "text": "Haggai is a Research Scientist at NVIDIA Research. His main fields of interest are machine learning, optimization and shape analysis. More specifically, he is working on applying deep learning to irregular domains (e.g., graphs, point clouds, and surfaces) and graph/shape matching problems. He completed his Ph.D. in 2019 at the Weizmann Institute of Science under the supervision of Prof. Yaron Lipman."
  },
  {
    "objectID": "logml2021/projects2021/project15/index.html#project",
    "href": "logml2021/projects2021/project15/index.html#project",
    "title": "Improved expressive power for message-passing networks via subgraph aggregation",
    "section": "Project",
    "text": "Project\nOwing to their scalability and simplicity, message-passing neural networks (MPNNS) are currently the leading architecture for deep learning on graph-structured data. However, [1,2] recently showed that these architectures have limited expressive power. The most famous example is that MPNNs cannot distinguish a graph that consists of two triangles and a graph that consists of a single cycle of length 6 (see attached figure). This limitation raises a fundamental question: is it possible to make MPNNs more expressive? Several recent works [3,4,5] suggested architectures that aim to address this problem.\nIn this project, we will develop a novel approach that tackles this problem. The key to our solution is the observation that while two graphs may not be distinguishable by an MPNN, it might be easy to find distinguishable subgraphs. Following that observation, we suggest treating each graph as a set of subgraphs generated according to some predefined rule, e.g., all graphs that can be obtained by removing one edge from the original graph. We propose to process this set using the DSS framework [6], which allows processing sets of symmetric elements. To deal with the possible computational burden, we will also consider efficient random sampling schemes to improve time complexity. We will study this new model’s theoretical properties, e.g., is it provably more expressive than other MPNNS? and evaluate its practical performance.\n[1] Morris, Christopher, et al. “Weisfeiler and leman go neural: Higher-order graph neural networks.” Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. No. 01. 2019.\n[2] Xu, Keyulu, et al. “How powerful are graph neural networks?.” arXiv preprint arXiv:1810.00826 (2018).\n[3] Abboud, Ralph, et al. “The Surprising Power of Graph Neural Networks with Random Node Initialization.” arXiv preprint arXiv:2010.01179 (2020).\n[4] Vignac, Clément, Andreas Loukas, and Pascal Frossard. “Building powerful and equivariant graph neural networks with structural message-passing.” arXiv e-prints (2020): arXiv-2006.\n[5] Bouritsas, Giorgos, et al. “Improving graph neural network expressivity via subgraph isomorphism counting.” arXiv preprint arXiv:2006.09252 (2020).\n[6] Maron, Haggai, et al. “On learning sets of symmetric elements.” International Conference on Machine Learning. PMLR, 2020."
  },
  {
    "objectID": "logml2021/projects2021/project7/index.html",
    "href": "logml2021/projects2021/project7/index.html",
    "title": "Efficient Fully Fourier Spherical Convolutional Networks",
    "section": "",
    "text": "I am a machine learning researcher at the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL), where I work with Professors Regina Barzilay and Tommi Jaakkola as part of the MIT Machine Learning for Pharmaceutical Discovery and Synthesis Consortium (MLPDS). Before this, between 2018-19, I was the NSF sponsored Institute Fellow at Brown University’s mathematics institute. I received my PhD in August 2018 for work on group-covariant neural networks, similarity learning and metric estimation."
  },
  {
    "objectID": "logml2021/projects2021/project7/index.html#shubhendu-trivedi",
    "href": "logml2021/projects2021/project7/index.html#shubhendu-trivedi",
    "title": "Efficient Fully Fourier Spherical Convolutional Networks",
    "section": "",
    "text": "I am a machine learning researcher at the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL), where I work with Professors Regina Barzilay and Tommi Jaakkola as part of the MIT Machine Learning for Pharmaceutical Discovery and Synthesis Consortium (MLPDS). Before this, between 2018-19, I was the NSF sponsored Institute Fellow at Brown University’s mathematics institute. I received my PhD in August 2018 for work on group-covariant neural networks, similarity learning and metric estimation."
  },
  {
    "objectID": "logml2021/projects2021/project7/index.html#project",
    "href": "logml2021/projects2021/project7/index.html#project",
    "title": "Efficient Fully Fourier Spherical Convolutional Networks",
    "section": "Project",
    "text": "Project\nA particularly successful and instructive special case of group equivariant neural networks is when the input consists of images painted on a sphere and the transformation to which we desire invariance are 3D rotations, leading to work on spherical CNNs. Approaches to the design of spherical CNNs include combined real and harmonic space methods (such as [1] and [2]) and fully Fourier space approaches (such as [3]). The former exploit efficient sampling theorems which ensure that the underlying 3D rotational symmetry is captured properly. However, they also lead to a somewhat unconventional architecture composed of repeated forward and backward Fourier transforms since the non-linearity is still applied pointwise in “real space”. However applications of the non-linearity in this manner also lead to errors which break strict rotational equivariance. On the other hand, fully Fourier approaches, which maintain a harmonic space representation of the input to the end, afford strict rotational equivariance, but are prohibitively expensive. In this project we will explore methods for speeding up such a harmonic space approach while still maintaining exact rotational equivariance. In particular, we will examine the channel structure of the approach of [3] building further off the approach of [4] to produce more efficient implementations.\n[1] Taco Cohen, Mario Geiger, Jonas Kohler, and Max Welling. Spherical CNNs. International Conference on Learning Representations, 2018.\n[2] Carlos Esteves, Christine Allen-Blanchette, Ameesh Makadia, and Kostas Daniilidis. Learning SO(3) Equivariant Representations with Spherical CNNs. European Conference on Computer Vision, 2018.\n[3] Risi Kondor, Zhen Lin and Shubhendu Trivedi, Clebsch-Gordan Nets: A Fully Fourier Space Spherical Convolutional Neural Network. IAdvances in Neural Information Processing Systems, 2018.\n[4] Oliver J. Cobb, Christopher G. R. Wallis, Augustine N. Mavor-Parker, Augustin Marignier, Matthew A. Price, Mayeul d’Avezac, Jason D. McEwen, Efficient Generalized Spherical CNNs, International Conference on Learning Representations, 2021."
  },
  {
    "objectID": "logml2021/projects2021/project8/index.html",
    "href": "logml2021/projects2021/project8/index.html",
    "title": "Self-supervised non-rigid correspondence by geodesic distortion minimization using the deformation field",
    "section": "",
    "text": "Oshri Halimi is a Ph.D. student in the electrical engineering faculty at Technion - Israel Institute of Technology, supervised by Prof. Ron Kimmel.\nHer research investigates geometric invariants and their application in computer vision and shapes analysis. In particular, she is interested in the interface between geometry and deep learning.\nShe published in top-tier conferences for computer vision (CVPR, ECCV) and organized workshops in the field: “iGDL 2020: Israeli Geometric Deep Learning Workshop” and “Learning and Processing of Geometric Visual Structures,” SIAM Conference on Imaging Science (SIAM-IS20). She was awarded the Israel Ministry of Science Jabotinsky Fellowship for Doctoral Students.\nShe holds B.Sc in physics and electrical engineering from Technion, which she graduated cum laude. She is an alumna of the Technion Excellence Program, the Archimedes Program in Technion Faculty of Chemistry, and a bronze medalist in the IChO. She served in Unit 8200."
  },
  {
    "objectID": "logml2021/projects2021/project8/index.html#oshri-halimi",
    "href": "logml2021/projects2021/project8/index.html#oshri-halimi",
    "title": "Self-supervised non-rigid correspondence by geodesic distortion minimization using the deformation field",
    "section": "",
    "text": "Oshri Halimi is a Ph.D. student in the electrical engineering faculty at Technion - Israel Institute of Technology, supervised by Prof. Ron Kimmel.\nHer research investigates geometric invariants and their application in computer vision and shapes analysis. In particular, she is interested in the interface between geometry and deep learning.\nShe published in top-tier conferences for computer vision (CVPR, ECCV) and organized workshops in the field: “iGDL 2020: Israeli Geometric Deep Learning Workshop” and “Learning and Processing of Geometric Visual Structures,” SIAM Conference on Imaging Science (SIAM-IS20). She was awarded the Israel Ministry of Science Jabotinsky Fellowship for Doctoral Students.\nShe holds B.Sc in physics and electrical engineering from Technion, which she graduated cum laude. She is an alumna of the Technion Excellence Program, the Archimedes Program in Technion Faculty of Chemistry, and a bronze medalist in the IChO. She served in Unit 8200."
  },
  {
    "objectID": "logml2021/projects2021/project8/index.html#project",
    "href": "logml2021/projects2021/project8/index.html#project",
    "title": "Self-supervised non-rigid correspondence by geodesic distortion minimization using the deformation field",
    "section": "Project",
    "text": "Project\nGeodesic distortion minimization has been demonstrated as an effective approach for self-supervised non-rigid correspondence; see “Unsupervised learning of dense shape correspondence‏” (CVPR 2019).\nThere, shape descriptors were represented by a deep network, and the network was optimized to minimize the geodesic distortion criteria of the correspondence, resulting through the Functional Maps pipeline.\nA novel method to differentiate the geodesic distortion with respect to the deformation field was introduced later in the paper “Limp: Learning latent shape representations with metric preservation priors” (ECCV 2020).\nThe proposed project aims at combining the observations in both mentioned publications. The goal is to represent the deformation field using a deep network and to optimize it to admit the following requirements:\n\nGeodesic distance preservation of the deformation field\nOverlapping between the deformed source shape and the target shape."
  },
  {
    "objectID": "logml2021/projects2021/project6/index.html",
    "href": "logml2021/projects2021/project6/index.html",
    "title": "Coarsening disassortative graphs",
    "section": "",
    "text": "Daniele is a Ph.D. student at the University of Lugano (CH) in the Graph Machine Learning Group. He holds a MSc in Computer Science and Engineering from Politecnico di Milano. His research is on graph neural networks and their applications to systems that change over time. He is also the main developer of Spektral, a library for graph deep learning in TensorFlow/Keras.\nDaniele is the co-host and manager of Smarter Podcast, a live streaming podcast in Italian that invites AI researchers from academia and industry to talk about their work and experience."
  },
  {
    "objectID": "logml2021/projects2021/project6/index.html#daniele-grattarola",
    "href": "logml2021/projects2021/project6/index.html#daniele-grattarola",
    "title": "Coarsening disassortative graphs",
    "section": "",
    "text": "Daniele is a Ph.D. student at the University of Lugano (CH) in the Graph Machine Learning Group. He holds a MSc in Computer Science and Engineering from Politecnico di Milano. His research is on graph neural networks and their applications to systems that change over time. He is also the main developer of Spektral, a library for graph deep learning in TensorFlow/Keras.\nDaniele is the co-host and manager of Smarter Podcast, a live streaming podcast in Italian that invites AI researchers from academia and industry to talk about their work and experience."
  },
  {
    "objectID": "logml2021/projects2021/project6/index.html#project",
    "href": "logml2021/projects2021/project6/index.html#project",
    "title": "Coarsening disassortative graphs",
    "section": "Project",
    "text": "Project\nMany real-world applications require us to study large-scale graphs which are computationally expensive to process and difficult to understand intuitively.\nTo solve this issue, graph coarsening (or, sometimes, “pooling”) techniques allow us to reduce the size of a graph while preserving its overall characteristics. Many works have been proposed recently to tackle this problem, especially in the field of graph neural networks, but it remains a challenging and open research direction.\nMost techniques for graph coarsening assume that the input graph is assortative, that is, a graph in which neighboring nodes are likely to be similar. Conversely, many real-world graphs are disassortative graphs in which connections are heterophilous. For example, scale-free graphs (in which nodes with small degree are likely to be connected to nodes with a high degree) are very frequent in nature and technology.\nIn this project, we will attempt to develop a coarsening algorithm for strongly disassortative graphs. This will require us to discard many assumptions that are usually exploited in the design of coarsening algorithms, and chiefly the assumption that clusters of nodes can be identified in densely connected communities.\nFirst, we will study the effect of typical coarsening algorithms when applied to disassortative graphs. Then, we will formulate our coarsening problem as an optimization to obtain a desired degree of disassortativity in the coarse graph. Finally, we will identify one or more solutions to the problem, either through procedural techniques or end-to-end learning"
  },
  {
    "objectID": "logml2021/projects2021/project17/index.html",
    "href": "logml2021/projects2021/project17/index.html",
    "title": "Pretraining graph neural networks with ELECTRA",
    "section": "",
    "text": "Wengong Jin is a Ph.D. student in MIT CSAIL advised by Regina Barzilay and Tommi Jaakkola. His research seeks to develop novel machine learning algorithms for structured data and utilize them to automate molecular science such as drug discovery, material design, and chemistry. He is particularly interested in deep generative models and graph neural networks. His work has been published in top ML venues like ICML, NeurIPS, ICLR, as well as top biology journals like Cell."
  },
  {
    "objectID": "logml2021/projects2021/project17/index.html#wengong-jin",
    "href": "logml2021/projects2021/project17/index.html#wengong-jin",
    "title": "Pretraining graph neural networks with ELECTRA",
    "section": "",
    "text": "Wengong Jin is a Ph.D. student in MIT CSAIL advised by Regina Barzilay and Tommi Jaakkola. His research seeks to develop novel machine learning algorithms for structured data and utilize them to automate molecular science such as drug discovery, material design, and chemistry. He is particularly interested in deep generative models and graph neural networks. His work has been published in top ML venues like ICML, NeurIPS, ICLR, as well as top biology journals like Cell."
  },
  {
    "objectID": "logml2021/projects2021/project17/index.html#project",
    "href": "logml2021/projects2021/project17/index.html#project",
    "title": "Pretraining graph neural networks with ELECTRA",
    "section": "Project",
    "text": "Project\nMolecular property prediction is an important task in cheminformatics. Current property prediction methods are based on graph neural networks and require a large amount of training data to achieve state-of-the-art performance. Unfortunately, most datasets in cheminformatic domains are small (e.g., less than 1000). On the other hand, pretraining methods have achieved great success in computer vision and natural language processing. In this project, we seek to investigate how to pretrain graph neural networks on a large collection of unlabeled molecules using ELECTRA (Clark et al., 2020). The goal is to learn a masked language model to generate corrupted molecules and train a discriminator to distinguish the real molecules from the fake molecules. The method will be evaluated on MoleculeNet benchmark (Wu et al., 2017) to test its empirical performance."
  },
  {
    "objectID": "logml2021/projects2021/project11/index.html",
    "href": "logml2021/projects2021/project11/index.html",
    "title": "Investigating Differentiable Graph Module",
    "section": "",
    "text": "Anees is currently a PhD student at the chair of Computer Aided Medical Procedure and Augmented Reality @ the Technical University of Munich working with Prof. Nassir Navab. Anees has worked on deep learning for medical applications with the main focus is on Geometric Deep Learning for Healthcare by providing solutions to brain-related disease diagnosis problems. She has worked towards solving technical challenges such a dealing with multiple graph scenario, graph structure heterogeneity. In 2019, Anees was awarded TUM-ICL incentive funding to collaborate with Prof. Michael Bronstein at Imperial College London. In this project, the team focused on the challenging problem of graph structure learning.\nHis research interest lie in the span of geometry and statistics. He develops machine learning techniques using geometric constructions, and works on the related numerical challenges. He is particularly interested in random geometries as they naturally appear in learning."
  },
  {
    "objectID": "logml2021/projects2021/project11/index.html#anees-kazi",
    "href": "logml2021/projects2021/project11/index.html#anees-kazi",
    "title": "Investigating Differentiable Graph Module",
    "section": "",
    "text": "Anees is currently a PhD student at the chair of Computer Aided Medical Procedure and Augmented Reality @ the Technical University of Munich working with Prof. Nassir Navab. Anees has worked on deep learning for medical applications with the main focus is on Geometric Deep Learning for Healthcare by providing solutions to brain-related disease diagnosis problems. She has worked towards solving technical challenges such a dealing with multiple graph scenario, graph structure heterogeneity. In 2019, Anees was awarded TUM-ICL incentive funding to collaborate with Prof. Michael Bronstein at Imperial College London. In this project, the team focused on the challenging problem of graph structure learning.\nHis research interest lie in the span of geometry and statistics. He develops machine learning techniques using geometric constructions, and works on the related numerical challenges. He is particularly interested in random geometries as they naturally appear in learning."
  },
  {
    "objectID": "logml2021/projects2021/project11/index.html#project",
    "href": "logml2021/projects2021/project11/index.html#project",
    "title": "Investigating Differentiable Graph Module",
    "section": "Project",
    "text": "Project\nGraph deep learning has recently emerged as a powerful technique especially in the medical domain. Graph-based methods have shown promising results on a broad spectrum of applications ranging from social science, biomedicine, and particle physics to computer vision, graphics, and chemistry. One of the current problems with most of the GCN based methods is the requirement of a pre-computed graph. However, in many scenarios, especially in the medical and healthcare domain, the graph may not be given.\nIn this project, we will explore how such a graph can be learned mainly for medical applications. Given the population of individuals, we focus on predicting the age and gender of each based on brain MRI features. The method is inspired by our recent work on the differentiable graph module [1, 2] which can be further extended to any problem where the graph is unknown and can be inferred from the input features. With this project, you get to learn how to define a graph learning problem in the non-/ medical domain and further collaboration with a brain mesh analysis project.\n[1] Kazi, A., Cosmo, L., Navab, N. and Bronstein, M., 2020. Differentiable graph module (dgm) graph convolutional networks. arXiv preprint arXiv:2002.04999.\n[2] Cosmo, L., Kazi, A., Ahmadi, S.A., Navab, N. and Bronstein, M., 2020, October. Latent-Graph Learning for Disease Prediction. In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 643-653). Springer, Cham."
  },
  {
    "objectID": "logml2021/projects2021/project20/index.html",
    "href": "logml2021/projects2021/project20/index.html",
    "title": "Platonic CNNs",
    "section": "",
    "text": "Taco Cohen is a machine learning research scientist at Qualcomm AI Research in Amsterdam and a PhD student at the University of Amsterdam, supervised by prof. Max Welling. He was a co-founder of Scyfer, a company focussed on active deep learning, acquired by Qualcomm in 2017. He holds a BSc in theoretical computer science from Utrecht University and a MSc in artificial intelligence from the University of Amsterdam (both cum laude). His research is focussed on understanding and improving deep representation learning, in particular learning of equivariant and disentangled representations, data-efficient deep learning, learning on non-Euclidean domains, and applications of group representation theory and non-commutative harmonic analysis, as well as deep learning based source compression. He has done internships at Google Deepmind (working with Geoff Hinton) and OpenAI. He received the 2014 University of Amsterdam thesis prize, a Google PhD Fellowship, ICLR 2018 best paper award for “Spherical CNNs”, and was named one of 35 innovators under 35 in Europe by MIT in 2018."
  },
  {
    "objectID": "logml2021/projects2021/project20/index.html#taco-cohen",
    "href": "logml2021/projects2021/project20/index.html#taco-cohen",
    "title": "Platonic CNNs",
    "section": "",
    "text": "Taco Cohen is a machine learning research scientist at Qualcomm AI Research in Amsterdam and a PhD student at the University of Amsterdam, supervised by prof. Max Welling. He was a co-founder of Scyfer, a company focussed on active deep learning, acquired by Qualcomm in 2017. He holds a BSc in theoretical computer science from Utrecht University and a MSc in artificial intelligence from the University of Amsterdam (both cum laude). His research is focussed on understanding and improving deep representation learning, in particular learning of equivariant and disentangled representations, data-efficient deep learning, learning on non-Euclidean domains, and applications of group representation theory and non-commutative harmonic analysis, as well as deep learning based source compression. He has done internships at Google Deepmind (working with Geoff Hinton) and OpenAI. He received the 2014 University of Amsterdam thesis prize, a Google PhD Fellowship, ICLR 2018 best paper award for “Spherical CNNs”, and was named one of 35 innovators under 35 in Europe by MIT in 2018."
  },
  {
    "objectID": "logml2021/projects2021/project20/index.html#project",
    "href": "logml2021/projects2021/project20/index.html#project",
    "title": "Platonic CNNs",
    "section": "Project",
    "text": "Project\nOmnidirectional signals occur in a wide variety of domains, from climate and weather science to astronomy and omnidirectional computer vision. Neural networks for omnidirectional data should respect the topological and geometric structure and symmetries of the signal domains (typically a sphere-like manifold). Many kinds of spherical CNNs have been developed, but typically these involve non-standard and costly operations that may be hard to implement. By contrast, the gauge equivariant Icosahedral CNN (1) is implemented by performing a standard conv2d over a collection of local charts concatenated into a feature map. One downside of the method is that it requires projecting the spherical signal to the icosahedron. On the other hand, the method is very efficient, and exactly equivariant to the rotational symmetries of the icosahedron."
  },
  {
    "objectID": "logml2021/projects2021/project3/index.html",
    "href": "logml2021/projects2021/project3/index.html",
    "title": "Navigating text adventures with algorithmic reasoners",
    "section": "",
    "text": "Petar Veličković is a Senior Research Scientist at DeepMind. He holds a PhD in Computer Science from the University of Cambridge (Trinity College), obtained under the supervision of Pietro Liò. His research interests involve devising neural network architectures that operate on nontrivially structured data (such as graphs), and their applications in algorithmic reasoning and computational biology. He has published relevant research in these areas at both machine learning venues (NeurIPS, ICLR, ICML-W) and biomedical venues and journals (Bioinformatics, PLOS One, JCB, PervasiveHealth). In particular, he is the first author of Graph Attention Networks—a popular convolutional layer for graphs—and Deep Graph Infomax—a scalable local/global unsupervised learning pipeline for graphs (featured in ZDNet). Further, his research has been used in substantially improving the travel-time predictions in Google Maps (covered by outlets including the CNBC, Endgadget, VentureBeat, CNET, the Verge and ZDNet)."
  },
  {
    "objectID": "logml2021/projects2021/project3/index.html#petar-veličković",
    "href": "logml2021/projects2021/project3/index.html#petar-veličković",
    "title": "Navigating text adventures with algorithmic reasoners",
    "section": "",
    "text": "Petar Veličković is a Senior Research Scientist at DeepMind. He holds a PhD in Computer Science from the University of Cambridge (Trinity College), obtained under the supervision of Pietro Liò. His research interests involve devising neural network architectures that operate on nontrivially structured data (such as graphs), and their applications in algorithmic reasoning and computational biology. He has published relevant research in these areas at both machine learning venues (NeurIPS, ICLR, ICML-W) and biomedical venues and journals (Bioinformatics, PLOS One, JCB, PervasiveHealth). In particular, he is the first author of Graph Attention Networks—a popular convolutional layer for graphs—and Deep Graph Infomax—a scalable local/global unsupervised learning pipeline for graphs (featured in ZDNet). Further, his research has been used in substantially improving the travel-time predictions in Google Maps (covered by outlets including the CNBC, Endgadget, VentureBeat, CNET, the Verge and ZDNet)."
  },
  {
    "objectID": "logml2021/projects2021/project3/index.html#project",
    "href": "logml2021/projects2021/project3/index.html#project",
    "title": "Navigating text adventures with algorithmic reasoners",
    "section": "Project",
    "text": "Project\nText-based adventure games, such as Zork, use natural language to describe their setup, receive actions from the player, and describe the effects of such actions. However, they typically also have a clear underlying geometric structure, which links different states (“rooms”) with each other as a result of taking actions (e.g. “moving”). This also endows them with a set of symmetries (e.g. it is often the case that going west, then going east, has the outcome of not changing the state). Hence, text adventures represent a fantastic test-bed for benchmarking agents on real-world noisy language data, while still grounding that data in a simplified underlying “world model”.\nNeural algorithmic reasoning (see Cappart et al., IJCAI’21; Section 3.3.) represents an emerging area of (graph) representation learning that seeks to emulate operations of classical algorithms and data structures within a high-dimensional latent space—allowing us to more directly execute classical algorithms over noisy data. A classical collection of algorithms shown to be efficiently learnable in this way are path-finding routines (such as the Bellman-Ford algorithm). Quickly solving text adventures typically involves (implicitly) constructing a map of the environment, determining the current and goal states in this map, then searching for a (shortest) path from the current state to the goal.\nAll of these components are tricky to do in a generic text adventure setting. To simplify, we will assume that the map is given to us. It is still non-trivial to map current and previous state descriptions to a particular current and goal state, and through the use of neural algorithmic reasoning, we will attempt to directly feed such natural language descriptions to an algorithm, and then use the output representations of that algorithm to find shortest paths to the required goal state.\nThe work will be based on the TextWorld environment, specifically the subset of coin-collecting environments proposed by Yuan, Côté et al. (EWRL’18), for which code and example agents are already available. While our focus will not necessarily be reinforcement learning, I anticipate that this will provide an excellent starting point. Neural path-finders will be trained by using the methodology proposed by Veličković et al. (ICLR’20) and deployed in the style of XLVIN (Deac et al., NeurIPS DeepRL’20)."
  },
  {
    "objectID": "logml2021/projects2021/project5/index.html",
    "href": "logml2021/projects2021/project5/index.html",
    "title": "Implicit planner GNNs for continuous control",
    "section": "",
    "text": "Andreea is a second year PhD student at Mila/University of Montreal with Prof Jian Tang. She is broadly interested in how learning can be improved through the use of graph representations, in particular in the context of reinforcement learning. She has previously worked on algorithmic alignment for implicit planning, as well as applications of graph representation learning to biotechnology, such as drug discovery and drug combinations."
  },
  {
    "objectID": "logml2021/projects2021/project5/index.html#andreea-deac",
    "href": "logml2021/projects2021/project5/index.html#andreea-deac",
    "title": "Implicit planner GNNs for continuous control",
    "section": "",
    "text": "Andreea is a second year PhD student at Mila/University of Montreal with Prof Jian Tang. She is broadly interested in how learning can be improved through the use of graph representations, in particular in the context of reinforcement learning. She has previously worked on algorithmic alignment for implicit planning, as well as applications of graph representation learning to biotechnology, such as drug discovery and drug combinations."
  },
  {
    "objectID": "logml2021/projects2021/project5/index.html#project",
    "href": "logml2021/projects2021/project5/index.html#project",
    "title": "Implicit planner GNNs for continuous control",
    "section": "Project",
    "text": "Project\nReal-world applications often require sequential decision making, and, in order to reason over longer time horizons, planning. One way of doing planning is by estimating the effects of actions and the values of the states. Then a plan can be constructed consisting of the actions which lead to overall highest possible value.\nFortunately, there is a known dynamic programming algorithm, Value Iteration (VI), which can find the optimal value function of a known Markov Decision Process (MDP). Even more, we know that the VI update is taking an expectation over the neighbouring states’ values, which is something a CNN can do in a grid, or, more generally, a graph neural network (GNN) in a graph. The GNN can then be used as part of the policy network. This has been directly validated in recent work, where GNNs have been successfully used to execute dynamic programming algorithms.\nIn this project, we will focus on the recently proposed eXecuted Latent Value Iteration Network (XLVIN) model [1], which provides us with one way to apply VI-style reasoning even if the underlying state graph is not known. While the XLVIN agent holds promise, it leaves several design decisions which have not been fully explored—perhaps the most interesting of which is generalising beyond discrete action spaces. By using a simple continuous control environment (such as LunarLander), we will first construct an XLVIN agent relying on simple MLP-based encoders. Then, we will take it one step further and bin the environment’s continuous action space, allowing for more fine-grained control.\nEventually, the number of bins may expand the computational budget of our planner (which normally applies an exhaustive breadth-first search strategy), so we will attempt various forms of exploratory planning. Ultimately, this can lead us to fully continuous action spaces, specified using, for example, a Gaussian distribution over various torque actions.\n[1] Deac A., Velickovic P., Milinkovic O., Bacon P., Tang J. et. al. 2020. “XLVIN: EXecuted Latent Value Iteration Nets.” http://arxiv.org/abs/2010.13146"
  },
  {
    "objectID": "sponsors.html",
    "href": "sponsors.html",
    "title": "Sponsors",
    "section": "",
    "text": "We gratefully acknowledge sponsorship from:"
  },
  {
    "objectID": "projects/Yidi-Qi.html",
    "href": "projects/Yidi-Qi.html",
    "title": "Calabi-Yau Metrics with U(1)-invariant Neural Networks",
    "section": "",
    "text": "Yidi Qi is a PhD student in the Department of Physics at Northeastern University, under the supervision of Fabian Ruehle. His research focuses on applying machine learning techniques to string theory and mathematics. He is also a junior investigator at the NSF AI Institute for Artificial Intelligence and Fundamental Interactions (IAIFI)."
  },
  {
    "objectID": "projects/Yidi-Qi.html#yidi-qi",
    "href": "projects/Yidi-Qi.html#yidi-qi",
    "title": "Calabi-Yau Metrics with U(1)-invariant Neural Networks",
    "section": "",
    "text": "Yidi Qi is a PhD student in the Department of Physics at Northeastern University, under the supervision of Fabian Ruehle. His research focuses on applying machine learning techniques to string theory and mathematics. He is also a junior investigator at the NSF AI Institute for Artificial Intelligence and Fundamental Interactions (IAIFI)."
  },
  {
    "objectID": "projects/Yidi-Qi.html#project",
    "href": "projects/Yidi-Qi.html#project",
    "title": "Calabi-Yau Metrics with U(1)-invariant Neural Networks",
    "section": "Project",
    "text": "Project\nCalabi-Yau manifolds are compact Kähler manifolds which admit a Ricci-flat metric. They play important roles in modern physics, notably as the extra dimensions in string theory. Knowing the explicit Ricci-flat metrics on Calabi-Yau manifolds is essential for constructing realistic models that describe our four-dimensional universe. However, finding such metrics requires solving the complex Monge-Ampère equation and it is generally believed that an analytical solution does not exist. Even solving it numerically has been proved to be challenging. Recent progress shows that specially designed physics-informed neural networks can help obtain numerical Calabi-Yau metrics much more efficiently. These networks must be invariant under a certain U(1)-action (which is the same as an SO(2)-action), and so far only one special architecture of invariant networks has been implemented. In this project, we will implement other architectures which are invariant under this U(1) group action. Because this gives much more flexibility, there is a chance that this can improve the performance of the physics-informed neural networks, therefore leading to better approximations for Calabi-Yau metrics."
  },
  {
    "objectID": "projects/Zhengang-Zhong.html",
    "href": "projects/Zhengang-Zhong.html",
    "title": "Geometry for Distribution Learning",
    "section": "",
    "text": "Zhengang Zhong is a third-year PhD student at Imperial College London. His research focuses on shape optimisation and data-driven stochastic optimal control. A particular emphasis has been placed on data-driven distributionally robust control, which encodes the knowledge about the uncertainty of the controlled system into safe control actions with the help of the information and optimal transport geometry. Before his PhD, Zhengang received his Diplom Ingenieur degree in Mechatronics at the Technical University of Dresden, Germany."
  },
  {
    "objectID": "projects/Zhengang-Zhong.html#zhengang-zhong",
    "href": "projects/Zhengang-Zhong.html#zhengang-zhong",
    "title": "Geometry for Distribution Learning",
    "section": "",
    "text": "Zhengang Zhong is a third-year PhD student at Imperial College London. His research focuses on shape optimisation and data-driven stochastic optimal control. A particular emphasis has been placed on data-driven distributionally robust control, which encodes the knowledge about the uncertainty of the controlled system into safe control actions with the help of the information and optimal transport geometry. Before his PhD, Zhengang received his Diplom Ingenieur degree in Mechatronics at the Technical University of Dresden, Germany."
  },
  {
    "objectID": "projects/Zhengang-Zhong.html#project",
    "href": "projects/Zhengang-Zhong.html#project",
    "title": "Geometry for Distribution Learning",
    "section": "Project",
    "text": "Project\nInformation and optimal transport geometry provide powerful tools for analyzing and understanding the characteristics of complex probability distributions, thereby fostering the development of fast and scalable methods for approximating these distributions. For example, with the help of optimal transport geometry, sampling problems can be viewed as a gradient flow with respect to the Wasserstein geometry [1].\nIn the project, we will conduct an experiment similar to section 5 in [2] and section 6 in [3]: we will solve Bayesian inference problems based on optimal transport geometry. Then we will compare the performance of Wasserstein variational inference with the methods using different metrics on the space of probability measures and classic MCMC methods based on various information geometries.\n[1] Garcia Trillos, N., B. Hosseini, and D. Sanz-Alonso. ““From Optimization to Sampling Through Gradient Flows.”” arXiv e-prints (2023): arXiv-2302.\n[2] Lambert, Marc, et al. ““Variational inference via Wasserstein gradient flows.”” Advances in Neural Information Processing Systems 35 (2022): 14434-14447.\n[3] Ambrogioni, Luca, et al. ““Wasserstein variational inference.”” Advances in Neural Information Processing Systems 31 (2018)."
  },
  {
    "objectID": "projects/Sahil-Manchanda.html",
    "href": "projects/Sahil-Manchanda.html",
    "title": "Learning to predict optimal solution value for NP-Hard Combinatorial problems",
    "section": "",
    "text": "Sahil Manchanda is a PhD scholar at the Department of Computer Science at the Indian Institute of Technology Delhi, working under the supervision of Prof. Sayan Ranu. Sahil works in the area of Learning to solve graph optimization problems with focus on Combinatorial Optimization, Graph Neural Networks, Lifelong Learning and Generative modeling. He is also interested in applications of Computer Science concepts in other fields such as Material Science and Hardware. He has interned at prestigious institutes such as the University of Tokyo, NAVER Labs- France and Qualcomm AI Research Amsterdam. His research works have been published in top conferences such as NeurIPS, AAAI, ICML, ECML-PKDD and LoG. Additionally he has one US patent granted to his name. Sahil has been the recipient of the Qualcomm Innovation Fellowship for the year 2022."
  },
  {
    "objectID": "projects/Sahil-Manchanda.html#sahil-manchanda",
    "href": "projects/Sahil-Manchanda.html#sahil-manchanda",
    "title": "Learning to predict optimal solution value for NP-Hard Combinatorial problems",
    "section": "",
    "text": "Sahil Manchanda is a PhD scholar at the Department of Computer Science at the Indian Institute of Technology Delhi, working under the supervision of Prof. Sayan Ranu. Sahil works in the area of Learning to solve graph optimization problems with focus on Combinatorial Optimization, Graph Neural Networks, Lifelong Learning and Generative modeling. He is also interested in applications of Computer Science concepts in other fields such as Material Science and Hardware. He has interned at prestigious institutes such as the University of Tokyo, NAVER Labs- France and Qualcomm AI Research Amsterdam. His research works have been published in top conferences such as NeurIPS, AAAI, ICML, ECML-PKDD and LoG. Additionally he has one US patent granted to his name. Sahil has been the recipient of the Qualcomm Innovation Fellowship for the year 2022."
  },
  {
    "objectID": "projects/Sahil-Manchanda.html#project",
    "href": "projects/Sahil-Manchanda.html#project",
    "title": "Learning to predict optimal solution value for NP-Hard Combinatorial problems",
    "section": "Project",
    "text": "Project\nRecently, a lot of interest has been shown in the ML community to learn to solve NP-Hard Combinatorial Optimization(CO) problems. The prime reasons being:\n\nDeep Learning models offer the advantage of speed-up(with good quality) due to GPU based acceleration which is expected to further enhance as GPU hardware improves.\nHeuristics can be learned/distilled directly from the data distribution, thus minimizing or eliminating the need for manually crafted designs.\n\nIn this project we take a different approach and aim to learn to predict the optimal values of NP-Hard Combinatorial problems. Combinatorial optimization solvers can be augmented with machine learning-based optimal solution value predictors to reduce the search space during the quest for precise and high-quality solutions[1]. Further, in some problems such as Graph Edit Distance, directly estimating the optimal value between 2 graphs is also very useful[2].\nRecently, one work[1] attempts to learn to predict optimal values for TSP and job-shop scheduling problems using a Graph Transformer. The paper has interesting results giving hope that with further enhancements and better modelling it might be possible to learn to predict the optimal value more accurately. However, to understand its applicability in practical scenarios, results on crucial aspects such as inference time, percentage errors against optimal solutions and scalability to large CO problem instances are not discussed in the paper. Further, analysis is also required on how does the cost-accuracy tradeoff vary as the model capacity changes especially w.r.t to Graph Transformer layers etc.\nGoal of project:\nThe goal of this project will be initially implement the paper[1] and find out the cost(running time) and error(%) trade off with different model capacities on a set of CO problems. Further, if time permits analyze how does the method scale for larger instances of CO problems such as TSP 100 and TSP 200.\nExecution Plan:\nThe paper mentions that it uses GraphGPS[3] Transformer from PyTorch Geometric. It is easy to use.\nI will provide instances for CO problems(Eg:- TSP for different sizes, Job-Shop Scheduling Problem etc.) and their optimal values to the students. The students will write code to train the parameters of the Graph Transformer. Analyze results w.r.t cost vs prediction accuracy(error) trade off and explore the Pareto frontier etc. If time permits then understand the generalization w.r.t problem size.\nFinally, based upon the results we get, we together hope to formulate and investigate an interesting problem :).\nReferences:\n[1] Wang, Tianze, Amir H. Payberah, and Vladimir Vlassov. “Graph Representation Learning with Graph Transformers in Neural Combinatorial Optimization.”\n[2] Ranjan, Rishabh, et al. “Greed: A neural framework for learning graph distance functions.” Advances in Neural Information Processing Systems 35 (2022): 22518-22530.\n[3] Rampášek, Ladislav, et al. “Recipe for a general, powerful, scalable graph transformer.” Advances in Neural Information Processing Systems 35 (2022): 14501-14515."
  },
  {
    "objectID": "projects/Dolores-Garcia.html",
    "href": "projects/Dolores-Garcia.html",
    "title": "Geometric GNNs for particle level reconstruction",
    "section": "",
    "text": "Dolores Garcia is a Senior Fellow in the Experimental Physical Department at CERN. She received her MSc in Theoretical Physics from Imperial College London, and her PhD in Telecommunications Engineering from University of Carlos III supervised by Joerg Widmer. Her research interests include equivariant machine learning, graph neural networks and the application of these to high energy physics."
  },
  {
    "objectID": "projects/Dolores-Garcia.html#dolores-garcia",
    "href": "projects/Dolores-Garcia.html#dolores-garcia",
    "title": "Geometric GNNs for particle level reconstruction",
    "section": "",
    "text": "Dolores Garcia is a Senior Fellow in the Experimental Physical Department at CERN. She received her MSc in Theoretical Physics from Imperial College London, and her PhD in Telecommunications Engineering from University of Carlos III supervised by Joerg Widmer. Her research interests include equivariant machine learning, graph neural networks and the application of these to high energy physics."
  },
  {
    "objectID": "projects/Dolores-Garcia.html#project",
    "href": "projects/Dolores-Garcia.html#project",
    "title": "Geometric GNNs for particle level reconstruction",
    "section": "Project",
    "text": "Project\nThe particle flow algorithm enables the reconstruction of the particle-level view of an event by integrating information from the entire detector, spanning from the tracker to the calorimeters. Machine learning (ML) holds promise in enhancing the quality of reconstruction by leveraging raw data and acquiring the ability to untangle complex events. In this project, the students will explore a geometric graph neural network approach for particle-level reconstruction, focusing on the simplified scenario of e+e- collisions. The primary objective is to investigate architectures capable of extracting complex geometric structures, particle showers, from geometric graphs generated through simulations. Specifically, the team will delve into understanding the significance of equivariance and locality for this problem. The project will involve crafting a representation, managing large graphs in a distributed manner, and assessing the physics outputs."
  },
  {
    "objectID": "projects/Sara-Veneziale.html",
    "href": "projects/Sara-Veneziale.html",
    "title": "Invariantly learning terminal singularities",
    "section": "",
    "text": "Sara Veneziale is a final year PhD student in the Department of Mathematics at Imperial College London, as part of the London School of Geometry and Number Theory. Her research is focused on applying machine learning and data analysis methods to problems arising from pure mathematics, with the aim of helping conjecture formulation. Before starting her PhD she studied Mathematics at the University of Warwick, where she completed her integrated masters in 2020."
  },
  {
    "objectID": "projects/Sara-Veneziale.html#sara-veneziale",
    "href": "projects/Sara-Veneziale.html#sara-veneziale",
    "title": "Invariantly learning terminal singularities",
    "section": "",
    "text": "Sara Veneziale is a final year PhD student in the Department of Mathematics at Imperial College London, as part of the London School of Geometry and Number Theory. Her research is focused on applying machine learning and data analysis methods to problems arising from pure mathematics, with the aim of helping conjecture formulation. Before starting her PhD she studied Mathematics at the University of Warwick, where she completed her integrated masters in 2020."
  },
  {
    "objectID": "projects/Sara-Veneziale.html#project",
    "href": "projects/Sara-Veneziale.html#project",
    "title": "Invariantly learning terminal singularities",
    "section": "Project",
    "text": "Project\nRecent work [1] uses a neural network to predict an important, but hard to track, property of geometrical shapes: that of having at worst terminal singularities. The network is later used to generate a lot of data, by replacing a computationally expensive routine with the neural network, to start sketching the landscape of a certain class of geometrical shapes.\nThe experiment in the paper is carried out on a special class of geometrical shapes, called toric varieties. These are highly symmetrical objects and their geometric information is summarised in a weight matrix. However, the space of weight matrices is subject to two group actions: one of the symmetric group shuffling the columns and one of GL_2(Z) acting on the left by multiplication. These two actions change the weight matrix but leave the actual geometrical object unchanged. In the paper, we identify a special form of a weight matrix (which we call the standard form) which precisely identifies one specific representative of each group orbit. Before training and testing, each weight matrix is transformed into the corresponding standard form, which allows us to avoid any symmetry problem in the design of the architecture of the model.\nThe aim of this project is to survey currently available invariant machine learning methods and apply them to this problem. The aim is to compare accuracy, training time, and training sample size with the results obtained by using the standard form. This is a very important analysis to perform. In fact, the results in the paper were carried out only for dimension eight and rank two geometrical objects (toric varieties) and the training of the model required more data than one might like, since it is relatively expensive to calculate. Finding that different methods and architectures perform better with less training data would greatly aid in replicating this result in different dimensions and ranks.\n[1] T. Coates, A. M. Kasprzyk, S. Veneziale. Machine learning detects terminal singularities. Thirty-seventh Conference on Neural Information Processing Systems (2023)."
  },
  {
    "objectID": "projects/Joshua-Robinson.html",
    "href": "projects/Joshua-Robinson.html",
    "title": "Powerful Graph Neural Networks for Relational Databases",
    "section": "",
    "text": "Joshua is a postdoctoral scholar at Stanford working with Jure Leskovec. He obtained his PhD from MIT CSAIL in 2023, where we was advised by Stefanie Jegelka and Suvrit Sra. His interests include 1) designing algorithms for learning over structured domains, such as graphs, eigenvectors, and relational databases, and 2) self-supervised representation learning. He was also a co-organizer of the first LoG conference."
  },
  {
    "objectID": "projects/Joshua-Robinson.html#joshua-robinson",
    "href": "projects/Joshua-Robinson.html#joshua-robinson",
    "title": "Powerful Graph Neural Networks for Relational Databases",
    "section": "",
    "text": "Joshua is a postdoctoral scholar at Stanford working with Jure Leskovec. He obtained his PhD from MIT CSAIL in 2023, where we was advised by Stefanie Jegelka and Suvrit Sra. His interests include 1) designing algorithms for learning over structured domains, such as graphs, eigenvectors, and relational databases, and 2) self-supervised representation learning. He was also a co-organizer of the first LoG conference."
  },
  {
    "objectID": "projects/Joshua-Robinson.html#project",
    "href": "projects/Joshua-Robinson.html#project",
    "title": "Powerful Graph Neural Networks for Relational Databases",
    "section": "Project",
    "text": "Project\nMuch of the world’s data is stored in relational databases, which contain multiple tables whose rows are connected via primary-foreign key relations [1]. Consequently, many interesting forecasting problems can be thought of as predictions on relational data (Who will win next weekend’s F1? Will patient A respond to treatment X?). Crucially, relational databases can be viewed as “relational graphs”, with one node per table row, and edges given by primary-foreign key relations. However, relational graphs are not arbitrary graphs. They are k-partite, and some of the node partitions have a fixed number of in- and out-edges per node. This suggests a need for specialized GNN architectures suited to this graph data. The aim of this project will be an initial scoping of the (in)suitability of existing GNNs for processing relational graphs. Depending on student interest, this project’s scope is both a) empirical: evaluating the performance of existing GNN models and components to determine best modeling practices (for testing we have recently released a benchmark suite: https://relbench.stanford.edu/), or b) theoretical: finding examples of relational data structures that existing GNNs cannot distinguish. In both cases, this lays the groundwork for designing more powerful graph networks for relational data.\n[1] Relational Deep Learning: Graph Representation Learning on Relational Databases, Fey et al. 2023 arXiv:2312.04615."
  },
  {
    "objectID": "projects/Lorenzo-Giusti.html",
    "href": "projects/Lorenzo-Giusti.html",
    "title": "Exploiting Graph Neural Networks for Prescriptive maintenance of CERN’s technical infrastructure",
    "section": "",
    "text": "Lorenzo Giusti is currently a senior research scientist at CERN working on geometric and topological deep learning for particle physics in the cryogenics group. Lorenzo holds a PhD in Data Science at La Sapienza, University of Rome, specialized in topological neural networks. His research includes a period of visiting at the University of Cambridge and as a research scientist intern at NASA’s Jet Propulsion Laboratory, where he led a project on Martian terrain modeling using spacecraft imagery and Neural Radiance Fields."
  },
  {
    "objectID": "projects/Lorenzo-Giusti.html#lorenzo-giusti",
    "href": "projects/Lorenzo-Giusti.html#lorenzo-giusti",
    "title": "Exploiting Graph Neural Networks for Prescriptive maintenance of CERN’s technical infrastructure",
    "section": "",
    "text": "Lorenzo Giusti is currently a senior research scientist at CERN working on geometric and topological deep learning for particle physics in the cryogenics group. Lorenzo holds a PhD in Data Science at La Sapienza, University of Rome, specialized in topological neural networks. His research includes a period of visiting at the University of Cambridge and as a research scientist intern at NASA’s Jet Propulsion Laboratory, where he led a project on Martian terrain modeling using spacecraft imagery and Neural Radiance Fields."
  },
  {
    "objectID": "projects/Lorenzo-Giusti.html#project",
    "href": "projects/Lorenzo-Giusti.html#project",
    "title": "Exploiting Graph Neural Networks for Prescriptive maintenance of CERN’s technical infrastructure",
    "section": "Project",
    "text": "Project\nCERN, the European Organization for Nuclear Research, is the largest centre for scientific research in particle physics, and it is known for its complex system of systems comprising advanced particle accelerators and detectors. To fulfill the physics program and deliver the required luminosity for the experiments, advanced tools are required to operate, maintain, and guide device consolidation. It is, therefore, critical to monitor the activities objectively and guide the implementation of strategies to improve performance, optimize costs and highlight key areas needing prioritization. Moreover, the availability of reliable, cost-effective, and energy-efficient sensors entails growing data that captures the underlying phenomena happening within the CERN’s technical infrastructure. To identify failures early and perform prescriptive maintenance of such a complex system of systems, this project aims to reveal hidden dependencies and approach the maintenance operations within the technical infrastructure of the largest particle accelerator complex of the world using graph neural networks."
  },
  {
    "objectID": "projects/Anna-Calissano.html",
    "href": "projects/Anna-Calissano.html",
    "title": "Matching graphs with spatial constrains",
    "section": "",
    "text": "Anna Calissano is a Chapman Fellow at the Department of Mathematics at Imperial College London. She conducts research on defining suitable geometrical embeddings and novel statistical tools for the analysis of set of graphs and networks. She works at the intersection of geometry, statistics, and computing. Her main applicational areas are urban planning and medical imaging. Anna received a PhD in Mathematics from Politecnico di Milano in 2021, under the supervision of Prof. Simone Vantini and Prof. Aasa Feragen (DTU). She worked as a postdoctoral researcher at INRIA (France) within the ERC Project Geometric Statistics leaded by Xavier Pennec."
  },
  {
    "objectID": "projects/Anna-Calissano.html#anna-calissano",
    "href": "projects/Anna-Calissano.html#anna-calissano",
    "title": "Matching graphs with spatial constrains",
    "section": "",
    "text": "Anna Calissano is a Chapman Fellow at the Department of Mathematics at Imperial College London. She conducts research on defining suitable geometrical embeddings and novel statistical tools for the analysis of set of graphs and networks. She works at the intersection of geometry, statistics, and computing. Her main applicational areas are urban planning and medical imaging. Anna received a PhD in Mathematics from Politecnico di Milano in 2021, under the supervision of Prof. Simone Vantini and Prof. Aasa Feragen (DTU). She worked as a postdoctoral researcher at INRIA (France) within the ERC Project Geometric Statistics leaded by Xavier Pennec."
  },
  {
    "objectID": "projects/Anna-Calissano.html#project",
    "href": "projects/Anna-Calissano.html#project",
    "title": "Matching graphs with spatial constrains",
    "section": "Project",
    "text": "Project\nProblem Statement: Given two graphs, finding a correspondence between the two sets of nodes has been broadly studied in the literature as graph matching. In many real-world applications, the matching is constrained: not every node can be assigned to every other node. Consider for example the brain structural connectivity of different subjects, a misalignment between brain parcels (i.e. nodes) can occur during the signal preprocessing. However, the only reasonable alignments are between neighboring parcels [1]. The goal of the project is to define a graph matching procedure with spatial constrains (e.g. the spatial distance on the cortex between parcels) on a set of structural connectivity matrices of healthy subjects from the Human Connectome Project.\nSolutions to explore: The problem can be addressed in different ways: (1) by adding a penalization term in the optimization procedure which depends on the spatial distance of the nodes (e.g. Rigid Graph Matching [2]); (2) by defining an optimization problem using the generators of the permutation group ([3]).\n[1] Calissano, A., Papadopoulo, T., Pennec, X., & Deslauriers-Gauthier, S. (2022). Graph Alignment Exploiting the Spatial Organisation Improves the Similarity of Brain Networks. In Human Brain Mapping (to appear)\n[2] Ravindra, V., Nassar, H., Gleich, D. F., & Grama, A. (2020). Rigid graph alignment. In Complex Networks and Their Applications VIII: Volume 1 Proceedings of the Eighth International Conference on Complex Networks and Their Applications COMPLEX NETWORKS 2019 8 (pp. 621-632). Springer International Publishing.\n[3] Coxeter, H. S., & Moser, W. O. (2013). Generators and relations for discrete groups (Vol. 14). Springer Science & Business Media."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nLOGML Summer School 2025\n",
    "section": "",
    "text": "LOGML Summer School 2025\n\n\nLondon, 7-11 July 2025\n\n\nLOGML (London Geometry and Machine Learning) aims to bring together mathematicians and computer scientists to collaborate on a variety of problems at the intersection of geometry and machine learning. There will be a selection of group projects, each overseen by an experienced mentor, talks by leading figures in the field, a variety of social events and a company networking night.\n\n\n\n\n\n\n\nMentor Applications (Open)\n\n\n\nApplications for group project mentors are now open! Kindly apply using this form. Mentors guide a small group of passionate early career researchers on a week-long project of their choosing at the intersection of geometry and machine learning.\nLOGML is not merely a summer school; it’s an incubator for innovation at the intersection of geometry and machine learning. Several working groups that started at LOGML went on to form longer-term collaborations leading to publications in notable conferences and journals, including:\n\nEqivariant Subgraph Aggregation Networks (ICLR 2022, Spotlight)\nTowards Training GNNs Using Explanation Directed Message Passing (LOG conference, 2022)\nUnsupervised Network Embedding Beyond Homophily (TMLR, 2022)\nEquivariant Mesh Attention Networks (TMLR, 2022)\nGeneralized Laplacian Positional Encoding for Graph Representation Learning  (Workshop on Symmetry and Geometry in Neural Representations, NeurIPS 2022)\nSurfing on the Neural Sheaf (Workshop on Symmetry and Geometry in Neural Representations, NeurIPS 2022)\nAccelerating Molecular Graph Neural Networks via Knowledge Distillation (Synergy of Scientific and Machine Learning Modeling workshop, ICML, 2023)\nImplicit Convolutional Kernels for Steerable CNNs (NeurIPS, 2023 )\n\nHere’s what you can anticipate:\n\nDeep dive into collaborative research: You’ll steer a group of typically five early-career researchers, working closely on a well-defined project. While 15 hours of the week are earmarked for project working time, the energy and enthusiasm often see groups dedicating more.\nDrive tangible outcomes: The LOGML experience is intensive, yet the time frame is concise. With only one week available, it’s essential to zero in on achievable milestones. In the past, mentors have found success in adapting existing algorithms to fresh datasets, implementing a pilot for a theoretical ideal, or laying the theoretical foundations for a longer-term project.\nEngage & enlighten: Apart from the project work, the week will be filled with lectures by leading figures in the field, and a company and networking night.\n\nApplications close on February 28th.\n\n\n\n\n\n\n\n\nParticipant Applications (Opening mid-February)\n\n\n\nThank you for your interest in our program. Applications will open in mid-February. We encourage you to check back for more details.\n\n\n\nYou can find a list of previous years’ projects and speakers under “Archives” above."
  },
  {
    "objectID": "logml2022.html",
    "href": "logml2022.html",
    "title": "Readings, lectures, and videos",
    "section": "",
    "text": "Each class session has a set of required readings that you should complete before watching the lecture."
  },
  {
    "objectID": "program.html",
    "href": "program.html",
    "title": "Schedule",
    "section": "",
    "text": "The provisional schedule of the events is available here. The timezone is London time. The talks are held in LT 308 (third floor) in the Huxley building.\nYou can add the events to your calendar via iCal.\n\n\nJoan Bruna: Quantitative Benefits of Rank in Attention Layers (Slides)\nAbstract: Attention-based mechanisms are widely used in machine learning, most prominently in transformers and graph neural networks. However, hyperparameters such as the rank of the attention matrices and the number of heads are kept nearly the same in all realizations of this architecture, without any theoretical justification. While the total number of parameters ( a proxy for `capacity’ in the language of modern scaling laws) only depends on the product of rank and number of heads, in this talk we will show dramatic tradeoffs between these two parameters. Namely, we present a simple and natural target function that can be represented using a single full-rank attention head for any context length, but that provably cannot be approximated by low-rank attention unless the number of heads is exponential in the embedding dimension, even for small context lengths. Moreover, we prove that, for short context lengths, adding depth allows the target to be approximated by low-rank attention. For long contexts, we conjecture that full-rank attention is necessary. Joint work with Noah Amsel and Gilad Yehudai.\nKathryn Hess Bellwald: Cochains are all you need (Slides)\nAbstract: I will present results from the thesis of my recently graduated student Kelly Maggs, a diverse collection of fruitful applications of the classical algebra-topological notion of “cochains” in signal processing, machine learning, and gene expression analysis. The first of these applications concerns the interplay between discrete Morse theory and combinatorial Hodge theory. The second involves the use of differential k-forms in Euclidean space to represent simplices in a simplicial complex and thus facilitate interpretability and geometric consistency in geometric deep learning, without message passing. The last application leads to a pipeline for detecting closed biological processes of various types (e.g., the cell cycle) from single-cell RNA seq data, based on persistent cohomology and lead-lag theory for embedded simplicial complexes. Before sketching these applications, I will introduce the notion of cochains and explain how they encode the interplay between geometry and topology.\nVishnu Jejjala: Deep Learning Topology (Slides)\nPietro Lio: Diffusion and geometric models for medicine\nPeter Lu: Tutorial on Optimal Transport (Slides)\nAnthea Monod: A Tropical Geometric Perspective on Deep Learning\nIslem Rekik: The landscape of generative GNNs in network neuroscience\nMichalis Vazirgiannis: Multimodal Graph Generative AI and applications to biomedical domain\nGraph generative models are recently gaining significant interest in current application domains. They are commonly used to model social networks, knowledge graphs, molecules and proteins. In this talk we will present the potential of graph generative models and our recent relevant efforts in the biomedical domain. More specifically we present a novel architecture that generates medical records as graphs with privacy guarantees. We capitalize and modify the graph Variational autoencoders (VAEs) architecture. We train the generative model with the well known MIMIC medical database and achieve generated data that are very similar to the real ones yet provide privacy guarantees. We achieve there as well promising results with potential for future application in broader biomedical related tasks. Finally we present ongoing research directions for multi modal generative models involving graphs and applications to protein function text generation – the prot2text model."
  },
  {
    "objectID": "people/advisors/anthea.html",
    "href": "people/advisors/anthea.html",
    "title": "Anthea Monod",
    "section": "",
    "text": "Home\n    Speakers\n    Anthea Monod\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nAnthea Monod is a Lecturer (Assistant Professor with tenure) in Biomathematics at the Department of Mathematics at Imperial College London. Her research areas primarily include topological data analysis, algebraic statistics, and nonlinear algebra applied to mathematical and computational biology. She earned her PhD from the Swiss Federal Institute of Technology in Lausanne (EPFL). Prior to joining Imperial, she held postdoctoral and visiting faculty positions at the Technion–Israel Institute of Technology, Duke University, Columbia University in the City of New York, and Tel Aviv University."
  },
  {
    "objectID": "people/advisors/bronstein.html",
    "href": "people/advisors/bronstein.html",
    "title": "Michael Bronstein",
    "section": "",
    "text": "Home\n    Speakers\n    Michael Bronstein \n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nMichael Bronstein is the DeepMind Professor of AI at the University of Oxford. He was previously Head of Graph Learning Research at Twitter, a professor at Imperial College London and held visiting appointments at Stanford, MIT, and Harvard. He has been affiliated with three Institutes for Advanced Study (at TUM as a Rudolf Diesel Fellow (2017-2019), at Harvard as a Radcliffe fellow (2017-2018), and at Princeton as a short-time scholar (2020)). Michael received his PhD from the Technion in 2007. He is the recipient of the EPSRC Turing AI World Leading Research Fellowship, Royal Society Wolfson Research Merit Award, Royal Academy of Engineering Silver Medal, five ERC grants, two Google Faculty Research Awards, and two Amazon AWS ML Research Awards. He is a Member of the Academia Europaea, Fellow of IEEE, IAPR, BCS, and ELLIS, ACM Distinguished Speaker, and World Economic Forum Young Scientist. In addition to his academic career, Michael is a serial entrepreneur and founder of multiple startup companies, including Novafora, Invision (acquired by Intel in 2012), Videocites, and Fabula AI (acquired by Twitter in 2019)."
  },
  {
    "objectID": "people/team/pragya.html",
    "href": "people/team/pragya.html",
    "title": "Pragya Singh",
    "section": "",
    "text": "Pragya recently finished her masters in Artificial Intelligence at Imperial College London. Prior to that, she finished her undergraduate in Engineering Physics from IIT Roorkee and worked as a quantitative researcher at Goldman Sachs. Her research interests include geometric and topological machine learning, especially their applications to natural sciences."
  },
  {
    "objectID": "people/team/zhengang.html",
    "href": "people/team/zhengang.html",
    "title": "Zhengang Zhong",
    "section": "",
    "text": "Zhengang Zhong is a postdoctoral researcher at the University of Warwick. Prior to joining Warwick, he completed his PhD at Imperial College London and received the Diplom degree from the Technical University of Dresden. His research is about learning and decision-making under uncertainty, focusing on optimal control and variational problems on graphs."
  },
  {
    "objectID": "people/team/valentina.html",
    "href": "people/team/valentina.html",
    "title": "Valentina Giunchiglia",
    "section": "",
    "text": "Valentina is a second-year PhD student jointly affiliated with Imperial College London and Harvard University funded by the Medical Research Council, under the supervision of Prof. Marinka Zitnik and Prof. Adam Hampshire. Her work lies at the intersection of artificial intelligence, biology, and health, with a focus on neuroscience. She is interested in a holistic study of diseases across multiple levels — from cellular and genetic to brain structure and clinical phenotypes — to drive advancements in precision medicine. To achieve this, she works on research spanning bioinformatics, artificial intelligence, and clinical and behavioural analysis, with the aim to gain expertise that enables her to leverage diverse data types and insights for precision medicine. Specifically, she is focused on developing multimodal foundation models that integrate diverse data types to effectively capture the complexity of diseases while embedding context-aware knowledge. Her long-term research goal is to investigate diseases using multimodal data (e.g., imaging, omics, and electronic health records) to capture diverse disease characteristics and identify precise, clinically applicable treatments."
  },
  {
    "objectID": "logml2022/speakers2022/other/petar-velickovic.html",
    "href": "logml2022/speakers2022/other/petar-velickovic.html",
    "title": "Petar Veličković",
    "section": "",
    "text": "Home\n    Speakers\n    Petar Veličković\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nPetar Veličković is a Senior Research Scientist at DeepMind. He holds a PhD in Computer Science from the University of Cambridge (Trinity College), obtained under the supervision of Pietro Liò. His research interests involve devising neural network architectures that operate on nontrivially structured data (such as graphs), and their applications in algorithmic reasoning and computational biology. He has published relevant research in these areas at both machine learning venues (NeurIPS, ICLR, ICML-W) and biomedical venues and journals (Bioinformatics, PLOS One, JCB, PervasiveHealth). In particular, he is the first author of Graph Attention Networks—a popular convolutional layer for graphs—and Deep Graph Infomax—a scalable local/global unsupervised learning pipeline for graphs (featured in ZDNet). Further, his research has been used in substantially improving the travel-time predictions in Google Maps (covered by outlets including the CNBC, Endgadget, VentureBeat, CNET, the Verge and ZDNet)."
  },
  {
    "objectID": "logml2022/speakers2022/other/taco-cohen.html",
    "href": "logml2022/speakers2022/other/taco-cohen.html",
    "title": "Taco Cohen",
    "section": "",
    "text": "Home\n    Speakers\n    Taco Cohen\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nTaco Cohen is a machine learning research scientist at Qualcomm AI Research in Amsterdam. He obtained his PhD at the University of Amsterdam, supervised by prof. Max Welling. He was a co-founder of Scyfer, a company focussed on active deep learning, acquired by Qualcomm in 2017. He holds a BSc in theoretical computer science from Utrecht University and a MSc in artificial intelligence from the University of Amsterdam (both cum laude). His research is focussed on understanding and improving deep representation learning, in particular learning of equivariant and disentangled representations, data-efficient deep learning, learning on non-Euclidean domains, and applications of group representation theory and non-commutative harmonic analysis, as well as deep learning based source compression. He has done internships at Google Deepmind (working with Geoff Hinton) and OpenAI. He received the 2014 University of Amsterdam thesis prize, a Google PhD Fellowship, ICLR 2018 best paper award for “Spherical CNNs”, and was named one of 35 innovators under 35 in Europe by MIT in 2018."
  },
  {
    "objectID": "logml2022/speakers2022/other/yang-hui-he.html",
    "href": "logml2022/speakers2022/other/yang-hui-he.html",
    "title": "Yang-Hui He",
    "section": "",
    "text": "Home\n    Speakers\n    Yang-Hui He\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nYang-Hui He is a Fellow at the London Institute for Mathematical Sciences,, Professor of mathematics at City, University of London, Lecturer at Merton College, University of Oxford and Chang-Jiang Chair of physics at NanKai University. He studied at Princeton, Cambridge and MIT and works at the interface between string theory, algebraic geometry and machine learning."
  },
  {
    "objectID": "logml2022/speakers2022/keynote/heather-harrington.html",
    "href": "logml2022/speakers2022/keynote/heather-harrington.html",
    "title": "Heather Harrington",
    "section": "",
    "text": "Home\n    Speakers\n    Heather Harrington\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nHeather Harrington obtained her PhD in 2010 from the Department of Mathematics at Imperial College London. She joined the Mathematical Institute at the University of Oxford in 2013 as a Hooke Research Fellow and EPSRC Postdoctoral Fellow and was affiliated with St Cross College and Keble College. Heather Harrington was promoted to Professor of Mathematics in 2020. She now has affiliations with St John’s College as a Research Fellow in Mathematics and the Sciences and the Wellcome Centre for Human Genetics an Associate Group Leader. Her research focuses on the problem of reconciling models and data by extracting information about the structure of models and the shape of data. She develops methods relying on techniques from computational algebraic geometry and topology to study complex biological systems. She is the Co-Director of the Centre for Topological Data Analysis. She has been recognised for her research with a Royal Society University Research Fellowship, London Mathematical Society Whitehead Prize, University of Cambridge Adams Prize, and a Philip Leverhulme Prize in Mathematics and Statistics."
  },
  {
    "objectID": "logml2022/projects.html",
    "href": "logml2022/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Adaptive frame averaging for invariant and equivariant representations\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nProf Bruno Ribeiro\n\n\n\n\n\n\n\n\n\n\n\n\nCharacterizing generalization and adversarial robustness for set networks\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nProf Tolga Birdal\n\n\n\n\n\n\n\n\n\n\n\n\nContrastive learning\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nDr Melanie Weber\n\n\n\n\n\n\n\n\n\n\n\n\nDImplicit neural filters for steerable CNNs\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nGabriele Cesa\n\n\n\n\n\n\n\n\n\n\n\n\nData reductions for graph attention variants\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nKaustubh Dholé\n\n\n\n\n\n\n\n\n\n\n\n\nDeep functional map\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nDr Abhishek Sharma\n\n\n\n\n\n\n\n\n\n\n\n\nDifferential geometry for representation learning\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nProf Georgios Arvanitidis\n\n\n\n\n\n\n\n\n\n\n\n\nDistilling large GNNs for molecules\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nJohannes Gasteiger\n\n\n\n\n\n\n\n\n\n\n\n\nEquivariant machine learning for vegetation dynamics\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nProf Soledad Villar\n\n\n\n\n\n\n\n\n\n\n\n\nExploiting domain structure for music ML tasks\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nDr Cătălina Cangea\n\n\n\n\n\n\n\n\n\n\n\n\nExploring network medicine principles encoded by graph neural networks\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nKexin Huang\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized Laplacian positional encoding for graph learning\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nDr Haggai Maron\n\n\n\n\n\n\n\n\n\n\n\n\nGeometric tools for investigating loss landscapes of deep neural networks\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nDr James Lucas\n\n\n\n\n\n\n\n\n\n\n\n\nGraph-rewiring for GNNs from a geometric perspective\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nDr Francesco di Giovanni\n\n\n\n\n\n\n\n\n\n\n\n\nHelmhotlz-Hodge Laplacians: edge flows and simplicial learning\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nProf Stefan Schonsheck\n\n\n\n\n\n\n\n\n\n\n\n\nLatent graph learning for multivariate time series\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nDr Xiang Zhang\n\n\n\n\n\n\n\n\n\n\n\n\nLearning graph rewiring using RL\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nDr Eli Meirom\n\n\n\n\n\n\n\n\n\n\n\n\nLearning non-geodesic submanifolds\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nProf Nina Miolane\n\n\n\n\n\n\n\n\n\n\n\n\nLine bundle cohomology formulae on Calabi-Yau threefolds\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nDr Andrei Constantin\n\n\n\n\n\n\n\n\n\n\n\n\nMachine learning the fine interior\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nProf Alexander Kasprzyk\n\n\n\n\n\n\n\n\n\n\n\n\nPDE-inspired sheaf neural networks\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nCristian Bodnar\n\n\n\n\n\n\n\n\n\n\n\n\nTowards training GNNs using explanation feedbacks\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nDr Chirag Agarwal\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Archives",
      "LOGML 2022",
      "Projects"
    ]
  },
  {
    "objectID": "logml2022/projects2022/project22/index.html",
    "href": "logml2022/projects2022/project22/index.html",
    "title": "Latent graph learning for multivariate time series",
    "section": "",
    "text": "Dr. Xiang Zhang is a postdoctoral fellow at Harvard University. He received his Ph.D. degree in Computer Science at the University of New South Wales. His research interests lie in data mining, deep learning, and graph representation learning with applications in pervasive healthcare, Brain-Computer Interfaces (BCIs), and biomedical sciences. Xiang’s research outcomes have been published in prestigious conferences (such as ICLR, NeurIPS, KDD, and UbiComp) and journals (like Nature Computational Science). Moreover, he has published a book, which is the only off-the-shelf one of its kind, in deep learning for BCI."
  },
  {
    "objectID": "logml2022/projects2022/project22/index.html#dr-xiang-zhang",
    "href": "logml2022/projects2022/project22/index.html#dr-xiang-zhang",
    "title": "Latent graph learning for multivariate time series",
    "section": "",
    "text": "Dr. Xiang Zhang is a postdoctoral fellow at Harvard University. He received his Ph.D. degree in Computer Science at the University of New South Wales. His research interests lie in data mining, deep learning, and graph representation learning with applications in pervasive healthcare, Brain-Computer Interfaces (BCIs), and biomedical sciences. Xiang’s research outcomes have been published in prestigious conferences (such as ICLR, NeurIPS, KDD, and UbiComp) and journals (like Nature Computational Science). Moreover, he has published a book, which is the only off-the-shelf one of its kind, in deep learning for BCI."
  },
  {
    "objectID": "logml2022/projects2022/project22/index.html#project",
    "href": "logml2022/projects2022/project22/index.html#project",
    "title": "Latent graph learning for multivariate time series",
    "section": "Project",
    "text": "Project\nMultivariate time series are prevalent in a variety of domains, including healthcare, transportation, space science, biology, and finance. Recently, it has attracted increasing attention to leverage the structural relationships among channels to learn stronger representation. Previous studies demonstrated that inter-sensor correlations bring rich information in modelling time series. In this project, we will learn how to use a graph neural network model to learn the latent relationship between different time series sensors (such as different leads in ECG signals). The learned graph patterns will correspond to the task. The model will be evaluated by the PhysioNet benchmark."
  },
  {
    "objectID": "logml2022/projects2022/project14/index.html",
    "href": "logml2022/projects2022/project14/index.html",
    "title": "Helmhotlz-Hodge Laplacians: edge flows and simplicial learning",
    "section": "",
    "text": "In the summer of 2020, Stefan joined UC Davis and the TETRAPODS Institute of Data Science, working under Prof. Naoki Saito. and the UC Davis math department as a Krener Assistant Professor. He teaches classes in applied math, specifically: numerical analysis, differential equations, optimization, and signal processing. Previously, he obtained a Ph.D. in the Applied Mathematics Department at Rensselaer Polytechnic Institute, working under Prof. Rongjie Lai. At RPI, he was secretary of our SIAM chapter, spent a semester at UCLA attending the IPAM long program “Geometry and Learning from Data in 3D and Beyond” and worked with researchers at IBM through the Rensselaer-IBM Artificial Intelligence Research Collaboration. He completed my thesis, “Computational Analysis of Deformable Manifolds: from Geometric Modeling to Deep Learning” in the spring of 2020, and won the Bill and Nancy Siegmann Applied Mathematical Modeling Prize. He studies computational differential geometry focusing on PDEs and optimization problems involving manifolds. Specifically, he is interested in applications in shape processing, signal analysis, and machine learning in non-Euclidean domains. He also has strong interests in harmonic analysis and generative processes for both Euclidean and non-Euclidean data."
  },
  {
    "objectID": "logml2022/projects2022/project14/index.html#prof-stefan-schonsheck",
    "href": "logml2022/projects2022/project14/index.html#prof-stefan-schonsheck",
    "title": "Helmhotlz-Hodge Laplacians: edge flows and simplicial learning",
    "section": "",
    "text": "In the summer of 2020, Stefan joined UC Davis and the TETRAPODS Institute of Data Science, working under Prof. Naoki Saito. and the UC Davis math department as a Krener Assistant Professor. He teaches classes in applied math, specifically: numerical analysis, differential equations, optimization, and signal processing. Previously, he obtained a Ph.D. in the Applied Mathematics Department at Rensselaer Polytechnic Institute, working under Prof. Rongjie Lai. At RPI, he was secretary of our SIAM chapter, spent a semester at UCLA attending the IPAM long program “Geometry and Learning from Data in 3D and Beyond” and worked with researchers at IBM through the Rensselaer-IBM Artificial Intelligence Research Collaboration. He completed my thesis, “Computational Analysis of Deformable Manifolds: from Geometric Modeling to Deep Learning” in the spring of 2020, and won the Bill and Nancy Siegmann Applied Mathematical Modeling Prize. He studies computational differential geometry focusing on PDEs and optimization problems involving manifolds. Specifically, he is interested in applications in shape processing, signal analysis, and machine learning in non-Euclidean domains. He also has strong interests in harmonic analysis and generative processes for both Euclidean and non-Euclidean data."
  },
  {
    "objectID": "logml2022/projects2022/project14/index.html#project",
    "href": "logml2022/projects2022/project14/index.html#project",
    "title": "Helmhotlz-Hodge Laplacians: edge flows and simplicial learning",
    "section": "Project",
    "text": "Project\nThe Laplacian operator is a ubiquitous tool in signal processing. Generalizations of this operator to manifolds (i.e., the Laplace-Beltrami operator) and graphs (i.e., the graph Laplacian) have proved to be very useful for learning on non-euclidean domains. Helmholtz-Hodge theory can further generalize these operators to higher-order differential forms. For example, the continuous 1-Laplacian can be used to analyze vector-fields on manifolds [1] and edge flows on directed or undirected graphs can be analyzed via wavelet-like bases generated by the Hodge-Laplacian [3]. Similarly, the dth-Laplacian can be used to define convolutional operations on d-degree complexes [2]. This project will use these operators to develop methods for solving signal processing problems such as denoising and in-painting on simplicial domains focussing on edge-based and face-based signals. We do so using both traditional variational methods and neural network-based approaches.\nReferences [1] Y.-C. Chen, M. Meila and I. G. Kevrekidis, Helmholtzian eigenmap: Topological feature discovery and edge flow learning from point cloud data, arXiv preprint arXiv:2103.07626, (2021). [2] S. Ebli, M. Defferrard, and G. Spreemann, Simplicial neural networks, arXiv preprint arXiv:2010.03633, (2020). [3] T. M. Roddenberry, F. Frantzen, M. T. Schaub, and S. Segarra, Hodgelets: Localized spectral representations of flows on simplicial complexes, arXiv preprint arXiv:2109.08728, (2021)."
  },
  {
    "objectID": "logml2022/projects2022/project12/index.html",
    "href": "logml2022/projects2022/project12/index.html",
    "title": "DImplicit neural filters for steerable CNNs",
    "section": "",
    "text": "Gabriele Cesa is a Research Associate at Qualcomm AI Research, Amsterdam and a PhD student at University of Amsterdam, under the supervision of Max Welling. Gabriele’s research focuses on augmenting machine learning methods with prior information about the geometry of a problem to achieve improved data efficiency and generalization. A particular emphasis has been given to equivariant neural networks, which can encode our knowledge about the symmetries in the data into the model’s architecture. Previously, Gabriele received a Master degree in Artificial Intelligence at the University of Amsterdam, Netherlands and a Bachelor degree in computer science at the University of Trento, Italy."
  },
  {
    "objectID": "logml2022/projects2022/project12/index.html#gabriele-cesa",
    "href": "logml2022/projects2022/project12/index.html#gabriele-cesa",
    "title": "DImplicit neural filters for steerable CNNs",
    "section": "",
    "text": "Gabriele Cesa is a Research Associate at Qualcomm AI Research, Amsterdam and a PhD student at University of Amsterdam, under the supervision of Max Welling. Gabriele’s research focuses on augmenting machine learning methods with prior information about the geometry of a problem to achieve improved data efficiency and generalization. A particular emphasis has been given to equivariant neural networks, which can encode our knowledge about the symmetries in the data into the model’s architecture. Previously, Gabriele received a Master degree in Artificial Intelligence at the University of Amsterdam, Netherlands and a Bachelor degree in computer science at the University of Trento, Italy."
  },
  {
    "objectID": "logml2022/projects2022/project12/index.html#project",
    "href": "logml2022/projects2022/project12/index.html#project",
    "title": "DImplicit neural filters for steerable CNNs",
    "section": "Project",
    "text": "Project\nSteerable CNNs [1] and, in particular, Euclidean Steerable CNNs [2, 3] provide a general framework for building neural networks which are equivariant to groups beyond translations, for example reflections and rotations. Euclidean Steerable CNNs rely on standard convolution with steerable filters, i.e. filters satisfying a steerability constraint [2, 3, 4]. Filters are typically parameterized by linear combination of a steerable filter basis [4], obtained by analytically solving the steerability constraint. However, we note that if the filter is parameterized by an implicit MLP [5] (e.g. for point cloud data), the steerability constraint just requires the MLP to be equivariant. This suggests a simpler way to build steerable CNNs, which does not rely on the polar decomposition of the filters and spherical harmonics. Moreover, this strategy enjoys the flexibility of the implicit neural function, supporting wider filters with limited parameter cost. Since no steerable basis is required, this approach can also be easily extended to different equivariance groups with minor changes, provided an equivariant MLP can be built. We will explore applications for point cloud data (with full rotational symmetries or with only azimuthal symmetries) and molecular data.\nReferences [1] A General Theory of Equivariant CNNs on Homogeneous Spaces, Taco Cohen and Mario Geiger and Maurice Weiler [2] 3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric Data, Maurice Weiler and Mario Geiger and Max Welling and Wouter Boomsma and Taco Cohen [3] Tensor field networks: Rotation- and translation-equivariant neural networks for 3D point clouds, Nathaniel Thomas and Tess Smidt and Steven Kearnes and Lusann Yang and Li Li and Kai Kohlhoff and Patrick Riley [4] A Program to Build E(N)-Equivariant Steerable CNNs, Gabriele Cesa and Leon Lang and Maurice Weiler [5] PointConv: Deep Convolutional Networks on 3D Point Clouds, Wenxuan Wu and Zhongang Qi and Li Fuxin"
  },
  {
    "objectID": "logml2022/projects2022/project9/index.html",
    "href": "logml2022/projects2022/project9/index.html",
    "title": "Line bundle cohomology formulae on Calabi-Yau threefolds",
    "section": "",
    "text": "Andrei is a Stephen Hawking Fellow in Theoretical Physics. He studied Physics in Bremen and received an MSc in Theoretical and Mathematical Physics from LMU Munich. He did his doctoral studies in Oxford, followed by two postdocs, one in Oxford and one in Uppsala. His research lies at the interface of string theory, algebraic geometry, and machine learning and focuses on developing mathematical and computational tools to investigate string theory and its implications for particle physics, cosmology and quantum gravity. One of the central drives of his research is to construct models of particle physics from string theory. His current work is related to the study of vector bundle cohomology and its applications to string model building. Computing cohomology is a crucial and time consuming step in the derivation of the spectrum of low-energy particles resulting from string compactifications. He has shown that in many cases of interest in string theory topological formulae for cohomology exist. These mathematical shortcuts can reduce the time needed for deciding the physical viability of a string compactification from several months of computer algebra to a split of a second. He is also working on adapting, refining and applying machine learning techniques to problems in string theory and mathematics. He is using these to generate new conjectures about Calabi-Yau manifolds, vector bundles and cohomology, as well as to probe the landscape of string theory solutions relevant for particle physics."
  },
  {
    "objectID": "logml2022/projects2022/project9/index.html#dr-andrei-constantin",
    "href": "logml2022/projects2022/project9/index.html#dr-andrei-constantin",
    "title": "Line bundle cohomology formulae on Calabi-Yau threefolds",
    "section": "",
    "text": "Andrei is a Stephen Hawking Fellow in Theoretical Physics. He studied Physics in Bremen and received an MSc in Theoretical and Mathematical Physics from LMU Munich. He did his doctoral studies in Oxford, followed by two postdocs, one in Oxford and one in Uppsala. His research lies at the interface of string theory, algebraic geometry, and machine learning and focuses on developing mathematical and computational tools to investigate string theory and its implications for particle physics, cosmology and quantum gravity. One of the central drives of his research is to construct models of particle physics from string theory. His current work is related to the study of vector bundle cohomology and its applications to string model building. Computing cohomology is a crucial and time consuming step in the derivation of the spectrum of low-energy particles resulting from string compactifications. He has shown that in many cases of interest in string theory topological formulae for cohomology exist. These mathematical shortcuts can reduce the time needed for deciding the physical viability of a string compactification from several months of computer algebra to a split of a second. He is also working on adapting, refining and applying machine learning techniques to problems in string theory and mathematics. He is using these to generate new conjectures about Calabi-Yau manifolds, vector bundles and cohomology, as well as to probe the landscape of string theory solutions relevant for particle physics."
  },
  {
    "objectID": "logml2022/projects2022/project9/index.html#project",
    "href": "logml2022/projects2022/project9/index.html#project",
    "title": "Line bundle cohomology formulae on Calabi-Yau threefolds",
    "section": "Project",
    "text": "Project\nCohomology is a universal tool in mathematics: from topology and geometry to algebra and number theory, cohomology is used to quantify the possible ways in which a problem fails to meet the naive expectations of the problem solver. As such, estimates of cohomology represent a vital key to problem solving. In areas of theoretical physics such as string theory, cohomology is linked to the low-energy quantum excitations of fields and strings that can be experimentally observed.\nIn practice, cohomology computations are notoriously difficult to carry out in general. However, it has been recently shown that line bundle cohomology dimensions on several classes of two-dimensional and three-dimensional complex manifolds, including compact toric surfaces, generalised del Pezzo surfaces, K3 surfaces and Calabi-Yau threefolds, are described by closed-form formulae. This new technique for the computation of cohomology uses a combination of algebro-geometric methods and exploratory machine learning methods. The goal of the project will be to go one step further and discover exact generating functions for line bundle cohomology on Calabi-Yau threefolds.\nReferences: [1] C. Brodie, A. Constantin, J. Gray, A. Lukas, F. Ruehle, Recent Developments in Line Bundle Cohomology and Applications to String Phenomenology, Nankai Symposium on Mathematical Dialogues 2021 Conference Proceedings, arXiv: 2112.12107. [2] C. Brodie, A. Constantin, R. Deen, A. Lukas, Machine Learning Line Bundle Cohomology, Fortsch.Phys. 68 (2020) 1, 1900087, arXiv: 1906.08730. [3] C. Brodie, A. Constantin, A. Lukas, Flops, Gromov-Witten invariants and symmetries of line bundle cohomology on Calabi-Yau three-folds, J.Geom.Phys. 171 (2022), arXiv: 2010.06597. [4] C. Brodie, A. Constantin, Cohomology Chambers on Complex Surfaces and Elliptically Fibered Calabi-Yau Three-folds, arXiv: 2009.01275."
  },
  {
    "objectID": "logml2022/projects2022/project1/index.html",
    "href": "logml2022/projects2022/project1/index.html",
    "title": "Data reductions for graph attention variants",
    "section": "",
    "text": "Kaustubh is a researcher at Emory University working in Natural Language Processing & Graph Neural Networks. He has a varied set of interests spanning efficient deep learning architectures, graph neural networks and dialog systems. He has been co-organizer of the GEM (Generation, Evaluation, Metrics) workshops for the past 2 years and the co-creator of the wisdom-of-researchers effort of NL-Augmenter last year. For 7 years, he led the R&D team of Amelia (IPsoft) designing the natural language capabilities for their conversation agent Amelia."
  },
  {
    "objectID": "logml2022/projects2022/project1/index.html#kaustubh-dholé",
    "href": "logml2022/projects2022/project1/index.html#kaustubh-dholé",
    "title": "Data reductions for graph attention variants",
    "section": "",
    "text": "Kaustubh is a researcher at Emory University working in Natural Language Processing & Graph Neural Networks. He has a varied set of interests spanning efficient deep learning architectures, graph neural networks and dialog systems. He has been co-organizer of the GEM (Generation, Evaluation, Metrics) workshops for the past 2 years and the co-creator of the wisdom-of-researchers effort of NL-Augmenter last year. For 7 years, he led the R&D team of Amelia (IPsoft) designing the natural language capabilities for their conversation agent Amelia."
  },
  {
    "objectID": "logml2022/projects2022/project1/index.html#project",
    "href": "logml2022/projects2022/project1/index.html#project",
    "title": "Data reductions for graph attention variants",
    "section": "Project",
    "text": "Project\nThere are a lot of data reduction techniques that have been applied over general transformers like Linformer (JL-Lemma), Reformer (using LSH), Nymstromformer (using Nymstrom method), etc. However, many of these approaches which have sped up attention computation have not been explored for speeding up graph attention variants. I am vouching for performing a baseline set of experiments to test some of these data reduction approaches to approximate GAT/GATv2’s attention and measure how much drop on some downstream task is seen."
  },
  {
    "objectID": "logml2022/projects2022/project19/index.html",
    "href": "logml2022/projects2022/project19/index.html",
    "title": "Adaptive frame averaging for invariant and equivariant representations",
    "section": "",
    "text": "Bruno Ribeiro is an Assistant Professor in the Department of Computer Science at Purdue University. He obtained his Ph.D. at the University of Massachusetts Amherst and was a postdoctoral fellow at Carnegie Mellon University from 2013-2015. His research interests involve devising learning algorithms and methods that operate in nontrivially structured data (such as graphs) and in the intersection between causality and machine learning. He received an NSF CAREER award in 2020 and has several best paper awards."
  },
  {
    "objectID": "logml2022/projects2022/project19/index.html#prof-bruno-ribeiro",
    "href": "logml2022/projects2022/project19/index.html#prof-bruno-ribeiro",
    "title": "Adaptive frame averaging for invariant and equivariant representations",
    "section": "",
    "text": "Bruno Ribeiro is an Assistant Professor in the Department of Computer Science at Purdue University. He obtained his Ph.D. at the University of Massachusetts Amherst and was a postdoctoral fellow at Carnegie Mellon University from 2013-2015. His research interests involve devising learning algorithms and methods that operate in nontrivially structured data (such as graphs) and in the intersection between causality and machine learning. He received an NSF CAREER award in 2020 and has several best paper awards."
  },
  {
    "objectID": "logml2022/projects2022/project19/index.html#project",
    "href": "logml2022/projects2022/project19/index.html#project",
    "title": "Adaptive frame averaging for invariant and equivariant representations",
    "section": "Project",
    "text": "Project\nMany machine learning tasks require learning functions that are invariant or equivariant to known symmetries of the input data. Unfortunately, there is a significant challenge in the design of neural network architectures that simultaneously (a) take into account the symmetries, (b) are expressive enough to have small generalization error, and (c) are computationally efficient. Murphy et al. [1, 2] have shown that it is possible to sacrifice (c) –computational efficiency– for (a) and (b) with the use of symmetrization (Reynolds operator). Recently, Puny et al. [3] have proposed a method (Frame Averaging) to improve the efficiency of symmetrization. However, in some tasks, Frame Averaging can lead to large generalization errors. This is because of a fundamental trade-off between computationally efficient and generalization error in symmetrization. This project will study this trade-off and propose efficient solutions to achieve both computational efficiency and better generalization error in the affected tasks.\nReferences [1] Murphy, Ryan L., Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro. Janossy pooling: Learning deep permutation-invariant functions for variable-size inputs. ICLR 2019. [2] Murphy, Ryan, Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro. Relational pooling for graph representations. In International Conference on Machine Learning, pp. 4663-4673. PMLR, 2019. [3] Puny, Omri, Matan Atzmon, Heli Ben-Hamu, Edward J. Smith, Ishan Misra, Aditya Grover, and Yaron Lipman. Frame Averaging for Invariant and Equivariant Network Design. ICLR 2022."
  },
  {
    "objectID": "logml2022/projects2022/project17/index.html",
    "href": "logml2022/projects2022/project17/index.html",
    "title": "Geometric tools for investigating loss landscapes of deep neural networks",
    "section": "",
    "text": "James Lucas is a Research Scientist at NVIDIA within the Toronto AI lab. He is also a PhD candidate at the University of Toronto, where he is supervised by Richard Zemel and Roger Grosse. James has a broad set of research interests within deep learning and ML more broadly. In the past, his research has focused on understanding and improving the optimization of deep neural networks. In particular, he has worked towards understanding the loss landscape geometry of these models and how we may leverage their properties in practice. He is also excited by deep generative models for 3D geometry and improving the data-efficiency of deep learning systems."
  },
  {
    "objectID": "logml2022/projects2022/project17/index.html#dr-james-lucas",
    "href": "logml2022/projects2022/project17/index.html#dr-james-lucas",
    "title": "Geometric tools for investigating loss landscapes of deep neural networks",
    "section": "",
    "text": "James Lucas is a Research Scientist at NVIDIA within the Toronto AI lab. He is also a PhD candidate at the University of Toronto, where he is supervised by Richard Zemel and Roger Grosse. James has a broad set of research interests within deep learning and ML more broadly. In the past, his research has focused on understanding and improving the optimization of deep neural networks. In particular, he has worked towards understanding the loss landscape geometry of these models and how we may leverage their properties in practice. He is also excited by deep generative models for 3D geometry and improving the data-efficiency of deep learning systems."
  },
  {
    "objectID": "logml2022/projects2022/project17/index.html#project",
    "href": "logml2022/projects2022/project17/index.html#project",
    "title": "Geometric tools for investigating loss landscapes of deep neural networks",
    "section": "Project",
    "text": "Project\nNeural networks have achieved extraordinary success, but this achievement is limited to only those networks that are trainable with stochastic gradient descent (SGD). What is special about these networks? At the very least, SGD can effectively traverse their loss landscapes. In this project, we will train neural networks and investigate their loss landscapes using tools from geometry.\nShocking phenomena have been discovered in relation to loss landscape geometry. For example, the existence of sparse subnetworks within larger networks that can be trained in isolation to achieve comparable performance [1]. Or linear paths within loss landscapes that exhibit monotonically decreasing loss [2, 3] (or that connect together several minima [4, 5]). Explaining these phenomena is challenging because loss landscapes in deep learning are extremely high dimensional and come with minimal guarantees on their geometric structure. However, in practice, we observe a surprisingly rich structure in the loss landscapes of neural networks.\nWe will review existing work that has successfully utilized geometric tools to better understand deep neural networks (for example, [3,6]). While this project will have its roots in deep learning theory, there will be a significant element of empirical analysis as we implement and evaluate standard deep learning architectures. Some general questions that we may hope to address include: how does the geometry around initialization differ from that at local minima? How does the geometry at initialization shape optimization? Can we identify global structure in the loss landscapes of neural networks?\nReferences [1] The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks, Frankle & Carbin. ICLR 2019 [2] Qualitatively characterizing neural network optimization problems, Goodfellow et al. ICLR 2015 [3] Analyzing Monotonic Linear Interpolation in Neural Network Loss Landscapes, Lucas et al. ICML 2021 [4] Topology and geometry of half-rectified network optimization, Freeman & Bruna. ICLR 2017 [5] Linear mode connectivity and the lottery ticket hypothesis, Frankle et al. ICML 2020 [6] Exponential expressivity in deep neural networks through transient chaos, Poole et al. NeurIPS 2016"
  },
  {
    "objectID": "logml2022/projects2022/project11/index.html",
    "href": "logml2022/projects2022/project11/index.html",
    "title": "Exploiting domain structure for music ML tasks",
    "section": "",
    "text": "Cătălina is a Research Scientist at DeepMind, having previously obtained her PhD at the University of Cambridge, supervised by Pietro Liò. Her main interests are learning in challenging data scenarios and using these methods in real-world applications; besides that, she deeply cares about music and is excited about the prospect of developing ML tools that support people who wish to be (more) creative! In the past, she has done research on cross-modal architectures, V&LN and VQA, scene graphs, hierarchical & topological relational representations and Neural Process-based models."
  },
  {
    "objectID": "logml2022/projects2022/project11/index.html#dr-cătălina-cangea",
    "href": "logml2022/projects2022/project11/index.html#dr-cătălina-cangea",
    "title": "Exploiting domain structure for music ML tasks",
    "section": "",
    "text": "Cătălina is a Research Scientist at DeepMind, having previously obtained her PhD at the University of Cambridge, supervised by Pietro Liò. Her main interests are learning in challenging data scenarios and using these methods in real-world applications; besides that, she deeply cares about music and is excited about the prospect of developing ML tools that support people who wish to be (more) creative! In the past, she has done research on cross-modal architectures, V&LN and VQA, scene graphs, hierarchical & topological relational representations and Neural Process-based models."
  },
  {
    "objectID": "logml2022/projects2022/project11/index.html#project",
    "href": "logml2022/projects2022/project11/index.html#project",
    "title": "Exploiting domain structure for music ML tasks",
    "section": "Project",
    "text": "Project\nLearning to represent and generate music is a highly relevant task for the machine learning field. This data domain is ideal for density estimation tasks and exhibits many interesting properties, such as long-term dependencies and patterns residing at various scales. More crucially, music generation is a main pillar of creative AI, which complements the ML community efforts in pushing scientific progress and gives us the amazing opportunity to assist artists in their creative process [0]!\nExisting state-of-the-art approaches for symbolic music generation [1, 2] operate on and produce sequences of tokens. However, the music domain contains structure at several scales: local (e.g. chords, arpeggios, multiple voices being played together in a fugue) and long-term/global (e.g. ABA form [3], the key that a piece is written in, repeating patterns/motifs). In that sense, there have been relatively few studies (e.g. [4, 5, 6, 7, 8]) that investigate explicit representations of structure for modelling and generating music. Each of these works has certain limitations—for instance, each piece modelled in [4] (see Fig. 2) is turned into a single graph by connecting all notes with temporal links, but does not capture the fundamental music-theoretical dependencies and insights that composers most likely used when writing the piece. Alternatively, the work in [7] and a few others look at rhythm-specific encodings only. Perhaps the closest modelling strategy to the one intended for this project is [8], where the authors choose to encode various musical relationships between bars of the score; however, it would be easier to work with more general graph representations of music, building up from first principles such as the circle of fifths / the tonality of a piece (e.g. use a sliding window and compute relationships / a graph within each local window and between windows at discrete points in the score - there are endless ways to think about this!)\nThis project aims to study the effects of leveraging music-theoretical graph representations in ML models. The goal is to encode symbolic music sequences in a principled manner that reflects the composition process and underlying structure up to a greater extent that previously. This encoding would then be passed as (additional) input to a model [1, 2, 9]. Finally, the effects on model performance would be studied in classification and/or generation scenarios (TBD based on time constraints).\nSteps: 1. Download dataset(s), choose task(s) 2. Become familiar with basic music theory concepts and design 1 or more ways of encoding the structure in symbolic music (see Appendix A.2 [1] for a description of event-based MIDI representations; see Chordify in Resources; see Wiki) 3. Set up model codebase and obtain baseline performance on chosen tasks 4. Add relational structure to the model and find suitable ways to encode it 5. Interpret the new results and investigate changes in model processing (e.g. visualising attention in layers, emerging patterns) 6. Open-source code, to allow researchers to preprocess their own music data and build more graph-based ML models for music tasks!\nResources - Music Transformer GitHub codebase - MusicVAE GitHub codebase - Perceiver AR codebase (to be released soon) - MAESTRO dataset - MetaMIDIDataset - Lakh MIDI dataset - MusPy - A toolkit for symbolic music generation - Chordify - music21 Documentation - Symbolic Representations - Fundamentals of music - A structural way to encode music (https://en.wikipedia.org/wiki/Tonnetz) studied by https://ojs.aaai.org/index.php/AAAI/article/view/11880 (here, the resulting encoding was an image)\nReferences [0] Music 2020 Archives - AI Art Gallery (http://www.aiartonline.com/category/music-2020/) [1] Music Transformer [2] General-purpose, long-context autoregressive modeling with Perceiver AR [3] Ternary form [4] Graph Neural Network for Music Score Data and Modeling Expressive Piano Performance [5] StructureNet: Inducing Structure in Generated Melodies - IR Anthology [6] Neurosymbolic Deep Generative Models for Sequence Data with Relational Constraints [7] Pop Music Transformer: Beat-based Modeling and Generation of Expressive Pop Piano Compositions [8] MELONS: generating melody with long-term structure using transformers and structure graph [9] A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music"
  },
  {
    "objectID": "logml2022/projects2022/project20/index.html",
    "href": "logml2022/projects2022/project20/index.html",
    "title": "Generalized Laplacian positional encoding for graph learning",
    "section": "",
    "text": "Haggai is a Research Scientist at NVIDIA Research and a member of NVIDIA’s TLV lab. His main fields of interest are machine learning, optimization, and shape analysis. In particular, he is working on applying deep learning to irregular domains (e.g., sets, graphs, point clouds, and surfaces), usually by leveraging their symmetry structure. He completed his Ph.D. in 2019 at the Weizmann Institute of Science under the supervision of Prof. Yaron Lipman. He will be joining the Faculty of Electrical and Computer Engineering at the Technion as an Assistant Professor in 2023."
  },
  {
    "objectID": "logml2022/projects2022/project20/index.html#dr-haggai-maron",
    "href": "logml2022/projects2022/project20/index.html#dr-haggai-maron",
    "title": "Generalized Laplacian positional encoding for graph learning",
    "section": "",
    "text": "Haggai is a Research Scientist at NVIDIA Research and a member of NVIDIA’s TLV lab. His main fields of interest are machine learning, optimization, and shape analysis. In particular, he is working on applying deep learning to irregular domains (e.g., sets, graphs, point clouds, and surfaces), usually by leveraging their symmetry structure. He completed his Ph.D. in 2019 at the Weizmann Institute of Science under the supervision of Prof. Yaron Lipman. He will be joining the Faculty of Electrical and Computer Engineering at the Technion as an Assistant Professor in 2023."
  },
  {
    "objectID": "logml2022/projects2022/project20/index.html#project",
    "href": "logml2022/projects2022/project20/index.html#project",
    "title": "Generalized Laplacian positional encoding for graph learning",
    "section": "Project",
    "text": "Project\nLaplacian positional encoding (LPE), i.e. using the graph laplacian eigenvectors as input node features to GNNs [1,2,3], has emerged as a successful and popular positional encoding scheme in the past few years. Nevertheless, the space of positional encodings on graphs is largely unexplored. Following the observation that laplacian eigenvectors are a solution to a specific problem: minimizing the L2 distance between node embeddings, weighted by their affinity score (see [1], section 3.1), our intention is to generalize LPEs to a family of positional encodings defined by the minimisers of the same function with general Lp norms (for p!=2). The PE formulation presented above raises several interesting challenges and questions, which we aim to explore in this project. How to calculate these positional encodings? While the L2 based functional can be easily minimized by solving a generalized eigenvalue problem, it is not clear how to approach the minimization of the generalized functionals above. Is there an efficient way to calculate/approximate these minimizers? What graph/node properties do these embeddings capture? Which graph learning tasks can benefit from them? Where do we expect to see them outperform L2 embeddings? Expressive power. What is the expressive power of GNNs that use these positional encodings? Symmetries. Is it possible to devise GNN architectures that are invariant to the natural symmetries of these embeddings? (see [4] for a solution to the L2 case) The plan is to explore these questions in the week we have and then continue the project with a focus on a specific challenge, aiming for a submission to ICLR/ICML. Candidates with some prior knowledge in optimization theory / GNN expressive power / equivariant deep learning are the most suitable for this project.\nReferences [1] Laplacian Eigenmaps for Dimensionality Reduction and Data Representation, Belkin and Niyogi, Neural Computation 2003 [2] Rethinking graph transformers with spectral attention, Kreuzer et al., NeurIPS 2021 [3] A generalization of transformer networks to graphs, Dwivedi et al., AAAI WS 2021 [4] Sign and Basis Invariant Networks for Spectral Graph Representation Learning, Lim et al., 2022"
  },
  {
    "objectID": "logml2022/projects2022/project3/index.html",
    "href": "logml2022/projects2022/project3/index.html",
    "title": "Distilling large GNNs for molecules",
    "section": "",
    "text": "Johannes Gasteiger is a PhD student at the Data Analytics and Machine Learning group at the TU Munich and has previously interned at FAIR and DeepMind. His main research interests are graph neural networks and any aspects that are relevant for their benefit to society – be it scalability, accuracy, fairness, applications, or robustness. His research has explored ways of combining graph structure with geometric spaces, with a special focus on applications in science, such as molecular systems. Before starting his PhD he studied Computer Science and Physics at the TU Munich and the University of Cambridge."
  },
  {
    "objectID": "logml2022/projects2022/project3/index.html#johannes-gasteiger",
    "href": "logml2022/projects2022/project3/index.html#johannes-gasteiger",
    "title": "Distilling large GNNs for molecules",
    "section": "",
    "text": "Johannes Gasteiger is a PhD student at the Data Analytics and Machine Learning group at the TU Munich and has previously interned at FAIR and DeepMind. His main research interests are graph neural networks and any aspects that are relevant for their benefit to society – be it scalability, accuracy, fairness, applications, or robustness. His research has explored ways of combining graph structure with geometric spaces, with a special focus on applications in science, such as molecular systems. Before starting his PhD he studied Computer Science and Physics at the TU Munich and the University of Cambridge."
  },
  {
    "objectID": "logml2022/projects2022/project3/index.html#project",
    "href": "logml2022/projects2022/project3/index.html#project",
    "title": "Distilling large GNNs for molecules",
    "section": "Project",
    "text": "Project\nPredicting the energy and forces of an atomic system is a central task for multiple fields of science, such as computational chemistry and material science. Large directional GNNs like GemNet [1] currently set the state of the art for this task on diverse datasets such as COLL and OC20. Unfortunately, these models are computationally expensive since they are centered around directed edge embeddings and their message passing is based on triplets or even quadruplets of nodes. This makes them prohibitively expensive for long simulations and large systems such as proteins. This project aims at making these models substantially faster, while retaining a similar level of accuracy. In order to achieve this, we will explore a combination of (i) cheap models that circumvent edge-based representations either partially or altogether such as PaiNN [2] and (ii) distilling large directional GNNs into cheaper regular GNNs.\nThere are three aspects that make distillation of atomic GNNs especially promising. First, the cheaper student GNN can actually have more learnable parameters than the teacher model. This is offset by using the parameters in a more efficient way that circumvents higher-order graph representations. Second, energy-and-force models allow us to generate an arbitrary amount of additional data without requiring a transfer set. Third, previous evidence suggests that models trained on small atomic systems generalize well to large systems. We thus do not need to train the expensive teacher GNN on large systems. Together, these properties suggest that an accurate and cheap atomic GNN might be within reach.\nReferences [1] Gasteiger, Becker, Günnemann. GemNet: Universal Directional Graph Neural Networks for Molecules. NeurIPS 2021 [2] Schütt, Unke, Gastegger. Equivariant message passing for the prediction of tensorial properties and molecular spectra. ICML 2021"
  },
  {
    "objectID": "logml2022/projects2022/project5/index.html",
    "href": "logml2022/projects2022/project5/index.html",
    "title": "Towards training GNNs using explanation feedbacks",
    "section": "",
    "text": "Chirag is a Research Scientist at Adobe Media and Data Science Research Lab and a visiting researcher at Harvard University. Chirag’s research interest includes developing trustworthy machine learning that goes beyond training models for specific downstream tasks and ensuring they satisfy other desirable properties, such as explainability, fairness, and robustness. He is one of the co-founders of the Trustworthy ML Initiative, a forum and seminar series related to Trustworthy ML, and an active member of the Machine Learning Collective research group that focuses on democratizing research by supporting open collaboration in machine learning (ML) research. His works have been published in top machine learning, artificial intelligence, and computer vision conferences, including ICML, AISTATS, UAI, and CVPR."
  },
  {
    "objectID": "logml2022/projects2022/project5/index.html#dr-chirag-agarwal",
    "href": "logml2022/projects2022/project5/index.html#dr-chirag-agarwal",
    "title": "Towards training GNNs using explanation feedbacks",
    "section": "",
    "text": "Chirag is a Research Scientist at Adobe Media and Data Science Research Lab and a visiting researcher at Harvard University. Chirag’s research interest includes developing trustworthy machine learning that goes beyond training models for specific downstream tasks and ensuring they satisfy other desirable properties, such as explainability, fairness, and robustness. He is one of the co-founders of the Trustworthy ML Initiative, a forum and seminar series related to Trustworthy ML, and an active member of the Machine Learning Collective research group that focuses on democratizing research by supporting open collaboration in machine learning (ML) research. His works have been published in top machine learning, artificial intelligence, and computer vision conferences, including ICML, AISTATS, UAI, and CVPR."
  },
  {
    "objectID": "logml2022/projects2022/project5/index.html#project",
    "href": "logml2022/projects2022/project5/index.html#project",
    "title": "Towards training GNNs using explanation feedbacks",
    "section": "Project",
    "text": "Project\nIntroduction. Graph Neural Networks (GNNs) are increasingly used as powerful tools for representing graph-structured data, such as social, information, chemical, and biological networks. As these models are deployed in critical applications (e.g., drug repurposing and crime forecasting), it becomes essential to ensure that the relevant stakeholders understand and trust their decisions. To this end, several approaches have been proposed in recent literature to explain the predictions of GNNs. Depending on the employed techniques, there are three broad categories: perturbation-based, gradient-based, and surrogate-based methods. While several classes of GNN explanation methods have been proposed, there is little to no work done on showing how to use these explanations to improve the GNN performance. In particular, there is no framework that leverages these explanations on the fly and aids the training of a GNN. This lack of understanding mainly stems from the fact that there is very little work on systematically analyzing the use of explanations generated by state-of-the-art GNN explanation methods\nProposal. Previous research in GraphXAI has focused on developing post-hoc explanation methods. In this work, we propose in-hoc GNN explanations that act as feedbacks, on the fly, during the training phase of a GNN model. In addition, we aim to use the generated explanations to improve the GNN training. Using explanations, we plan to define local neighborhoods for neural message passing, e.g., for a correctly classified node u, we can generate the most important nodes in its local neighborhood N_u and then use it as a prior or generate augmented samples for guiding the message-passing of similar nodes in the subsequent training stages. To this extent, we propose to have an explanation layer after every message-passing layer which acts as a unit buffer that passes all the information to the upper layers during the forward pass, but propagates the explanation information back to the graph representations."
  },
  {
    "objectID": "logml2022/speaker.html",
    "href": "logml2022/speaker.html",
    "title": "Speakers",
    "section": "",
    "text": "Keynote speakers\n\n\n\n\n\n\n\n\n\n\nHeather Harrington\n\n\nUniversity of Oxford\n\n\n\n\n\n\n\n\n\n\n\n\n\nJustin Solomon\n\n\nMIT\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nSpeakers\n\n\n\n\n\n\n\n\n\n\nAlice Le Brigant\n\n\nUniversity Paris 1 Pantheon Sorbonne\n\n\n\n\n\n\n\n\n\n\n\n\n\nPetar Veličković\n\n\nDeepMind\n\n\n\n\n\n\n\n\n\n\n\n\n\nSmita Krishnaswamy\n\n\nYale University\n\n\n\n\n\n\n\n\n\n\n\n\n\nTaco Cohen\n\n\nQualcomm\n\n\n\n\n\n\n\n\n\n\n\n\n\nYang-Hui He\n\n\nCity, University of London\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Archives",
      "LOGML 2022",
      "Speakers"
    ]
  },
  {
    "objectID": "logml2024/speakers2024/other/anthea.html",
    "href": "logml2024/speakers2024/other/anthea.html",
    "title": "Anthea Monod",
    "section": "",
    "text": "Home\n    Speakers\n    Anthea Monod\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nAnthea Monod is a Lecturer (Assistant Professor with tenure) in Biomathematics at the Department of Mathematics at Imperial College London. Her research areas primarily include topological data analysis, algebraic statistics, and nonlinear algebra applied to mathematical and computational biology. She earned her PhD from the Swiss Federal Institute of Technology in Lausanne (EPFL). Prior to joining Imperial, she held postdoctoral and visiting faculty positions at the Technion–Israel Institute of Technology, Duke University, Columbia University in the City of New York, and Tel Aviv University."
  },
  {
    "objectID": "logml2024/speakers2024/other/joan.html",
    "href": "logml2024/speakers2024/other/joan.html",
    "title": "Joan Bruna",
    "section": "",
    "text": "Home\n    Speakers\n    Joan Bruna\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nI am an Associate Professor of Computer Science, Data Science and Mathematics (affiliated) at the Courant Institute and the Center for Data Science, New York University. I belong to the CILVR (Computational Intelligence, Learning, Vision and Robotics) group and I co-founded the MaD (Math and Data) group. During Spring 2020 I was also a Member of the Institute for Advanced Study, part of the Special Year on Optimization, Statistics and Theoretical Machine Learning.\nMy research interests touch several areas of Machine Learning, Signal Processing and High-Dimensional Statistics. I am particularly interested in mathematical foundations of Deep Learning, covering optimization, representation and statistical aspects and their interplay. I am also interested in developing applications to computational science, in areas such as Quantum chemistry, cosmology and climate modeling.\nAs of Fall 2020, I am also a Visiting Scholar at the Flatiron Institute (Simons Foundation), at the Center for Computational Mathematics, where I am co-organizing a working group on the Mathematics of Deep Learning."
  },
  {
    "objectID": "logml2024/speakers2024/keynote/kathryn-hess.html",
    "href": "logml2024/speakers2024/keynote/kathryn-hess.html",
    "title": "Kathryn Hess",
    "section": "",
    "text": "Home\n    Speakers\n    Kathryn Hess\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nKathryn Hess received her PhD in algebraic topology from MIT and held positions at the universities of Stockholm, Nice, and Toronto before moving to the EPFL, where she is now full professor of mathematics and life sciences. Her research focuses on algebraic topology and its applications, primarily in the life sciences, but also in materials science. In her work in applied topology, she has elaborated methods based on topological data analysis for high-throughput screening of nanoporous crystalline materials, classification and synthesis of neuron morphologies, and classification of neuronal network dynamics. She has also developed and applied innovative topological approaches to network theory."
  },
  {
    "objectID": "logml2024/projects2024/project14/index.html",
    "href": "logml2024/projects2024/project14/index.html",
    "title": "Invariantly learning terminal singularities",
    "section": "",
    "text": "Sara Veneziale is a final year PhD student in the Department of Mathematics at Imperial College London, as part of the London School of Geometry and Number Theory. Her research is focused on applying machine learning and data analysis methods to problems arising from pure mathematics, with the aim of helping conjecture formulation. Before starting her PhD she studied Mathematics at the University of Warwick, where she completed her integrated masters in 2020."
  },
  {
    "objectID": "logml2024/projects2024/project14/index.html#sara-veneziale",
    "href": "logml2024/projects2024/project14/index.html#sara-veneziale",
    "title": "Invariantly learning terminal singularities",
    "section": "",
    "text": "Sara Veneziale is a final year PhD student in the Department of Mathematics at Imperial College London, as part of the London School of Geometry and Number Theory. Her research is focused on applying machine learning and data analysis methods to problems arising from pure mathematics, with the aim of helping conjecture formulation. Before starting her PhD she studied Mathematics at the University of Warwick, where she completed her integrated masters in 2020."
  },
  {
    "objectID": "logml2024/projects2024/project14/index.html#project",
    "href": "logml2024/projects2024/project14/index.html#project",
    "title": "Invariantly learning terminal singularities",
    "section": "Project",
    "text": "Project\nRecent work [1] uses a neural network to predict an important, but hard to track, property of geometrical shapes: that of having at worst terminal singularities. The network is later used to generate a lot of data, by replacing a computationally expensive routine with the neural network, to start sketching the landscape of a certain class of geometrical shapes.\nThe experiment in the paper is carried out on a special class of geometrical shapes, called toric varieties. These are highly symmetrical objects and their geometric information is summarised in a weight matrix. However, the space of weight matrices is subject to two group actions: one of the symmetric group shuffling the columns and one of GL_2(Z) acting on the left by multiplication. These two actions change the weight matrix but leave the actual geometrical object unchanged. In the paper, we identify a special form of a weight matrix (which we call the standard form) which precisely identifies one specific representative of each group orbit. Before training and testing, each weight matrix is transformed into the corresponding standard form, which allows us to avoid any symmetry problem in the design of the architecture of the model.\nThe aim of this project is to survey currently available invariant machine learning methods and apply them to this problem. The aim is to compare accuracy, training time, and training sample size with the results obtained by using the standard form. This is a very important analysis to perform. In fact, the results in the paper were carried out only for dimension eight and rank two geometrical objects (toric varieties) and the training of the model required more data than one might like, since it is relatively expensive to calculate. Finding that different methods and architectures perform better with less training data would greatly aid in replicating this result in different dimensions and ranks.\n[1] T. Coates, A. M. Kasprzyk, S. Veneziale. Machine learning detects terminal singularities. Thirty-seventh Conference on Neural Information Processing Systems (2023)."
  },
  {
    "objectID": "logml2024/projects2024/project12/index.html",
    "href": "logml2024/projects2024/project12/index.html",
    "title": "Mixed Curvature Graph Neural Networks",
    "section": "",
    "text": "Rishi Sonthalia is a Hedrick Assistant Adjunct Professor in the Math department at UCLA. He completed his Ph.D. from the University of Michigan where he won the Peter Smereka prize for the best Applied Math thesis. Rishi’s research interested is in mathematics for machine learning with a special focus on generalization, optimization, and the role of geometry."
  },
  {
    "objectID": "logml2024/projects2024/project12/index.html#rishi-sonthalia",
    "href": "logml2024/projects2024/project12/index.html#rishi-sonthalia",
    "title": "Mixed Curvature Graph Neural Networks",
    "section": "",
    "text": "Rishi Sonthalia is a Hedrick Assistant Adjunct Professor in the Math department at UCLA. He completed his Ph.D. from the University of Michigan where he won the Peter Smereka prize for the best Applied Math thesis. Rishi’s research interested is in mathematics for machine learning with a special focus on generalization, optimization, and the role of geometry."
  },
  {
    "objectID": "logml2024/projects2024/project12/index.html#project",
    "href": "logml2024/projects2024/project12/index.html#project",
    "title": "Mixed Curvature Graph Neural Networks",
    "section": "Project",
    "text": "Project\nRecent work has shown that hyperbolic geometry can be very useful in improving the performance of neural networks, including graph neural networks. There has also been recent work suggesting that using the correct geometry (based on curvature) can be used to alleviate oversquashing in GNNs. Hence it is currently of relevance to understand the performance of GNNs that use mix curvature geometries. For this a variety of different models have been proposed - product manifolds (Gu et al. 2019), hierarchical hyperbolic spaces (Sonthalia et al. 2022), the space of positive definite matrices (Lopez et al. 2021), as well neural networks that intertwine standard Euclidean and hyperbolic layers (Cui at al. 2022)."
  },
  {
    "objectID": "logml2024/projects2024/project9/index.html",
    "href": "logml2024/projects2024/project9/index.html",
    "title": "On the Geometry of Relative Representations",
    "section": "",
    "text": "Marco Fumero is an ELLIS Ph.D. student at Sapienza University of Rome (moving to IST Austria as PostDoc in prof. Locatello group) in the GLADIA research group led by Professor Emanuele Rodolà. His previous experiences includes a research internship at Autodesk AI LAB and Amazon AWS AI working on geometric deep earning and causal representation learning topics. His research stands at the intersection of geometry and deep learning with a focus on representation learning, disentanglement and out-of-distribution generalization. He has been recently focusing on the direction of latent space communication, and, more broadly, on the question of when how and why distinct learning processes yield similar representation, organizing also the UniReps workshop at NeurIPS 2023 on these topics."
  },
  {
    "objectID": "logml2024/projects2024/project9/index.html#marco-fumero",
    "href": "logml2024/projects2024/project9/index.html#marco-fumero",
    "title": "On the Geometry of Relative Representations",
    "section": "",
    "text": "Marco Fumero is an ELLIS Ph.D. student at Sapienza University of Rome (moving to IST Austria as PostDoc in prof. Locatello group) in the GLADIA research group led by Professor Emanuele Rodolà. His previous experiences includes a research internship at Autodesk AI LAB and Amazon AWS AI working on geometric deep earning and causal representation learning topics. His research stands at the intersection of geometry and deep learning with a focus on representation learning, disentanglement and out-of-distribution generalization. He has been recently focusing on the direction of latent space communication, and, more broadly, on the question of when how and why distinct learning processes yield similar representation, organizing also the UniReps workshop at NeurIPS 2023 on these topics."
  },
  {
    "objectID": "logml2024/projects2024/project9/index.html#project",
    "href": "logml2024/projects2024/project9/index.html#project",
    "title": "On the Geometry of Relative Representations",
    "section": "Project",
    "text": "Project\nNeural networks embed the geometric structure of a data manifold lying in a high-dimensional space into latent representations. Ideally, the distribution of the data points in the latent space should depend only on the task, the data, the loss, and other architecture-specific constraints. However, factors such as the random weights initialization, training hyperparameters, or other sources of randomness in the training phase may induce incoherent latent spaces that hinder any form of reuse. In a recent work [1] , it has been empirically observed that, under the same data and modeling choices, the angles between the encodings within distinct latent spaces do not change.To exploit this observation,we can compute a representation based on the latent similarity between each sample and a fixed set of training points, denoted as anchors. Given a specific choice of anchors and similarity function, the representation will retain different properties: choosing cosine similarity in the latent spaces enforces the desired invariance to angle preserving transformation of the latent space, without any additional training procedures. Neural architectures can leverage these relative representations to guarantee, in practice, invariance to latent isometries and rescalings, effectively enabling latent space communication: from zero-shot model stitching to latent space comparison between diverse settings, on different datasets, spanning various modalities (images, text, graphs), tasks (e.g., classification, reconstruction) and architectures (e.g., CNNs, GCNs, transformers).\nThe choice of similarity function should not be limited to capture invariances to angle preserving transformations. As shown in [2], other choices are good as well, and there’s not a clear best choice for capturing transformation across distinct latent spaces, depending on the setting and nuisance factors that affect the representations and cause the spaces to differ.\nIn this project the objective is to follow this direction by extending the notion of similarity function in two different ways aiming at enhancing the expressivity of this framework using differential geometry and topological analysis tools.\nHigher Order Relative Representations: The first direction aims to enhance the scope of the relative representation function by considering a similarity function that takes three or more arguments in input, switching from considering binary relations between data points to n-way relations. From a geometrical perspective, the standard relative representation corresponds to constructing a bipartite graph between anchors and sample: considering n-way relations is analogous to constructing a simplicial complex in the latent space. For instance, three-way relationships correspond to triangles, four-way to tetrahedra, and so on. The objective here is to assess whether this extension enriches the framework’s expressiveness from both practical and theoretical standpoints.\nRelative Geodesic Representations: Our second area of exploration involves utilizing geodesic distance as a similarity function, calculated within latent spaces, as a metric for relative representation. This approach ensures that the relative space remains invariant to the isometries of the data’s manifold, defined by geodesic distance. The primary objective of this project is to experimentally determine whether this set of transformations results in a more expressive and efficient relative space, one that more accurately encapsulates the relationships and transformations across different latent spaces. A secondary goal is to devise effective approximation techniques for computing geodesics, thereby enhancing the practical efficiency of the method.\n[1] Moschella, L., Maiorca, V., Fumero, M., Norelli, A., Locatello, F., & Rodola, E. (2022). Relative representations enable zero-shot latent space communication.ICLR 2023\n[2] Cannistraci, I., Moschella, L., Fumero, M., Maiorca, V., & Rodolà, E. (2023). From Bricks to Bridges: Product of Invariances to Enhance Latent Space Communication. arXiv\n[3] Shao, H., Kumar, A., & Thomas Fletcher, P. (2018). The Riemannian geometry of deep generative models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (pp. 315-323)."
  },
  {
    "objectID": "logml2024/projects2024/project1/index.html",
    "href": "logml2024/projects2024/project1/index.html",
    "title": "Predicting the pathogenicity of a (missense) mutation",
    "section": "",
    "text": "Abhishek is currently a senior research scientist in AI dept of Illumina, Cambridge. He did his PhD in computer science from Ecole polytechnique and masters in Applied Math from Ecole Centrale Paris. Before that, he also spent time as a researcher at EPFL, INRIA and MPI. In general, he is very interested in ML breakthroughs in real world problems with major societal impact."
  },
  {
    "objectID": "logml2024/projects2024/project1/index.html#abhishek-sharma",
    "href": "logml2024/projects2024/project1/index.html#abhishek-sharma",
    "title": "Predicting the pathogenicity of a (missense) mutation",
    "section": "",
    "text": "Abhishek is currently a senior research scientist in AI dept of Illumina, Cambridge. He did his PhD in computer science from Ecole polytechnique and masters in Applied Math from Ecole Centrale Paris. Before that, he also spent time as a researcher at EPFL, INRIA and MPI. In general, he is very interested in ML breakthroughs in real world problems with major societal impact."
  },
  {
    "objectID": "logml2024/projects2024/project1/index.html#project",
    "href": "logml2024/projects2024/project1/index.html#project",
    "title": "Predicting the pathogenicity of a (missense) mutation",
    "section": "Project",
    "text": "Project\nPredicting the effect of a (missense) mutation accurately is one of the central problems in biology. Currently, two paradigms dominate benchmarks for missense variant pathogenicity prediction, namely PrimateAI-3D and alpha-missense [1]. Interestingly, both follow totally different approaches in their learned model. While PrimateAI-3D is trained from scratch, alpha-missense [2] follows a pretraining-finetuning strategy. Both approaches rely on MSA, Protein 3D structures as input and attention mechanism in their model architecture. The goal of the project is to explore the alpha-missense pretraining-fine tuning strategy and find (simpler/better) alternatives E.g. [3] outlines a (simpler) alternative to the pre-training part (alphafold2.)\n1 . PrimateAI-3D outperforms AlphaMissense in real-world cohorts (Under Review) https://www.medrxiv.org/content/10.1101/2024.01.12.24301193v1\n\nAccurate proteome-wide missense variant effect prediction with AlphaMissense Cheng et al. Science, 2023\nEfficient and accurate prediction of protein structure using RoseTTAFold2. Minkyung Baek, Ivan Anishchenko, Ian Humphreys, Qian Cong, David Baker, Frank DiMaio Bio arxiv, 2023.\nhttps://structural-bioinformatics.netlify.app/index/"
  },
  {
    "objectID": "logml2024/projects2024/project10/index.html",
    "href": "logml2024/projects2024/project10/index.html",
    "title": "Multimodal Protein Representation Learning",
    "section": "",
    "text": "Michail Chatzianastasis is a PhD student at École Polytechnique in Paris, under the supervision of Prof. Michalis Vazirgiannis. He has a keen interest in machine learning on graph-structured data, focusing on developing neural network architectures for solving real-world problems in biology. His current research focuses on multimodal generative models for protein representation learning, combining both graph and text modalities using Graph Neural Networks and Large Language Models. Previously, he interned at Flatiron Institute of Simons Foundation in New York, working on graph neural networks for cancer gene prediction under the guidance of Prof. Zijun Frank Zhang."
  },
  {
    "objectID": "logml2024/projects2024/project10/index.html#michail-chatzianastasis",
    "href": "logml2024/projects2024/project10/index.html#michail-chatzianastasis",
    "title": "Multimodal Protein Representation Learning",
    "section": "",
    "text": "Michail Chatzianastasis is a PhD student at École Polytechnique in Paris, under the supervision of Prof. Michalis Vazirgiannis. He has a keen interest in machine learning on graph-structured data, focusing on developing neural network architectures for solving real-world problems in biology. His current research focuses on multimodal generative models for protein representation learning, combining both graph and text modalities using Graph Neural Networks and Large Language Models. Previously, he interned at Flatiron Institute of Simons Foundation in New York, working on graph neural networks for cancer gene prediction under the guidance of Prof. Zijun Frank Zhang."
  },
  {
    "objectID": "logml2024/projects2024/project10/index.html#project",
    "href": "logml2024/projects2024/project10/index.html#project",
    "title": "Multimodal Protein Representation Learning",
    "section": "Project",
    "text": "Project\nProteins are fundamental building blocks of life, playing crucial roles in various biological processes. Representing proteins is a multifaceted challenge due to their complex structural and functional characteristics. Traditionally, proteins have been represented in various ways, including as amino acid sequences, 2D graphs, and 3D graphs, each capturing different aspects of protein structure and function. However, these diverse representations often provide limited insight when considered in isolation. In this project, we aim to address this challenge by exploring multimodal protein representation learning methods[1,2], with the goal of discovering the most effective way to represent proteins or fusing these modalities to enhance downstream tasks. The primary goal of this project is to develop techniques that can effectively unify and harness the information contained in different modalities of protein representation. This involves understanding the complementary aspects of amino acid sequences, 2D graphs (e.g., contact maps), and 3D graphs (e.g., protein structures) and finding a way to merge and learn useful representations from them. By improving the representation learning process, we aim to boost the performance of downstream protein-related tasks such as function property prediction, and protein-protein interaction prediction. Our efforts will result in more accurate models that can have a profound impact on bioinformatics and drug discovery.\n[1] Xu, Minghao, et al. “Protst: Multi-modality learning of protein sequences and biomedical texts.”\n[2] Abdine, Hadi, et al. “Prot2Text: Multimodal Protein’s Function Generation with GNNs and Transformers.”"
  },
  {
    "objectID": "logml2024/projects2024/project16/index.html",
    "href": "logml2024/projects2024/project16/index.html",
    "title": "Geometry for Distribution Learning",
    "section": "",
    "text": "Zhengang Zhong is a third-year PhD student at Imperial College London. His research focuses on shape optimisation and data-driven stochastic optimal control. A particular emphasis has been placed on data-driven distributionally robust control, which encodes the knowledge about the uncertainty of the controlled system into safe control actions with the help of the information and optimal transport geometry. Before his PhD, Zhengang received his Diplom Ingenieur degree in Mechatronics at the Technical University of Dresden, Germany."
  },
  {
    "objectID": "logml2024/projects2024/project16/index.html#zhengang-zhong",
    "href": "logml2024/projects2024/project16/index.html#zhengang-zhong",
    "title": "Geometry for Distribution Learning",
    "section": "",
    "text": "Zhengang Zhong is a third-year PhD student at Imperial College London. His research focuses on shape optimisation and data-driven stochastic optimal control. A particular emphasis has been placed on data-driven distributionally robust control, which encodes the knowledge about the uncertainty of the controlled system into safe control actions with the help of the information and optimal transport geometry. Before his PhD, Zhengang received his Diplom Ingenieur degree in Mechatronics at the Technical University of Dresden, Germany."
  },
  {
    "objectID": "logml2024/projects2024/project16/index.html#project",
    "href": "logml2024/projects2024/project16/index.html#project",
    "title": "Geometry for Distribution Learning",
    "section": "Project",
    "text": "Project\nInformation and optimal transport geometry provide powerful tools for analyzing and understanding the characteristics of complex probability distributions, thereby fostering the development of fast and scalable methods for approximating these distributions. For example, with the help of optimal transport geometry, sampling problems can be viewed as a gradient flow with respect to the Wasserstein geometry [1].\nIn the project, we will conduct an experiment similar to section 5 in [2] and section 6 in [3]: we will solve Bayesian inference problems based on optimal transport geometry. Then we will compare the performance of Wasserstein variational inference with the methods using different metrics on the space of probability measures and classic MCMC methods based on various information geometries.\n[1] Garcia Trillos, N., B. Hosseini, and D. Sanz-Alonso. ““From Optimization to Sampling Through Gradient Flows.”” arXiv e-prints (2023): arXiv-2302.\n[2] Lambert, Marc, et al. ““Variational inference via Wasserstein gradient flows.”” Advances in Neural Information Processing Systems 35 (2022): 14434-14447.\n[3] Ambrogioni, Luca, et al. ““Wasserstein variational inference.”” Advances in Neural Information Processing Systems 31 (2018)."
  },
  {
    "objectID": "logml2024/projects2024/project4/index.html",
    "href": "logml2024/projects2024/project4/index.html",
    "title": "Geometric GNNs for particle level reconstruction",
    "section": "",
    "text": "Dolores Garcia is a Senior Fellow in the Experimental Physical Department at CERN. She received her MSc in Theoretical Physics from Imperial College London, and her PhD in Telecommunications Engineering from University of Carlos III supervised by Joerg Widmer. Her research interests include equivariant machine learning, graph neural networks and the application of these to high energy physics."
  },
  {
    "objectID": "logml2024/projects2024/project4/index.html#dolores-garcia",
    "href": "logml2024/projects2024/project4/index.html#dolores-garcia",
    "title": "Geometric GNNs for particle level reconstruction",
    "section": "",
    "text": "Dolores Garcia is a Senior Fellow in the Experimental Physical Department at CERN. She received her MSc in Theoretical Physics from Imperial College London, and her PhD in Telecommunications Engineering from University of Carlos III supervised by Joerg Widmer. Her research interests include equivariant machine learning, graph neural networks and the application of these to high energy physics."
  },
  {
    "objectID": "logml2024/projects2024/project4/index.html#project",
    "href": "logml2024/projects2024/project4/index.html#project",
    "title": "Geometric GNNs for particle level reconstruction",
    "section": "Project",
    "text": "Project\nThe particle flow algorithm enables the reconstruction of the particle-level view of an event by integrating information from the entire detector, spanning from the tracker to the calorimeters. Machine learning (ML) holds promise in enhancing the quality of reconstruction by leveraging raw data and acquiring the ability to untangle complex events. In this project, the students will explore a geometric graph neural network approach for particle-level reconstruction, focusing on the simplified scenario of e+e- collisions. The primary objective is to investigate architectures capable of extracting complex geometric structures, particle showers, from geometric graphs generated through simulations. Specifically, the team will delve into understanding the significance of equivariance and locality for this problem. The project will involve crafting a representation, managing large graphs in a distributed manner, and assessing the physics outputs."
  },
  {
    "objectID": "logml2024/projects2024/project2/index.html",
    "href": "logml2024/projects2024/project2/index.html",
    "title": "Matching graphs with spatial constrains",
    "section": "",
    "text": "Anna Calissano is a Chapman Fellow at the Department of Mathematics at Imperial College London. She conducts research on defining suitable geometrical embeddings and novel statistical tools for the analysis of set of graphs and networks. She works at the intersection of geometry, statistics, and computing. Her main applicational areas are urban planning and medical imaging. Anna received a PhD in Mathematics from Politecnico di Milano in 2021, under the supervision of Prof. Simone Vantini and Prof. Aasa Feragen (DTU). She worked as a postdoctoral researcher at INRIA (France) within the ERC Project Geometric Statistics leaded by Xavier Pennec."
  },
  {
    "objectID": "logml2024/projects2024/project2/index.html#anna-calissano",
    "href": "logml2024/projects2024/project2/index.html#anna-calissano",
    "title": "Matching graphs with spatial constrains",
    "section": "",
    "text": "Anna Calissano is a Chapman Fellow at the Department of Mathematics at Imperial College London. She conducts research on defining suitable geometrical embeddings and novel statistical tools for the analysis of set of graphs and networks. She works at the intersection of geometry, statistics, and computing. Her main applicational areas are urban planning and medical imaging. Anna received a PhD in Mathematics from Politecnico di Milano in 2021, under the supervision of Prof. Simone Vantini and Prof. Aasa Feragen (DTU). She worked as a postdoctoral researcher at INRIA (France) within the ERC Project Geometric Statistics leaded by Xavier Pennec."
  },
  {
    "objectID": "logml2024/projects2024/project2/index.html#project",
    "href": "logml2024/projects2024/project2/index.html#project",
    "title": "Matching graphs with spatial constrains",
    "section": "Project",
    "text": "Project\nProblem Statement: Given two graphs, finding a correspondence between the two sets of nodes has been broadly studied in the literature as graph matching. In many real-world applications, the matching is constrained: not every node can be assigned to every other node. Consider for example the brain structural connectivity of different subjects, a misalignment between brain parcels (i.e. nodes) can occur during the signal preprocessing. However, the only reasonable alignments are between neighboring parcels [1]. The goal of the project is to define a graph matching procedure with spatial constrains (e.g. the spatial distance on the cortex between parcels) on a set of structural connectivity matrices of healthy subjects from the Human Connectome Project.\nSolutions to explore: The problem can be addressed in different ways: (1) by adding a penalization term in the optimization procedure which depends on the spatial distance of the nodes (e.g. Rigid Graph Matching [2]); (2) by defining an optimization problem using the generators of the permutation group ([3]).\n[1] Calissano, A., Papadopoulo, T., Pennec, X., & Deslauriers-Gauthier, S. (2022). Graph Alignment Exploiting the Spatial Organisation Improves the Similarity of Brain Networks. In Human Brain Mapping (to appear)\n[2] Ravindra, V., Nassar, H., Gleich, D. F., & Grama, A. (2020). Rigid graph alignment. In Complex Networks and Their Applications VIII: Volume 1 Proceedings of the Eighth International Conference on Complex Networks and Their Applications COMPLEX NETWORKS 2019 8 (pp. 621-632). Springer International Publishing.\n[3] Coxeter, H. S., & Moser, W. O. (2013). Generators and relations for discrete groups (Vol. 14). Springer Science & Business Media."
  }
]